# Energy Transformer


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

<style>
    .red { color:rgb(247, 109, 104); }
    .blue { color:rgb(64, 130, 200); }
    .green { color:rgb(89, 203, 78); }
    .yellow { color:rgb(252, 211, 28); }
</style>

## Transformers look like Dynamic Systems

> Squint, and the Transformer looks like a dynamic system.

The transformer is the engine behind the best neural networks we have
today. At its core, the transformer is a stack of *L* transformer blocks
that takes a length *N* sequence of input tokens
{**x**<sub>1</sub><sup>(0)</sup>, …, **x**<sub>*N*</sub><sup>(0)</sup>}
and outputs a length *N* sequence of output tokens
{**x**<sub>1</sub><sup>(*L*)</sup>, …, **x**<sub>*N*</sub><sup>(*L*)</sup>}.
Each token **x**<sub>*i*</sub><sup>(*l*)</sup> ∈ ℝ<sup>*D*</sup> is a
vector of dimension *D*.

Each transformer block contains four important components:

1.  An <span class="red">**attention mechanism**</span> that lets each
    token exchange information with other tokens in the same sequence.
    The attention is described to have multiple “attention heads”,
    allowing different heads to route different kinds of information
    between tokens.
2.  A <span class="blue">**2-layer MLP**</span> transforms each token
    independently into the representation for the next layer. *The MLP
    does not allow tokens to mix with each other.*
3.  A <span class="green">**Layer-Normalization**</span> normalizes the
    token representation for gradient and computation stability. We will
    consider the version of the Transformer with *pre-layernorms*, where
    normalization occurs before the attention and MLP operations.
4.  A <span class="yellow">**residual connection**</span> that adds the
    input to the output of each of these sub-operations.

When blocks are stacked, the residual connections connect the blocks
together and form a “residual highway” that consists entirely of
additions and normalizations. Each `Attention` and `MLP` operation
*exclusively add* information to the highway. If we index each stacked
block as *t*, you can see some resemblance to a discrete dynamical
system, where the final computation is an “integration” of sorts over
the operation of the entire network. At a very high level, you can see
the transformer as a dynamical system where *F*<sub>*t*</sub> is the
operation of the *t*-th block.

<span id="eq-generic-dynamical-system">
**x**<sup>(*t* + 1)</sup> = **x**<sup>(*t*)</sup> + *F*<sub>*t*</sub>(**x**<sup>(*t*)</sup>)   (1)
</span>

![](./figs/standard-transformer.png)

**Associative Memory** (AM) requires a global energy function, where
each “computational block” minimizes the total energy of the system.
<a href="#eq-generic-dynamical-system"
class="quarto-xref">Equation 1</a> would look like a gradient descent on
some energy function if we could guarantee:

1.  *F*<sub>*t*</sub> was the same each iteration i.e., all blocks
    {*F*<sub>1</sub>, …, *F*<sub>*L*</sub>} **share parameters**. Let’s
    call this shared-parameter block *F* and it is independent of the
    number of recurrent steps or “layers”.
2.  *F* could be described as the gradient of a scalar **energy** (i.e.,
    *F*(**x**) = −∇*E*(**x**)).
3.  The energy *E* is both **continuous** and **bounded from below**. We
    should be able to write a continuous-time, gradient flow equation
    for the system and make some guarantees about the convergence limit
    of the system.

The rest of this notebook derives the familiar transformer into a true
associative memory. This architecture is called the “Energy Transformer”
(Hoover et al. 2024). It will require fundamental modifications to the
Transformer block. Specifically, the transformer block becomes a
ℝ<sup>*N* × *D*</sup> ↦ ℝ scalar-valued energy function, and each
“layer” of the model becomes a gradient descent step down the energy.
However different the derivation, the end result preserves the four main
components of a Transformer with attention, MLP, layernorms, and
residual connections.

This overview is summarized in
<a href="#fig-standard-transformer" class="quarto-xref">Figure 1</a>.

## Using a pretrained Energy Transformer

## Training an Energy Transformer

We train ET on a simple dataset, should work on CPU.

The Transformer is a very flexible computing paradigm that can be used
for the two major approaches of modern language modeling: **masked token
prediction** (e.g., BERT and diffusion-style transformers) where you
predict the fraction of input tokens that are MASKed using information
from the unmasked tokens, and **autoregressive language modeling**
(e.g., GPT-style models), where each token in the input sequence is
transformed into the next prediction token.

<div id="refs" class="references csl-bib-body hanging-indent"
entry-spacing="0">

<div id="ref-hoover2024energy" class="csl-entry">

Hoover, Benjamin, Yuchen Liang, Bao Pham, Rameswar Panda, Hendrik
Strobelt, Duen Horng Chau, Mohammed Zaki, and Dmitry Krotov. 2024.
“Energy Transformer.” *Advances in Neural Information Processing
Systems* 36.
<https://proceedings.neurips.cc/paper_files/paper/2023/file/57a9b97477b67936298489e3c1417b0a-Paper-Conference.pdf>.

</div>

</div>
