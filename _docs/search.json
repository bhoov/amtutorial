[
  {
    "objectID": "lib/index.html",
    "href": "lib/index.html",
    "title": "Reusable utils",
    "section": "",
    "text": "Reusable utils\nTo aid in the pedagogy of the tutorial, a lot of logic (e.g., data processing, boilerplate training code, etc.) has been ported to this small library of reusable utilities.\nAll notebooks are designed to be run using CPU only. Requirements can be installed with:\npip install amtutorial"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Associative Memory Tutorial",
    "section": "",
    "text": "A complete introduction to Associative Memories and Hopfield Networks\n\nThis website serves as a living companion to the tutorial manuscript and to the tutorial presentation at ICML 2025. It dreams of being a one-stop shop for learning all things about Associative Memory. It’s still working towards that.\n\n\n\nPlaying with the codebase\n\n\n\n\n\nThe website is a (growing) collection of notebook demos on Associative Memory. Each notebook is primarily a blog post on this site, but it is also fully runnable on colab and as a raw .ipynb file using the uv environment setup below.\n\nDense binary storage, also distributed as colab notebook and raw .ipynb.\nEnergy Transformer, also distributed as colab notebook and raw .ipynb. \nDiffusion as Memory, also distributed as colab notebook and raw .ipynb.\nDistributed Associative Memory, also distributed as colab notebook and raw .ipynb.\n\nSee the overview in tutorials for a bit more detail.\nTo add new examples, edit the source tutorial notebooks (as either .ipynb or plain text .qmd files) saved in nbs/tutorial/.\n\n\n\n\n\n\nSlow first run\n\n\n\nThe first time you run the notebooks will be slow. We cache some of the long-running code after the first time, but the cache will not persist across Colab sessions.\n\n\n\n\n\npip install amtutorial\nWe aim for simplicity and clarity in the notebooks. Thus, we migrate some helper functions (particularly around loading and processing data, see nbs/lib/01_data_utils.qmd) to a pypi package called amtutorial to avoid cluttering the notebooks. An added benefit of this is that all dependencies needed to run these notebooks can be installed using pip install amtutorial.\n\n\n\n\n\n\nHow this website is built\n\n\n\nThe website is built using an in-house fork of nbdev that develops everything in this tutorial from source .ipynb or .qmd files saved in nbs/. The website, pypi package, and package documentation all come for free with nbdev. The in-house fork enables working with plain text .qmd files instead of .ipynb files. With the right extensions and hotkeys, .qmd files are pleasant to develop inside VSCode and interop seamlessly with both git and AI tooling.\n\n\n\n\n\n\n\n\n\npip install amtutorial\n\n## Install torch to run the `diffusion as memory` notebook. CPU or CUDA versions work\n# pip install torch --index-url https://download.pytorch.org/whl/cpu\n\n## OPTIONAL: For rendering videos in notebooks, use ffmpeg. Can use conda to install as\n#conda install conda-forge::ffmpeg conda-forge::openh264 \nThen open up the .ipynb notebooks in tutorial_ipynbs/ in your favorite notebook editor, using the same env where you installed amtutorial.\n\n\n\nPre-requisites\n\nInstall uv using curl -LsSf https://astral.sh/uv/install.sh | sh\nInstall quarto\nWe use conda (or better yet, mamba) for managing the ffmpeg dependency, which only matters if ffmpeg is not already installed on your system.\n\nSetting up the environment\nFrom the root of the repo:\nuv sync\nsource .venv/bin/activate\n\n# Expose venv to ipython\nuv run ipython kernel install --user --env VIRTUAL_ENV $(pwd)/.venv --name=amtutorial \n\n## Install torch to run the `diffusion as memory` notebook. CPU or CUDA versions work\n# pip install torch --index-url https://download.pytorch.org/whl/cpu\n\n# OPTIONAL: For rendering videos in notebooks\nconda install conda-forge::ffmpeg conda-forge::openh264 \nDevelopment pipelines\nView a local version of the website with:\nuv run nbdev_preview\nPushes to main deploy the website. The site will be live after a few minutes on github.\ngit checkout main\n\n# Update the website. Takes a moment even with cached training runs\nmake deploy && git add . && git commit -m \"Update site\" && git push\nMake a minor-patch update to the pypi package (preferably, only if amtutorials/src was updated):\nmake pypi && uv run nbdev_pypi\nUseful scripts (for reference only)\nuv run nbdev_preview                         # Preview website locally\nbash scripts/prep_website_deploy.sh          # Sync dependencies, export qmd notebooks to ipynb for colab, and build website\nbash scripts/export_qmd_as_ipynb.sh          # Export qmd notebooks to ipynb for colab\nuv run python scripts/sync_dependencies.py   # Sync nbdev and pyproject.toml dependencies\nuv run python scripts/prep_pypi.py           # Bump patch version and sync dependencies\nuv run nbdev_pypi                            # Push to pypi"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Associative Memory Tutorial",
    "section": "",
    "text": "Playing with the codebase\n\n\n\n\n\nThe website is a (growing) collection of notebook demos on Associative Memory. Each notebook is primarily a blog post on this site, but it is also fully runnable on colab and as a raw .ipynb file using the uv environment setup below.\n\nDense binary storage, also distributed as colab notebook and raw .ipynb.\nEnergy Transformer, also distributed as colab notebook and raw .ipynb. \nDiffusion as Memory, also distributed as colab notebook and raw .ipynb.\nDistributed Associative Memory, also distributed as colab notebook and raw .ipynb.\n\nSee the overview in tutorials for a bit more detail.\nTo add new examples, edit the source tutorial notebooks (as either .ipynb or plain text .qmd files) saved in nbs/tutorial/.\n\n\n\n\n\n\nSlow first run\n\n\n\nThe first time you run the notebooks will be slow. We cache some of the long-running code after the first time, but the cache will not persist across Colab sessions.\n\n\n\n\n\npip install amtutorial\nWe aim for simplicity and clarity in the notebooks. Thus, we migrate some helper functions (particularly around loading and processing data, see nbs/lib/01_data_utils.qmd) to a pypi package called amtutorial to avoid cluttering the notebooks. An added benefit of this is that all dependencies needed to run these notebooks can be installed using pip install amtutorial.\n\n\n\n\n\n\nHow this website is built\n\n\n\nThe website is built using an in-house fork of nbdev that develops everything in this tutorial from source .ipynb or .qmd files saved in nbs/. The website, pypi package, and package documentation all come for free with nbdev. The in-house fork enables working with plain text .qmd files instead of .ipynb files. With the right extensions and hotkeys, .qmd files are pleasant to develop inside VSCode and interop seamlessly with both git and AI tooling."
  },
  {
    "objectID": "index.html#sec-installation",
    "href": "index.html#sec-installation",
    "title": "Associative Memory Tutorial",
    "section": "",
    "text": "pip install amtutorial\n\n## Install torch to run the `diffusion as memory` notebook. CPU or CUDA versions work\n# pip install torch --index-url https://download.pytorch.org/whl/cpu\n\n## OPTIONAL: For rendering videos in notebooks, use ffmpeg. Can use conda to install as\n#conda install conda-forge::ffmpeg conda-forge::openh264 \nThen open up the .ipynb notebooks in tutorial_ipynbs/ in your favorite notebook editor, using the same env where you installed amtutorial.\n\n\n\nPre-requisites\n\nInstall uv using curl -LsSf https://astral.sh/uv/install.sh | sh\nInstall quarto\nWe use conda (or better yet, mamba) for managing the ffmpeg dependency, which only matters if ffmpeg is not already installed on your system.\n\nSetting up the environment\nFrom the root of the repo:\nuv sync\nsource .venv/bin/activate\n\n# Expose venv to ipython\nuv run ipython kernel install --user --env VIRTUAL_ENV $(pwd)/.venv --name=amtutorial \n\n## Install torch to run the `diffusion as memory` notebook. CPU or CUDA versions work\n# pip install torch --index-url https://download.pytorch.org/whl/cpu\n\n# OPTIONAL: For rendering videos in notebooks\nconda install conda-forge::ffmpeg conda-forge::openh264 \nDevelopment pipelines\nView a local version of the website with:\nuv run nbdev_preview\nPushes to main deploy the website. The site will be live after a few minutes on github.\ngit checkout main\n\n# Update the website. Takes a moment even with cached training runs\nmake deploy && git add . && git commit -m \"Update site\" && git push\nMake a minor-patch update to the pypi package (preferably, only if amtutorials/src was updated):\nmake pypi && uv run nbdev_pypi\nUseful scripts (for reference only)\nuv run nbdev_preview                         # Preview website locally\nbash scripts/prep_website_deploy.sh          # Sync dependencies, export qmd notebooks to ipynb for colab, and build website\nbash scripts/export_qmd_as_ipynb.sh          # Export qmd notebooks to ipynb for colab\nuv run python scripts/sync_dependencies.py   # Sync nbdev and pyproject.toml dependencies\nuv run python scripts/prep_pypi.py           # Bump patch version and sync dependencies\nuv run nbdev_pypi                            # Push to pypi"
  },
  {
    "objectID": "tutorial/distributed_memory.html",
    "href": "tutorial/distributed_memory.html",
    "title": "Distributed Memory",
    "section": "",
    "text": "In this notebook, we demonstrate how we utilize random features to disentangle the size of the Dense Associative Memory network from the number of memories to be stored. Given the standard log-sum-exp energy \\(E_\\beta(\\cdot; \\boldsymbol{\\Xi})\\), corresponding to a model \\(f_\\boldsymbol{\\Xi}\\) of size \\(O(DK)\\), we demonstrate how we can use the trigonometric random features to develop an approximate energy \\(\\tilde{E}_\\beta(\\cdot; \\mathbf{T})\\) using a distributed representation \\(\\mathbf{T}\\) of the memories \\(\\boldsymbol{\\Xi} = \\{ \\boldsymbol{\\xi}^\\mu, \\mu \\in [\\![ K ]\\!] \\}\\), thus giving us a model \\(f_{\\mathbf{T}}\\) of size \\(O(Y)\\).\nFor further details on this work, please see [1].",
    "crumbs": [
      "tutorial",
      "Distributed Memory"
    ]
  },
  {
    "objectID": "tutorial/distributed_memory.html#exact-energy-function",
    "href": "tutorial/distributed_memory.html#exact-energy-function",
    "title": "Distributed Memory",
    "section": "Exact Energy Function",
    "text": "Exact Energy Function\nConsider a set of memories \\(\\boldsymbol{\\Xi} = \\{ \\boldsymbol{\\xi}^1, \\ldots, \\boldsymbol{\\xi}^K \\}\\) where each memory \\(\\boldsymbol{\\xi}^\\mu \\in \\mathbb{R}^D\\) is a vector in a \\(D\\)-dimensional Euclidean space. For a state vector \\(\\mathbf{v} \\in \\mathbb{R}^D\\), the commonly used log-sum-exp energy is given by \\[\nE_\\beta( \\mathbf{v}; \\boldsymbol{\\Xi} ) = - \\frac{1}{\\beta} \\log \\sum_{\\mu = 1}^K \\exp \\left(- \\frac{\\beta}{2} \\left\\Vert \\mathbf{v} - \\boldsymbol{\\xi}^\\mu \\right \\Vert^2 \\right),\n\\tag{1}\\]\nwhere \\(\\beta &gt; 0\\) is the inverse-temperature controlling the sharpness of the energy near the memories, with larger values of \\(\\beta\\) implying sharper energy landscapes, while smaller values induce smoother ones.\nWe can implement this energy function as follows\n\ndef lse_energy(\n    state: Float[Array, \"D\"],\n    memories: Float[Array, \"K D\"],\n    beta: float\n) -&gt; Float[Array, \"\"]:\n    \"\"\"\n    Compute the standard log-sum-exp energy\n    using the negative square Euclidean distance\n    as the similarity\n    \"\"\"\n    return -(1 / beta) * jax.nn.logsumexp(\n        -beta / 2 * ((state - memories) ** 2).sum(-1),\n        axis=0\n    )\n\n\nVisualizing the Energy in 2D\nFirst we randomly generate \\(K=8\\) memories \\(\\boldsymbol{\\Xi} = \\{ \\boldsymbol{\\xi}^\\mu, \\mu = 1, \\ldots, 8 \\}\\) in \\(D=2\\) dimensions.\n\n# Randomly generate memories in \n# the selected domain\nrngidx = 0\nD = 2\nK = 8\nXi = jr.uniform(\n    rnglist[rngidx], (K, D)\n) * 2 * maxabs - maxabs\n\nGiven this set \\(\\boldsymbol{\\Xi}\\) of memories, we compute and visualize the 2D energy landscape defined by Equation 1 for varying values of the inverse-temperature \\(\\beta \\in \\{10^{-2}, 10^{-1}, 1, 10 \\}\\). We also highlight the \\(K = 8\\) memories on these landscapes with the \\(\\boldsymbol{\\star}\\) symbol. We will also cache the energy landscapes for future visualizations.\n\n\nComputing and visualizing the 2D energy landscape.\nbetas = [0.01, 0.1, 1, 10]\nfigscaler = 2\nfig, axs = plt.subplots(\n    1, len(betas), figsize=(\n        len(betas) * figscaler, \n        figscaler\n    ),\n    sharex=True, sharey=True\n)\nbeta_en_cache = {}\nfor b, ax in tqdm(\n    zip(betas, axs), total=len(betas),\n    colour=TQDMCOLOR, ncols=50\n):\n    en = np.zeros_like(V[0])\n    for i, j in product(\n        range(nsteps),\n        range(nsteps)\n    ):\n        en[i,j] = lse_energy(\n            V[:, i, j], Xi, b\n        )\n    beta_en_cache[b] = en\n    plot_energy_landscape(\n        en, ax, np.array([\n            xmin, xmax, ymin, ymax\n        ])\n    )\n    plot_states(\n        Xi, ax, marker='*', color=MCOLOR\n    )\n    ax.set_title(\n        r\"$\\beta$\" + f\":{b:0.2f}\"\n    )\nplt.show()\n\n\n\n100%|███████████████| 4/4 [00:04&lt;00:00,  1.09s/it]\n\n\n\n\n\n\n\n\n\n\n\n\n\nMinimizing the Energy via Gradient Descent\nFor an initial state vector \\(\\mathbf{q} \\in \\mathbb{R}^D\\), we can minimize its energy utilizing the energy gradient. Initializing the energy descent at the \\(\\mathbf{q}\\), that is \\(\\mathbf{v}^{(0)} \\gets \\mathbf{q}\\), we perform the following gradient descent steps for \\(T\\) iterations: \\[\n\\mathbf{v}^{(t)} \\gets \\mathbf{v}^{(t-1)} - \\alpha \\nabla_{\\mathbf{v}} E_\\beta( \\mathbf{v}^{(t-1)}; \\boldsymbol{\\Xi} ),\n\\] for \\(t = 1, \\ldots, T\\) with \\(\\alpha &gt; 0\\) as the step-size (or learning rate) for the energy descent. The final \\(\\mathbf{v}^{(T)}\\) is the output of the model.\nWe can implement this using auto-differentiation in JAX by computing the gradient of the lse_energy function with respect to its input state.\n\ndef lse_energy_descent( \n    q: Float[Array, \"D\"],\n    memories: Float[Array, \"K D\"],\n    beta,\n    energy_fn,\n    depth: int=10,\n    alpha: float = 0.01,\n    return_grads=False,\n    clamp_idxs: Optional[Bool[Array, \"D\"]]=None\n) -&gt; Float[Array, \"D\"]: \n    \"\"\"\n    Energy descent with the LSE energy\n    \"\"\"\n    dEdxf = jax.jit(\n        jax.value_and_grad(energy_fn)\n    )\n    logs = {}\n    def step(x, i):\n        E, dEdx = dEdxf(x, memories, beta)\n        if clamp_idxs is not None:\n            dEdx = jnp.where(clamp_idxs, 0, dEdx)\n        x = x - alpha * dEdx\n        aux = (E, dEdx) if return_grads else (E,)\n        return x, aux\n    x, aux = jax.lax.scan(\n        step, q, jnp.arange(depth)\n    )\n    logs['energies'] = aux[0]\n    if return_grads:\n        logs['grads'] = aux[1]\n    return x, logs\n\n\nEnergy Descent with the LSE Energy\nAs an example, we will use the previous set of memories to perform the energy descent for a randomly generated query \\(\\mathbf{q} \\in \\mathbb{R}^D\\) using the above function lse_energy_descent. We will note the intermediate states \\(\\mathbf{v}^{(0)}, \\ldots, \\mathbf{v}^{(T)}\\), and the energy \\(E_\\beta(\\mathbf{v}^{(t)}; \\boldsymbol{\\Xi}), t \\in [\\![ T ]\\!]\\) at each layer of the DenseAM (equivalently, the energy at each iteration of the energy gradient descent).\nIn the following example, the number of DenseAM layers (equivalently, the number of energy descent steps) is set at \\(T = 1000\\), and we use a step-size \\(\\alpha = 0.01\\). We will plot the intermediate states at every NUPDATES=25 layers. We show results for three randomly generated queries, where the first row of the plot visualizes their intermediate states of these three queries during the energy descent with the \\(\\textcolor{blue}{\\bullet}\\) symbol, while the next three rows visualize their respective energy descent through the \\(T\\) DenseAM layers. We will cache the intermediate states and energies of the exact energy descent for future visualizations.\n\n\nPerforming and visualizing the energy-descent for three randomly generated queries (initial states).\nNSTATES = 20\nNUPDATES = 25\nNQUERIES = 3\nALPHA = 0.01\nrngidx = 8\nfig, axs = plt.subplots(\n    NQUERIES+1, len(betas), figsize=(\n        len(betas) * figscaler, \n        NQUERIES+2 * figscaler\n    ),\n    sharex=\"row\",\n)\nbeta_true_states_en_cache = {}\nfor bidx, b in tqdm(\n    enumerate(betas), total=len(betas),\n    colour=TQDMCOLOR, ncols=50\n):\n    plot_energy_landscape(\n        beta_en_cache[b], axs[0, bidx],\n        np.array([xmin, xmax, ymin, ymax])\n    )\n    plot_states(\n        Xi, axs[0, bidx],\n        marker='*', color=MCOLOR\n    )\n    axs[0, bidx].set_title(\n        r\"$\\beta$\" + f\":{b:0.2f}\"\n    )\n    # Randomly generating queries\n    queries = jr.uniform(\n        rnglist[rngidx], (NQUERIES, D)\n    ) * 2 * maxabs - maxabs\n    beta_cache = []\n    for qidx, query in enumerate(queries):\n        qstates = [query]\n        qens = []\n        # Perform energy descent\n        for i in range(NSTATES):\n            query, logs = lse_energy_descent(\n                query, Xi, b, lse_energy,\n                depth=NUPDATES, alpha=ALPHA\n            )\n            qstates += [query]\n            qens += [logs['energies']]\n        qstates = np.array(qstates)\n        plot_states(\n            qstates, axs[0, bidx],\n            marker='o', color=QCOLOR\n        )\n        qens = np.array(qens).reshape(-1)\n        plot_energy_descent(\n            qens, axs[qidx+1, bidx],\n            color=QCOLOR\n        )\n        axs[qidx+1, bidx].set_title(\n            f\"Query {qidx+1}\"\n        )\n        beta_cache += [(qstates, qens)]\n    beta_true_states_en_cache[b] = beta_cache\nfig.tight_layout()\nplt.show()\n\n\n\n100%|███████████████| 4/4 [00:11&lt;00:00,  2.80s/it]",
    "crumbs": [
      "tutorial",
      "Distributed Memory"
    ]
  },
  {
    "objectID": "tutorial/distributed_memory.html#viewing-energy-as-a-kernel-sum",
    "href": "tutorial/distributed_memory.html#viewing-energy-as-a-kernel-sum",
    "title": "Distributed Memory",
    "section": "Viewing Energy as a Kernel Sum",
    "text": "Viewing Energy as a Kernel Sum\nIt is easy to see that the aforemention energy function Equation 1 can be viewed as a kernel sum. Specifying a kernel function \\(\\kappa: \\mathbb{R}^D \\times \\mathbb{R}^D \\to \\mathbb{R}\\) such that \\(\\kappa(\\mathbf{x}, \\mathbf{x}') = \\exp(-\\frac{1}{2} \\Vert \\mathbf{x} - \\mathbf{x}' \\Vert^2 )\\), the radial basis function or RBF kernel, we can reduce the energy function to a kernel sum as follows:\n\\[\\begin{align}\nE_\\beta( \\mathbf{v}; \\boldsymbol{\\Xi} )\n& = - \\frac{1}{\\beta} \\log \\sum_{\\mu = 1}^K \\underbrace{\n  \\exp \\left(- \\frac{\\beta}{2} \\left\\Vert \\mathbf{v} - \\boldsymbol{\\xi}^\\mu \\right \\Vert^2 \\right)\n}_{\\kappa(\\sqrt{\\beta}\\mathbf{v}, \\sqrt{\\beta}\\boldsymbol{\\xi}^\\mu)}\n\\\\\n& = - \\frac{1}{\\beta} \\log \\sum_{\\mu = 1}^K \\kappa \\left(\\sqrt{\\beta}\\mathbf{v}, \\sqrt{\\beta}\\boldsymbol{\\xi}^\\mu \\right),\n\\end{align}\\]\nIn general, we need to keep around all the memories \\(\\boldsymbol{\\Xi} = \\left\\{ \\boldsymbol{\\xi}^\\mu \\in \\mathbb{R}^D, \\mu \\in [\\![ K ]\\!] \\right\\}\\) to compute this kernel sum, and thus the energy function.\n\nSimplifying the Kernel Sum with Random Features\nHowever, if there exists a feature map \\(\\Phi: \\mathbb{R}^D \\to \\mathbb{R}^Y\\), such that, the dot-product in this feature space approximates the kernel function as follows:\n\\[\n\\kappa(\\mathbf{x}, \\mathbf{x}') \\approx \\left\\langle \\Phi(\\mathbf{x}), \\Phi(\\mathbf{x}') \\right\\rangle,\n\\]\nthen the kernel sum can be simplified as (dropping the \\(\\beta\\) for now)\n\\[\\begin{align}\n\\sum_{\\mu = 1}^K \\kappa(\\mathbf{v}, \\boldsymbol{\\xi}^\\mu)\n& \\approx \\sum_{\\mu = 1}^K \\left\\langle \\Phi(\\mathbf{v}), \\Phi(\\boldsymbol{\\xi}^\\mu) \\right\\rangle\n\\\\\n& = \\left\\langle \\Phi(\\mathbf{v}), \\sum_{\\mu = 1}^K \\Phi(\\boldsymbol{\\xi}^\\mu) \\right\\rangle\n\\\\\n& = \\left\\langle \\Phi(\\mathbf{v}), \\mathbf{T} \\right\\rangle, \\qquad \\text{where} \\quad\n\\mathbf{T} = \\sum_{\\mu = 1}^K \\Phi(\\boldsymbol{\\xi}^\\mu).\n\\end{align}\\]\nIn this case, we need to compute the vector – the distributed memories – \\(\\mathbf{T} \\in \\mathbb{R}^Y\\) using all the \\(K\\) memories \\(\\left\\{ \\boldsymbol{\\xi}^\\mu, \\mu \\in [\\![ K ]\\!] \\right\\}\\) just once, and then use \\(\\mathbf{T}\\) for all subsequent kernel sum approximation without needing access to the original memories. Bringing the inverse-temperature \\(\\beta\\) into this, we would instead need to utilize \\(\\Phi(\\sqrt{\\beta}\\mathbf{v})\\), and compute the distributed memories as \\(\\mathbf{T} = \\sum_\\mu \\Phi(\\sqrt{\\beta} \\boldsymbol{\\xi}^\\mu)\\).\n\n\nExamples of Random Features\nVarious approximate feature maps have been developed for the RBF kernel. The first feature map proposed by [2] utilizes random features and trigonometric function. More recently, [3] have proposed positive random features utilizing the exponential function. Both these random features are presented below, where \\(\\boldsymbol{\\omega}^i \\sim \\mathcal{N}(0, \\mathbf{I}_D), i \\in [\\![ Y ]\\!]\\) and \\(\\mathcal{N}(0, \\mathbf{I}_D)\\) is the \\(D\\)-dimensional multivariate isotropic standard normal distribution:\n\\[\n\\Phi(\\mathbf{x}) = \\frac{1}{\\sqrt{Y}} \\left[ \\begin{array}{c}\n  \\cos \\langle \\boldsymbol{\\omega}^1, \\mathbf{x} \\rangle \\\\\n  \\sin \\langle \\boldsymbol{\\omega}^1, \\mathbf{x} \\rangle \\\\\n  \\cos \\langle \\boldsymbol{\\omega}^2, \\mathbf{x} \\rangle \\\\\n  \\sin \\langle \\boldsymbol{\\omega}^2, \\mathbf{x} \\rangle \\\\\n  \\cdots \\\\\n  \\cos \\langle \\boldsymbol{\\omega}^Y, \\mathbf{x} \\rangle \\\\\n  \\sin \\langle \\boldsymbol{\\omega}^Y, \\mathbf{x} \\rangle \\\\\n\\end{array}\\right],\n\\qquad\n\\Phi(\\mathbf{x}) = \\frac{\\exp(- \\left\\Vert \\mathbf{x} \\right\\Vert^2)}{\\sqrt{2Y}} \\left[ \\begin{array}{c}\n  \\exp (+\\langle \\boldsymbol{\\omega}^1, \\mathbf{x} \\rangle) \\\\\n  \\exp (-\\langle \\boldsymbol{\\omega}^1, \\mathbf{x} \\rangle) \\\\\n  \\exp (+\\langle \\boldsymbol{\\omega}^2, \\mathbf{x} \\rangle) \\\\\n  \\exp (-\\langle \\boldsymbol{\\omega}^2, \\mathbf{x} \\rangle) \\\\\n  \\cdots \\\\\n  \\exp (+\\langle \\boldsymbol{\\omega}^Y, \\mathbf{x} \\rangle) \\\\\n  \\exp (-\\langle \\boldsymbol{\\omega}^Y, \\mathbf{x} \\rangle) \\\\\n\\end{array}\\right],\n\\]\nNote that both these random features generate \\(2Y\\)-dimensional feature map \\(\\Phi(\\mathbf{x}) \\in \\mathbb{R}^{2Y}\\) with \\(Y\\) random features \\(\\boldsymbol{\\omega}^i, i \\in [\\![ Y ]\\!]\\). We implement the random features with trigonometric functions below.\n\ndef sin_cos_phi(\n    x: Float[Array, \"... D\"],\n    RF: Float[Array, \"Y D\"],\n    beta: float\n) -&gt; Float[Array, \"... 2Y\"]:\n    \"\"\"\n    Random features with trigonometric function\n    \"\"\"\n    Y = RF.shape[0]\n    h = jnp.sqrt(beta) * (x @ RF.T)\n    return 1 / jnp.sqrt(Y) * jnp.concatenate(\n        [ jnp.cos(h), jnp.sin(h)], axis=-1\n    )\n\n\nApproximating the Kernel Value\nHere we will visualize the quality of the kernel value approximations obtained with the trigonometric random features. We randomly generate samples \\(\\mathbf{x} \\in \\mathbb{R}^D\\) from the domain, and compare their kernel values with the memories \\(\\boldsymbol{\\xi}^\\mu\\). That is, we compare the true RBF kernel value \\(\\kappa(\\sqrt{\\beta}\\mathbf{x}, \\sqrt{\\beta}\\boldsymbol{\\xi}^\\mu) = \\exp(-\\frac{\\beta}{2} \\Vert \\mathbf{x} - \\boldsymbol{\\xi}^\\mu \\Vert^2)\\) to approximated kernel value using the random features \\(\\left\\langle \\Phi(\\sqrt{\\beta}\\mathbf{x}), \\sqrt{\\beta}\\Phi(\\boldsymbol{\\xi}^\\mu) \\right\\rangle\\) for different values of \\(\\beta\\), \\(\\mu \\in [\\![ K ]\\!]\\) and different random samples \\(\\mathbf{x}\\).\nIn the following, we compute the RBF kernel \\(\\kappa(\\sqrt{\\beta}\\mathbf{x}, \\sqrt{\\beta}\\boldsymbol{\\xi}^\\mu) = \\exp(-\\frac{\\beta}{2} \\Vert \\mathbf{x} - \\boldsymbol{\\xi}^\\mu \\Vert^2)\\).\n\ndef rbfkernel(\n    x: Float[Array, \"D\"],\n    y: Float[Array, \"D\"],\n    beta: float,\n) -&gt; Float[Array, \"\"]:\n    \"\"\"\n    Compute the standard RBF kernel\n    between two vectors.\n    \"\"\"\n    return jnp.exp(\n        -0.5 * beta * ((x - y) ** 2).sum()\n    )\n\nWe generate \\(Y=2^{15}\\) random features for the memories \\(\\boldsymbol{\\Xi}\\) using the sin_cos_phi function.\n\nY = np.power(2, 15)\nrngidx = 4\nRF = jr.normal(rnglist[rngidx], (Y, D))\nphi_Xi = sin_cos_phi(Xi, RF, 1.0)\n\nNow we generate NRANDS=100 random \\(\\mathbf{x} \\in \\mathbb{R}^D\\) and compare their exact and approximate kernel values with the memories.\n\n\nComputing and comparing exact and approximate random-feature-based kernel function values.\nrngidx = 9\nNRANDS = 100\nrxs = jr.uniform(\n    rnglist[rngidx], (NRANDS, D)\n) * 2 * maxabs - maxabs\n\nfig, axs = plt.subplots(\n    1, len(betas), figsize=(\n        len(betas) * figscaler, \n        figscaler\n    ),\n    sharex=True, sharey=True\n)\nfor b, ax in tqdm(\n    zip(betas, axs), total=len(betas),\n    colour=TQDMCOLOR, ncols=50\n):\n    phi_rxs = sin_cos_phi(rxs, RF, b)\n    true_kvals = np.array([\n        rbfkernel(x, y, b)\n        for x in rxs for y in Xi\n    ])\n    approx_kvals = np.array([\n        x.dot(y) \n        for x in phi_rxs for y in phi_Xi\n    ])\n    ax.scatter(true_kvals, approx_kvals)\n    ax.set_xlabel(\n        r\"$\\kappa(\\mathbf{x},$\" \n        + r\"$\\boldsymbol{\\xi}^\\mu)$\"\n    )\n    ax.set_title(r\"$\\beta$\" + f\":{b:0.2f}\")\n    ax.axis('square')\naxs[0].set_ylabel(\n    r\"$\\langle \\Phi(\\mathbf{x}),$\" \n    + r\"$\\Phi(\\boldsymbol{\\xi}^\\mu) \\rangle$\"\n)\nfig.tight_layout()\nplt.show()\n\n\n\n100%|███████████████| 4/4 [00:00&lt;00:00,  4.12it/s]\n\n\n\n\n\n\n\n\n\n\n\nIn the above figure, a good approximation would put all the points in the scatter plots on the diagonal. Based on these figures, we make the following observations with a fixed value of \\(D, Y\\): - Small values of \\(\\beta\\) push the RBF kernel values close to 1, and the random features do not approximate these values well, generally significantly underestimating the kernel values as all the points in the scatter plots concentrate in the lower corner. - As \\(\\beta\\) grows, the approximation quality improves, with \\(\\beta = 1\\) producing really good approximation of the exact kernel values. The true kernel values better span the range of \\([0, 1]\\) and the random features produce high quality approximations. - When \\(\\beta\\) increases beyond a point, most pairwise RBF kernel values go close to zero, and approximation quality again falls.\nNote that overall performance will continue to improve as the ratio \\(D/Y\\) decreases for any given value of \\(\\beta\\). But it is a known issue that the trigonometric random features do not approximate kernel values close to zero or close to one very well.\n\n\nVisualizing the Random Features\nHere we visualize the first NRFS=6 of the random features across the data domain for varying values of \\(\\beta\\). The use of the trigonometric function is visible through the periodic nature of these random features, where larger values of \\(\\beta\\) lead to shorter periods.\n\n\nVisualizing a subset of the random features over the input domain.\nNRFS = 6\nfig, axs = plt.subplots(\n    NRFS, len(betas), figsize=(\n        len(betas) * figscaler, \n        NRFS * figscaler\n    ),\n    sharex=True, sharey=True\n)\nfor bidx, b in tqdm(\n    enumerate(betas),\n    total=len(betas),\n    colour=TQDMCOLOR,\n    ncols=50\n):\n    rfs = np.zeros(\n        [2*NRFS, V.shape[1], V.shape[2]]\n    )\n    for i in range(nsteps):\n        rfs[:, i, :] = sin_cos_phi(\n            V[:, i, :].T, RF[:NRFS, :], b\n        ).T\n    for i in range(NRFS):\n        plot_energy_landscape(\n            rfs[i, :, :], axs[i, bidx], \n            np.array([\n                xmin, xmax, ymin, ymax\n            ]),\n            colormap='YlGn'\n        )\n        plot_states(\n            Xi, axs[i, bidx],\n            marker='*', color=MCOLOR)\n    axs[0, bidx].set_title(\n        r\"$\\beta$\" + f\":{b:0.2f}\"\n    )\nplt.show()\n\n\n\n100%|███████████████| 4/4 [00:00&lt;00:00, 10.49it/s]",
    "crumbs": [
      "tutorial",
      "Distributed Memory"
    ]
  },
  {
    "objectID": "tutorial/distributed_memory.html#approximating-the-energy-with-random-features",
    "href": "tutorial/distributed_memory.html#approximating-the-energy-with-random-features",
    "title": "Distributed Memory",
    "section": "Approximating the Energy with Random Features",
    "text": "Approximating the Energy with Random Features\nGiven the random features \\(\\Phi: \\mathbb{R}^D \\to \\mathbb{R}^{2Y}\\), we can approximate the energy as\n\\[\\begin{align}\nE_\\beta( \\mathbf{v}; \\boldsymbol{\\Xi} )\n& = - \\frac{1}{\\beta} \\log \\sum_{\\mu = 1}^K\n  \\exp \\left(- \\frac{\\beta}{2} \\left\\Vert \\mathbf{v} - \\boldsymbol{\\xi}^\\mu \\right \\Vert^2 \\right)\n\\\\\n& \\approx - \\frac{1}{\\beta} \\log \\sum_{\\mu = 1}^K \\left\\langle \\Phi(\\mathbf{v}), \\Phi(\\boldsymbol{\\xi}^\\mu) \\right\\rangle\n= - \\frac{1}{\\beta} \\log \\left\\langle \\Phi(\\mathbf{v}), \\sum_{\\mu = 1}^K \\Phi(\\boldsymbol{\\xi}^\\mu) \\right\\rangle\n\\\\\n& = - \\frac{1}{\\beta} \\log \\left\\langle \\Phi(\\mathbf{v}), \\mathbf{T} \\right\\rangle,\n\\qquad \\text{with} \\quad\n\\mathbf{T} = \\sum_{\\mu = 1}^K \\Phi(\\boldsymbol{\\xi}^\\mu).\n\\end{align}\\]\nNow we can define an approximate random-feature-based energy function \\(\\tilde{E}_\\beta (\\mathbf{v}; \\mathbf{T}) \\approx E_\\beta (\\mathbf{v}; \\boldsymbol{\\Xi})\\) as follows:\n\\[\n\\tilde{E}_\\beta (\\mathbf{v}; \\mathbf{T}) = - \\frac{1}{\\beta} \\log \\left\\langle \\Phi(\\mathbf{v}), \\mathbf{T} \\right\\rangle,\n\\qquad \\text{with} \\quad\n\\mathbf{T} = \\sum_{\\mu = 1}^K \\Phi(\\boldsymbol{\\xi}^\\mu)\n\\tag{2}\\]\nWe implement this approximate energy below using random features below given the \\(\\{ \\boldsymbol{\\omega}^i, i \\in [\\![ Y ]\\!] \\}\\) and the distributed memories \\(\\mathbf{T} \\in \\mathbb{R}^{2Y}\\).\n\ndef approx_lse_energy(\n    state: Float[Array, \"... D\"],\n    RF: Float[Array, \"Y D\"],\n    beta: float,\n    T: Float[Array, \"2Y\"],\n    eps=1e-10\n) -&gt; Float[Array, \"...\"]:\n    \"\"\"\n    Compute the approx energy with\n    random features\n    \"\"\"\n    h = sin_cos_phi(state, RF, beta) @ T \n    h = jnp.clip(h,  a_min=eps)\n    return -(1 / beta) * jnp.log(h)\n\nHere we compare the exact energy landscape to energy landscape approximated with random features for varying values of \\(\\beta\\) given the set of memories \\(\\boldsymbol{\\Xi}\\). For a given value of \\(\\beta\\), we first compute \\(\\mathbf{T} = \\sum_{\\mu=1}^K \\Phi(\\sqrt{\\beta} \\boldsymbol{\\xi}^\\mu )\\), and the use it to compute the approximate energy landscape.\nThe first row of the plots show the (cached) true energy landscape, and the second row shows the energy landscape induced by the approximate energy computed using the distributed memories. Note that we highlight the original memories in the first row of the plots with the true energy landscape.\n\n\nComputing and visualizing the approximate energy landscape, and comparing it to the exact energy landscape.\nfig, axs = plt.subplots(\n    2, len(betas), figsize=(\n        len(betas) * figscaler, \n        2 * figscaler\n    ),\n    sharex=True, sharey=True\n)\nfor bidx, b in tqdm(\n    enumerate(betas), total=len(betas),\n    colour=TQDMCOLOR, ncols=50\n):\n    # Computing the T tensor, \n    # summing the random features\n    # over all memories\n    T_Xi = sin_cos_phi(Xi, RF, b).sum(0)\n    # Computing the approx energy \n    # over the domain\n    app_en = np.zeros_like(V[0])\n    for i in range(nsteps):\n        app_en[i, :] = approx_lse_energy(\n            V[:, i, :].T, RF, b, T_Xi\n        )\n    # Plotting the exact and approx energy\n    plot_energy_landscape(\n        beta_en_cache[b], axs[0, bidx],\n        np.array([xmin, xmax, ymin, ymax])\n    )\n    plot_states(\n        Xi, axs[0, bidx],\n        marker='*', color=MCOLOR\n    )\n    plot_energy_landscape(\n        app_en, axs[1, bidx],\n        np.array([xmin, xmax, ymin, ymax])\n    )\n    axs[0, bidx].set_title(\n        r\"$\\beta$\" + f\":{b:0.2f}\"\n    )\nplt.show()\n\n\n\n100%|███████████████| 4/4 [00:02&lt;00:00,  1.79it/s]\n\n\n\n\n\n\n\n\n\n\n\nFor small values of \\(\\beta\\), the exact and approximate energy landscapes appear visually similar. However, for larger values of \\(\\beta\\), the energy landscapes start differing significantly, especially farther away from the memories. However, note how the approximate energy still forms a local minima around each of the original memories even though the actual basins of attraction of this approximate energy are significantly smaller. For \\(\\beta = 10\\), there are 8 local minima, matching the total number of \\(K=8\\) original memories.\n\nApproximate Energy Descent\nFor a state vector \\(\\mathbf{v} \\in \\mathbb{R}^D\\), we can approximately reduce its energy \\(E_\\beta(\\mathbf{v}; \\boldsymbol{\\Xi})\\) by utilizing the gradient \\(\\nabla_{\\mathbf{v}} \\tilde{E}_\\beta(\\mathbf{v}; \\mathbf{T})\\) of the random-feature based approximate energy \\(\\tilde{E}_\\beta(\\mathbf{v}; \\mathbf{T})\\). Initializing the energy descent at the input \\(\\mathbf{q}\\), that is \\(\\tilde{\\mathbf{v}}^{(0)} \\gets \\mathbf{q}\\), we perform the following gradient descent steps for \\(T\\) iterations: \\[\n\\tilde{\\mathbf{v}}^{(t)} \\gets \\tilde{\\mathbf{v}}^{(t-1)} - \\alpha \\nabla_{\\mathbf{v}} \\tilde{E}_\\beta( \\tilde{\\mathbf{v}}^{(t-1)}; \\mathbf{T} ),\n\\] for \\(t = 1, \\ldots, T\\) with \\(\\alpha &gt; 0\\) as the step-size (or learning rate) for the energy descent. The final \\(\\tilde{\\mathbf{v}}^{(T)}\\) is the output of this model. This output will be different than the output \\(\\mathbf{v}^{(T)}\\) obtained using the exact energy gradient \\(\\nabla_{\\mathbf{v}} E_\\beta( \\mathbf{v}; \\boldsymbol{\\Xi} )\\).\nThe gradient of the approximate energy in Equation 2 does not require access to the original memories \\(\\boldsymbol{\\Xi}\\), and can be computed solely using the random features \\(\\{ \\boldsymbol{\\omega}^i, i \\in [\\![ Y ]\\!] \\}\\) and the consolidated memories \\(\\mathbf{T} \\in \\mathbb{R}^{2Y}\\): \\[\n\\nabla_{\\mathbf{v}} \\tilde{E}_\\beta ( \\mathbf{v}; \\mathbf{T} )\n= - \\frac{1}{\\beta} \\nabla_{\\mathbf{v}} \\log \\left\\langle \\Phi(\\mathbf{v}), \\mathbf{T} \\right\\rangle\n= - \\frac{1}{\\beta \\left\\langle \\Phi(\\mathbf{v}), \\mathbf{T} \\right\\rangle} \\left[ \\nabla_{\\mathbf{v}} \\Phi(\\mathbf{v}) \\right]^\\top \\mathbf{T}.\n\\] The gradient \\(\\nabla_{\\mathbf{v}} \\Phi(\\mathbf{v})\\) of the random feature map \\(\\Phi: \\mathbb{R}^D \\to \\mathbb{R}^{2Y}\\) with respect to its input is a \\((2Y \\times D)\\) matrix.\nWe can implement this using auto-differentiation in JAX by computing the gradient of the approx_lse_energy function with respect to its input state.\n\ndef approx_lse_energy_descent(\n    q: Float[Array, \"D\"],\n    RF: Float[Array, \"Y D\"],\n    beta: float,\n    T: Float[Array, \"2Y\"],\n    energy_fn,\n    depth: int=10,\n    alpha: float = 0.01, \n    return_grads=False,\n    clamp_idxs: Optional[Bool[Array, \"D\"]]=None\n) -&gt; Float[Array, \"D\"]: \n    \"\"\"\n    Using the approx random feature energy.\n    run energy descent\n    \"\"\"\n    dEdxf = jax.jit(\n        jax.value_and_grad(energy_fn)\n    )\n    logs = {}\n    @jax.jit\n    def step(x, i):\n        E, dEdx = dEdxf(x, RF, beta, T)\n        if clamp_idxs is not None:\n            dEdx = jnp.where(clamp_idxs, 0, dEdx)\n        x = x - alpha * dEdx\n        aux = (E, dEdx) if return_grads else (E,)\n        return x, aux\n    x, aux = jax.lax.scan(\n        step, q, jnp.arange(depth)\n    )\n    logs['energies'] = aux[0]\n    if return_grads:\n        logs['grads'] = aux[1]\n    return x, logs\n\nWe will now compare the energy descent dynamics of this gradient using the random-feature based approximate energy to the exact energy gradient from earlier. We keep the step-size \\(\\alpha\\) and the number of DenseAM layers \\(T\\) the same as the exact energy descent with \\(\\alpha = 0.01\\) and \\(T = 1000\\). For each of the intermediate states obtained with this approximate energy descent, we compute the exact energy to check how it decreases through the distributed-memory DenseAM layers.\nWe will use the cached energy landscapes and the cached intermediate states and energies for the exact energy descent to highlight the similarities and differences. The intermediate states for the three queries with exact energy descent will be shown with the \\(\\textcolor{blue}{\\bullet}\\) symbol, while the intermediate states with the approximate random-features-based energy will be show with the \\(\\textcolor{orange}{\\bullet}\\) symbol.\n\n\nPerforming and visualizing the exact & approximate energy-descent for three randomly generated queries.\nrngidx = 8\nfig, axs = plt.subplots(\n    NQUERIES+1, len(betas), figsize=(\n        len(betas) * figscaler, \n        NQUERIES+2 * figscaler\n    ),\n    sharex=\"row\",\n)\nfor bidx, b in tqdm(\n    enumerate(betas), total=len(betas),\n    colour=TQDMCOLOR, ncols=50\n):\n    # using cached energy landscape\n    plot_energy_landscape(\n        beta_en_cache[b], axs[0, bidx],\n        np.array([xmin, xmax, ymin, ymax])\n    )\n    plot_states(\n        Xi, axs[0, bidx],\n        marker='*', color=MCOLOR\n    )\n    axs[0, bidx].set_title(\n        r\"$\\beta$\" + f\":{b:0.2f}\"\n    )\n    # Computing the T tensor, \n    # summing the random features\n    # over all memories\n    T_Xi = sin_cos_phi(Xi, RF, b).sum(0)\n    # Randomly generating queries\n    queries = jr.uniform(\n        rnglist[rngidx], (NQUERIES, D)\n    ) * 2 * maxabs - maxabs\n    beta_cache = []\n    for qidx, query in enumerate(queries):    \n        qstates = [query]\n        qens = []\n        # Perform energy descent using \n        # the approx gradient\n        for i in range(NSTATES):\n            query, logs = approx_lse_energy_descent(\n                query, RF, b, T_Xi, approx_lse_energy,\n                depth=NUPDATES, alpha=ALPHA\n            )\n            qstates += [query]\n            qens += [logs['energies']]\n        # using cached exact descent stats\n        ex_qstates, ex_qens = beta_true_states_en_cache[b][qidx]\n        plot_states(\n            ex_qstates, axs[0, bidx],\n            marker='o', color=QCOLOR\n        )\n        plot_energy_descent(\n            ex_qens, axs[qidx+1, bidx],\n            color=QCOLOR\n        )\n        axs[qidx+1, bidx].set_title(\n            f\"Query {qidx+1}\"\n        )\n        qstates = np.array(qstates)\n        plot_states(\n            qstates, axs[0, bidx],\n            marker='.', color=AQCOLOR\n        )\n        qens = np.array([\n            [lse_energy(qs, Xi, b)]*NUPDATES\n            for qs in qstates\n        ]).reshape(-1)\n        plot_energy_descent(\n            qens, axs[qidx+1, bidx], \n            color=AQCOLOR\n        )\nfig.tight_layout()\nplt.show()\n\n\n\n100%|███████████████| 4/4 [00:12&lt;00:00,  3.07s/it]\n\n\n\n\n\n\n\n\n\n\n\nThe above results show that, for small to moderately large \\(\\beta\\), with sufficiently large number of random features \\(Y\\), the gradient of the random-feature based approximate energy matches the dynamics of the exact energy gradient. See how the \\(\\textcolor{orange}{\\bullet}\\) symbols for the approximate energy gradient are completely overlapping with \\(\\textcolor{blue}{\\bullet}\\) symbol for the exact energy descent. However, for large \\(\\beta\\), the gradient of the approximate energy is no longer able to reduce the energy of the initial state if the initial state happens to be quite far from all the memories, implying a large initial energy \\(E_\\beta ( \\mathbf{v}^{(0)}; \\boldsymbol{\\Xi} )\\). Note that one or the three queries is able to reduce its energy and match the exact energy descent.\nOur previous results showed that the random-feature based kernel approximation does not perform well if \\(\\beta\\) is too small, or too large. However, the approximate kernel based energy gradient is sufficient for low \\(\\beta\\) regime, highlighting that energy descent is possible even if the kernel approximation is poor. This is because the energy of the initial state at low \\(\\beta\\) is already quite low.\nMore precisely, we can bound the divergence between the output of the exact model with memory representation \\(f_{\\boldsymbol{\\Xi}}( \\mathbf{q} ) = \\mathbf{v}^{(T)}\\) and the output of the approximate model with distributed representations \\(f_{\\mathbf{T}}( \\mathbf{q} ) = \\tilde{\\mathbf{v}}^{(T)}\\) under the following conditions: - For any \\(\\mathbf{x}, \\mathbf{x}' \\in \\mathbb{R}^D\\), there is a universal constant \\(C_1 &gt; 0\\) such that \\[\\left| \\kappa(\\mathbf{x}, \\mathbf{x}') - \\left\\langle \\Phi(\\mathbf{x}) , \\Phi(\\mathbf{x}') \\right\\rangle \\right| \\leq C_1 \\sqrt{\\frac{D}{Y}}.\\] - The step-size \\(\\alpha\\) is selected, such that, for a universal constant \\(C_2 \\in (0, 1)\\) \\[ \\alpha \\leq \\frac{C_2}{T (1 + 2K \\beta \\exp(\\beta/2))}.\\]\nThen the divergence is bounded as see [1], Corollary 1: \\[\n\\left\\Vert f_{\\boldsymbol{\\Xi}}(\\mathbf{q}) - f_{\\mathbf{T}}(\\mathbf{q}) \\right\\Vert\n= \\left\\Vert \\mathbf{v}^{(T)} - \\tilde{\\mathbf{v}}^{(T)} \\right\\Vert\n\\leq \\frac{C_1 C_2 \\exp(E_\\beta(\\mathbf{q}; \\boldsymbol{\\Xi}) - 1/2)}{\\beta (1 - C_2)}\n\\]\nWe can also show a more general result without the restriction on the step-size \\(\\alpha\\) see [1], Theorem 1.\n\n\nDrDAM class\nWe can put together Distributed representation DenseAM or DrDAM into a single class for convenience.\n\nclass DrDAM:\n    \"\"\"\n    DenseAM through the Lens of Random Features\n    \"\"\"\n    def __init__(self, key, D, Y, beta):\n        self.RF = jr.normal(key, (Y, D))\n        self.beta = beta\n        self.Y = Y\n        self.Tdim = 2*Y\n        self.D = D\n\n    def phi(\n        self, x: Float[Array, \"... D\"]\n    ) -&gt; Float[Array, \"... 2Y\"]:\n        \"\"\"Compute the random features \"\"\"\n        return sin_cos_phi(x, self.RF, self.beta)\n\n    def sim(\n        self, x: Float[Array, \"D\"],\n        y: Float[Array, \"D\"]\n    ) -&gt; Float[Array, \"\"]:\n        \"\"\"\n        Compute the exact RBF kernel for two vectors\n        \"\"\"\n        return rbfkernel(x, y, self.beta)\n\n    def energy(\n        self, x: Float[Array, \"D\"],\n        memories: Float[Array, \"M D\"]\n    ) -&gt; Float[Array, \"\"]:\n        \"\"\"Compute the standard LSE energy\"\"\"\n        return lse_energy(x, memories, self.beta)\n\n    def rf_approx_energy(\n        self, x: Float[Array, \"D\"],\n        T: Float[Array, \"2Y\"], eps=1e-10\n    ) -&gt; Float[Array, \"\"]:\n        \"\"\"\n        Compute the approx LSE energy with random features\n        \"\"\"\n        return approx_lse_energy(x, self.RF, self.beta, T)\n    \n    def rf_approx_sim(\n        self, x: Float[Array, \"D\"],\n        y: Float[Array, \"D\"]\n    ) -&gt; Float[Array, \"\"]:\n        \"\"\"Compute the approx RBF kernel for two vector\"\"\"\n        return self.phi(x) @ self.phi(y)\n\n    def dist_memories(\n        self, memories: Float[Array, \"M D\"]\n    ) -&gt; Float[Array, \"2Y\"]:\n        \"\"\"\n        Compute the random-feature based distributed\n        representation of the memories\n        \"\"\"\n        return self.phi(memories).sum(0)\n\n    def energy_descent( \n        self, q: Float[Array, \"D\"], \n        memories: Float[Array, \"M D\"], \n        depth: int=1000, alpha: float = 0.1,\n        return_grads=False, \n        clamp_idxs: Optional[Bool[Array, \"D\"]]=None\n    ) -&gt; Float[Array, \"D\"]: \n        \"\"\"Run exact energy descent\"\"\"\n        return lse_energy_descent( \n            q, memories, self.beta, lse_energy,\n            depth, alpha, return_grads, clamp_idxs\n        )\n\n    def rf_approx_energy_descent(\n        self, q: Float[Array, \"D\"],\n        T: Float[Array, \"2Y\"], \n        depth: int=1000, alpha: float = 0.1,\n        return_grads=False,\n        clamp_idxs: Optional[Bool[Array, \"D\"]]=None\n    ) -&gt; Float[Array, \"D\"]: \n        \"\"\"Run approx energy descent\"\"\"\n        return approx_lse_energy_descent(\n            q, self.RF, self.beta, T, approx_lse_energy,\n            depth, alpha, return_grads, clamp_idxs\n        )\n\nWe will demonstrate the use of this class with \\(K = 20\\) memories in \\(D=10\\) dimensions and \\(Y=10^4\\) random features. We will use the LSE energy with an inverse-temperature \\(\\beta=25\\) and create an instance of the DrDAM class.\n\nrngidx = 9\nD = 30\nY = 100_000\nn_memories = 20\nn_queries = 100\nbeta = 25\nkdam = DrDAM(\n    rnglist[rngidx], D=D, Y=Y, beta=beta\n)\n\nComparing the exact and approximate RBF kernel values for a pair of points.\n\nrngidx = 0\nxpair = (\n    jr.uniform(rnglist[rngidx], (D,2 )) &gt; 0.5\n) / jnp.sqrt(D)\nprint(\n    f\"Exact RBF kernel value: \"\n    f\"{kdam.sim(xpair[:, 0], xpair[:, 1]):0.4f}\"\n)\nprint(\n    f\"Approx RBF kernel value: \"\n    f\"{kdam.rf_approx_sim(xpair[:, 0], xpair[:, 1]):.04f}\"\n)\n\nExact RBF kernel value: 0.0013\nApprox RBF kernel value: 0.0030\n\n\nGenerating some memories, and their distribution representation along with some random initial states.\n\nrngidx = 2\nmemories = (\n    jr.uniform(rnglist[rngidx], (n_memories, D)) &gt; 0.5\n) / jnp.sqrt(D)\nrngidx = 6\nqueries = (\n    jr.uniform(rnglist[rngidx], (n_queries, D)) &gt; 0.5\n) / jnp.sqrt(D)\nprint(\n    f\"Generated {memories.shape[0]} memories\"\n    f\" in {memories.shape[1]} dimensions\"\n)\nT = kdam.dist_memories(memories)\nprint(\n    f\"Distributed representation of \"\n    f\"these memories in {T.shape[0]} dimensions\"\n)\nprint(\n    f\"Generated {queries.shape[0]} initial \"\n    f\"states in {queries.shape[1]} dimensions\"\n)\n\nGenerated 20 memories in 30 dimensions\nDistributed representation of these memories in 200000 dimensions\nGenerated 100 initial states in 30 dimensions\n\n\nWe will compare the exact and the approximate energy using the distributed memory\n\nprint(\n    f\"Exact energy for a point: \"\n    f\"{kdam.energy(xpair[:, 0], memories):0.4f}\"\n)\nprint(\n    f\"Approx energy for the same point: \"\n    f\"{kdam.rf_approx_energy(xpair[:, 0], T):0.4f}\"\n)\n\nExact energy for a point: 0.0847\nApprox energy for the same point: 0.0886\n\n\nThis is a comparison of the exact and approximate energies of all initial states. Better approximation is denoted by the points on the scatter plot lying on the diagonal.\n\nexact_energies = jnp.array([\n    kdam.energy(q, memories).item() \n    for q in queries\n])\nrf_approx_energies = kdam.rf_approx_energy(\n    queries, T\n)\nplt.figure(figsize=(4,4))\nplt.scatter(\n    exact_energies,\n    rf_approx_energies\n)\nplt.xlabel(\n    \"Exact Energy \" + \n    r\"$E_\\beta(\\mathbf{q}; \\boldsymbol{\\Xi})$\"\n)\nplt.ylabel(\n    \"Approx Energy \" + \n    r\"$\\tilde{E}_\\beta(\\mathbf{q}; \\mathbf{T})$\"\n)\nplt.axis('square')\nplt.title(\"Exact energy vs Approx energy\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFor 10 queries, we will perform the inference through a \\(T=10\\) layer DenseAM and compute and report the divergence between the exact energy descent model and the distributed memory DenseAM.\n\nfor qidx in range(10):\n    exact_out, _ = kdam.energy_descent(\n        queries[qidx], memories, depth=10, alpha=0.1\n    )\n    approx_out, _ = kdam.rf_approx_energy_descent(\n        queries[qidx], T, depth=10, alpha=0.1\n    )\n    print(\n        f\"Initial state {qidx+1}: \"\n        f\" Initial energy: \"\n        f\"{kdam.energy(queries[qidx], memories):0.4f}, \"\n        f\"Divergence in the output: \"\n        f\"{jnp.sqrt(((exact_out - approx_out)**2).sum()):0.4f}\"\n    )\n\nInitial state 1:  Initial energy: 0.0950, Divergence in the output: 0.0238\nInitial state 2:  Initial energy: 0.1262, Divergence in the output: 0.0269\nInitial state 3:  Initial energy: 0.1140, Divergence in the output: 0.0436\nInitial state 4:  Initial energy: 0.0997, Divergence in the output: 0.0285\nInitial state 5:  Initial energy: 0.1023, Divergence in the output: 0.0306\nInitial state 6:  Initial energy: 0.1107, Divergence in the output: 0.0352\nInitial state 7:  Initial energy: 0.0991, Divergence in the output: 0.0269\nInitial state 8:  Initial energy: 0.1278, Divergence in the output: 0.0383\nInitial state 9:  Initial energy: 0.1082, Divergence in the output: 0.0325\nInitial state 10:  Initial energy: 0.1035, Divergence in the output: 0.0356",
    "crumbs": [
      "tutorial",
      "Distributed Memory"
    ]
  },
  {
    "objectID": "tutorial/energy_transformer.html#flow-perspective-of-transformers",
    "href": "tutorial/energy_transformer.html#flow-perspective-of-transformers",
    "title": "Energy Transformer",
    "section": "Flow perspective of Transformers",
    "text": "Flow perspective of Transformers\n\nSquint, and the Transformer looks like a dynamical system.\n\nAt its core, the transformer is a stack of \\(L\\) transformer blocks that takes a length \\(N\\) sequence of input tokens \\(\\{\\mathbf{x}^{(0)}_1, \\ldots, \\mathbf{x}^{(0)}_N\\}\\) and outputs a length \\(N\\) sequence of output tokens \\(\\{\\mathbf{x}^{(L)}_1, \\ldots, \\mathbf{x}^{(L)}_N\\}\\). Each token \\(\\mathbf{x}^{(l)}_i \\in \\mathbb{R}^D\\) is a vector of dimension \\(D\\).\nWhen blocks are stacked, the residual connections form a “residual highway” that consists entirely of normalizations and additions from Attention and MLP operations.\n\n\n\n\n\n\nFigure 1: A vanilla Transformer Block consisting of 4 main operations: (multi-headed) attention, MLP, (pre-)layernorms, and residual connections. The Transformer is a stack of these blocks, which we show depicted as a “residual highway” design. The residual highway showcases how each block “perturbs” its input, and the mathematical operation looks like a dynamical system. If the system can be described such that the operation of each block is a gradient descent, the system becomes an energy-based model.\n\n\n\nAssociative Memory (AM) requires a global energy function, where each computation minimizes the total energy of the system. Our goal is to derive an energy function whose gradient looks as much like the Transformer block as possible.\n\n\n\nThe Energy Transformer block, shown as the derivative of its energy. Attention and Hopfield Network (symmetric MLP) updates are computed in parallel. Updates are added to the input via a residual connection that is a byproduct of ET describing a dynamical system.",
    "crumbs": [
      "tutorial",
      "Energy Transformer"
    ]
  },
  {
    "objectID": "tutorial/energy_transformer.html#introducing-energy-into-the-transformer",
    "href": "tutorial/energy_transformer.html#introducing-energy-into-the-transformer",
    "title": "Energy Transformer",
    "section": "Introducing Energy into the Transformer",
    "text": "Introducing Energy into the Transformer\nWe will now build a kind of associative memory called the “Energy Transformer” [1] that turns the familiar transformer operation into an energy minimization. Energy Transformer (ET) defines a single energy on an \\(\\mathbf{x} \\in \\mathbb{R}^{N \\times D}\\) collection of tokens, where we can think of each token \\(\\mathbf{x}_B\\) as a “particle” that knows some information about itself and needs to figure out what it should become. Some particles (unmasked tokens) already know their identity, while others (masked tokens) only know their position and must discover their identity by interacting with their neighbors.\nMinimizing the energy of the Energy Transformer (ET) is a recurrent process. The entire transformer consists of a single Transformer block, and each “layer” of the transformer becomes a gradient descent step down the energy. This gradient descent step looks remarkably like a standard transformer block, complete with attention, MLP-like operations, layer normalizations, and residual connections.\nThe global energy combines two intuitive ideas: (1) attention energy that encourages masked tokens to align with relevant unmasked tokens, and (2) memory energy that ensures all tokens look like realistic patterns the model has learned. The gradient of each of these energies look like a self-attention and MLP, respectively, with some shared weight constraints.\nThis is one of those situations where the code ends up being significantly simpler than the equations. We write the equations for completeness, but feel free to skip to Section 2.3 for succinct code.\n\nAttention Energy\nWe describe the energy of a multi-headed attention with \\(H\\) heads, where the \\(h\\)-th head of attention is parameterized by \\(\\mathbf{W}_h^Q, \\mathbf{W}_h^K \\in \\mathbb{R}^{D \\times Y}\\), where \\(Y\\) is the “head dimension”. The input to the attention is the normalized token vectors \\(\\hat{\\mathbf{x}} \\in \\mathbb{R}^{N \\times D}\\). In the math that follows, we index the heads by \\(h=1\\ldots H\\), the head dimension by \\(\\alpha=1\\ldots Y\\), tokens by \\(A,B,C=1 \\ldots N\\), and each token vector by \\(i,j=1\\ldots D\\).\n\n\n\n\n\n\nEinstein notation\n\n\n\nWe find it convenient to use Einstein notation for the math, since it maps 1:1 to the einops operations we’ll use in the code. If you aren’t familiar with the notation, check out this awesome tutorial. But fair warning, the equations at first look pretty complicated with all the indices.\nOne tip for reading equations with lots of indices: you don’t need to remember the shape or order of tensors, just remember the meaning of the indices. The number of subscripts is the number of dimensions of the tensor, and the meaning of each dimension is captured in the index name. For example, let \\(B=1\\ldots N\\) index the token position in a sequence, and let \\(i=1\\ldots D\\) index into each token vector. \\(x_{Bi}\\) is an element of a 2-dimensional tensor capturing the sequence length \\(N\\) and token dimension \\(D\\). Transposes don’t have meaning since things are named, so \\(x_{Bi} = x_{iB}\\). So long as you know the index semantics, you can read always read the equation. Everything is just scalar multiplication and addition.\n\n\nThe familiar queries and keys are computed as normal linear transformations:\n\\[\n   \\begin{split}\n        K_{h \\alpha B} &= \\sum\\limits_j W^K_{h \\alpha j}\\; \\hat{x}_{Bj}, \\qquad \\mathbf{K} \\in \\mathbb{R}^{H \\times Y \\times N} \\\\\n        Q_{h \\alpha C} &= \\sum\\limits_j W^Q_{h \\alpha j}\\; \\hat{x}_{Cj}, \\qquad \\mathbf{Q} \\in \\mathbb{R}^{H \\times Y \\times N}\n    \\end{split}\n\\]\nOur familiar “raw attention scores” (pre-softmax) are still the dot-product correlations between each query and key:\n\\[\nA_{hBC} = \\sum_{\\alpha} K_{h\\alpha B} Q_{h\\alpha C}\n\\]\nNow for the different part: we describe the energy of the attention as the negative log-sum-exp of the attention scores. We will use the \\(\\beta\\) as an inverse-temperature hyperparameter to scale the attention scores.\n\\[\nE^\\text{ATT} = -\\frac{1}{\\beta} \\sum_{h=1}^H \\sum_{C=1}^N \\log \\left( \\sum_{B \\neq C} \\exp(\\beta A_{hBC}) \\right)\n\\tag{1}\\]\nAs we saw in a previous notebook, the negative log-sum-exp is an exponential variation of the Dense Associative Memory. The cool thing is that the gradient of the negative log-sum-exp is the softmax, which is what we’d like to see in the attention update rule.\n\n\n\n\n\n\nWhere are our values?\n\n\n\nYou may recall that traditional attention also has a value matrix. When we take the gradient of Equation 1, we lose the flexibility to include an independently parameterized values: the values must be a function of the queries and the keys.\n\n\n\n\nMemory Energy\nIn traditional transformers, the MLP (without biases) can be written as a two-layer feedforward network with a ReLU on the hidden activations. The MLP is parameterized by two weight matrices \\(\\mathbf{V}, \\mathbf{W} \\in \\mathbb{R}^{M \\times D}\\) where \\(M\\) is the size of the hidden layer (\\(M=4D\\) is often viewed as the default expansion factor atop token dimension \\(D\\)). Let’s again use Einstein notation, where \\(\\mu=1\\ldots M\\) indexes the hidden units, \\(i,j=1\\ldots D\\) index the token dimensions, and \\(B=1\\ldots N\\) indexes each token.\n\\[\n\\text{MLP}(\\hat{\\mathbf{x}})_{Bi} = \\sum_\\mu W_{\\mu i} \\; \\text{ReLU}\\left(\\sum_j V_{\\mu j} \\hat{\\mathbf{x}}_{Bj}\\right)\n\\tag{2}\\]\nIf we assume weight sharing between \\(\\mathbf{V} = \\mathbf{W} = \\boldsymbol{\\xi}\\), this is a gradient descent step down the energy of a Hopfield Network\n\\[\nE^{\\text{HN}}(\\hat{\\mathbf{x}}) = - \\sum_{B, \\mu} F\\left(\\sum_j \\xi_{\\mu j} \\hat{\\mathbf{x}}_{Bj}\\right)\n\\]\nwith rectified quadratic energy \\(F(\\cdot) := \\frac12 \\text{ReLU}(\\cdot)^2\\). If we say \\(f(\\cdot) := F'(\\cdot) = \\text{ReLU}(\\cdot)\\), the negative gradient of the energy is\n\\[\n-\\frac{\\partial E^{\\text{HN}}(\\mathbf{\\hat{x}})}{\\partial \\hat{x}_{Bi}}\n= \\sum_\\mu \\xi_{\\mu i} \\; f\\left(\\sum_j \\xi_{\\mu j} \\hat{\\mathbf{x}}_{Bj}\\right),\n\\]\nwhich is identical to the MLP operation in Equation 2 with a weight sharing constraint.\n\n\n\n\n\n\nNote\n\n\n\nIt is perfectly reasonable to consider other convex functions \\(F\\) for use in the energy. Polynomials of higher degree \\(n\\) or exponential functions are both valid and will yield Dense Associative Memory. However, because traditional Transformers use a ReLU activation, we use a rectified quadratic energy.\n\n\n\n\nET in code\nLet’s implement the attention energy in code. We will use jax and equinox for our code.\n\n## Uncomment for colab users\n# !pip install amtutorial\n\n\n\nNecessary imports\nimport jax, jax.numpy as jnp, jax.random as jr, jax.tree_util as jtu, jax.lax as lax\nimport equinox as eqx\nfrom dataclasses import dataclass\nfrom typing import *\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport imageio.v2 as imageio\nfrom glob import glob\nfrom fastcore.basics import *\nfrom fastcore.meta import *\nimport matplotlib.pyplot as plt\nfrom jaxtyping import Float, Array\nimport functools as ft\nfrom einops import rearrange\nfrom amtutorial.data_utils import get_et_imgs, get_et_checkpoint\n\n\nThe EnergyTransformer class captures all the token processing in the entire transformer. There are maybe 7 lines of code that perform the actual energy computation. This single energy function, when paired with a layer-norm, is analogous to the full computation across all layers of a traditional transformer. The only things missing are some some token and position embedding matrices to make it work on real data, but we will do that in the following section.\nFirst, let’s describe the configuration for ET:\n\nclass ETConfig(eqx.Module):\n  D: int = 768 # token dimension\n  H: int = 12 # number of heads\n  Y: int = 64 # head dimension\n  M: int = 3072 # MLP size\n  beta: Optional[float] = None # Inverse temperature for attention, defaults to 1/sqrt(Y)\n  prevent_self_attention: bool = True # Prevent explicit self-attention\n  def get_beta(self): return self.beta or 1/jnp.sqrt(self.Y)\n\nsmallETConfig = ETConfig(D=12, H=2, Y=6, M=24)\nmediumETConfig = ETConfig(D=128, H=4, Y=32, M=256)\nfullETConfig = ETConfig(D=768, H=12, Y=64, M=3072, beta=1/jnp.sqrt(64))\n\nThe ETConfig class captures all the dimensions and default hyperparameters for ET. The only thing left to do is implement the energies of Energy Transformer\n\nclass EnergyTransformer(eqx.Module):\n  config: ETConfig\n  Wq: Float[Array, \"H D Y\"] # Query projection\n  Wk: Float[Array, \"H D Y\"] # Key projection\n  Xi: Float[Array, \"M D\"]\n\nEnergyTransformer is parameterized by only three matrices: \\(\\mathbf{W}^Q, \\mathbf{W}^K\\) and \\(\\mathbf{Xi}\\) (we did not choose to introduce any biases, though we could have).\nWe use these parameters to define both the attention energy and the memory energy.\n\n@patch\ndef attn_energy(self: EnergyTransformer, xhat: Float[Array, \"N D\"]):\n  beta = self.config.get_beta()\n  K = jnp.einsum(\"kd,hdy-&gt;khy\", xhat, self.Wk)\n  Q = jnp.einsum(\"qd,hdy-&gt;qhy\", xhat, self.Wq)\n  N = K.shape[0]\n  if self.config.prevent_self_attention:\n    bmask = jnp.ones((N, N)) - jnp.eye(N) # Prevent self-attention\n  else:\n    bmask = jnp.ones((N, N))\n  A = jax.nn.logsumexp(beta * jnp.einsum(\"khy,qhy-&gt;hqk\", K, Q), b=bmask, axis=-1)\n  return -1/beta * A.sum()\n\n@patch\ndef hn_energy(self: EnergyTransformer, xhat: Float[Array, \"N D\"]):\n  \"\"\"ReLU-based \"memory energy\" using a Hopfield Network\"\"\"\n  hid = jnp.einsum(\"nd,md-&gt;nm\", xhat, self.Xi)\n  return -0.5 * (hid.clip(0) ** 2).sum()\n\nThe total energy is just the sum of the attention and memory energies.\n\n@patch\ndef energy(self: EnergyTransformer, xhat: Float[Array, \"N D\"]):\n  \"Total energy of the Energy Transformer\"\n  return self.attn_energy(xhat) + self.hn_energy(xhat)\n\nAnd finally, let’s make a classmethod to easily initialize the module with random parameters.\n\n@patch(cls_method=True)\ndef rand_init(cls: EnergyTransformer, key, config: ETConfig):\n  key1, key2, key3 = jr.split(key, 3)\n  return cls(config,\n    Wq=jr.normal(key1, (config.H, config.D, config.Y)) / jnp.sqrt(config.Y),\n    Wk=jr.normal(key2, (config.H, config.D, config.Y)) / jnp.sqrt(config.Y),\n    Xi=jr.normal(key3, (config.M, config.D)) / jnp.sqrt(config.D)\n  )\n\n\n\n\n\n\n\nSpecial Layer Normalization\n\n\n\nNote that the xhat inputs above are all layer-normalized tokens. However, like other AMs, we restrict ourselves to using non-linearities that are gradients of a convex Lagrangian function. Our “special layernorm” is the same as the standard layer normalization except that we need our learnable gamma parameter to be a scalar instead of a vector of shape D. We will just show this in code below.\n\nclass EnergyLayerNorm(eqx.Module):\n  \"\"\"Define our primary activation function (modified LayerNorm) as a lagrangian with energy\"\"\"\n  gamma: Float[Array, \"\"]  # Scaling scalar\n  delta: Float[Array, \"D\"] # Bias per token\n  use_bias: bool = False\n  eps: float = 1e-5\n    \n  def lagrangian(self, x):\n    \"\"\"Integral of the standard LayerNorm\"\"\"\n    D = x.shape[-1]\n    xmeaned = x - x.mean(-1, keepdims=True)\n    t1 = D * self.gamma * jnp.sqrt((1 / D * xmeaned**2).sum() + self.eps)\n    if not self.use_bias: return t1\n    t2 = (self.delta * x).sum()\n    return t1 + t2\n\n  def __call__(self, x):\n    \"\"\"LayerNorm. The derivative of the Lagrangian\"\"\"\n    xmeaned = x - x.mean(-1, keepdims=True)\n    v = self.gamma * (xmeaned) / jnp.sqrt((xmeaned**2).mean(-1, keepdims=True)+ self.eps)\n    if self.use_bias: return v + self.delta\n    return v\n\n\n\nThat’s it! We rely on autograd to do the energy minimization, or the “inference” pass through the entire transformer.\nLet’s check that the energy both monotonically decreases and is bounded from below.\n\nkey = jr.PRNGKey(11)\net = EnergyTransformer.rand_init(key, config=smallETConfig)\nlnorm = EnergyLayerNorm(gamma=1., delta=jnp.zeros(et.config.D))\n\ndef energy_recall(Efn, x_init, nsteps, step_size):\n  \"Simple gradient descent to recall a memory\"\n  @jax.jit\n  def gd_step(x, i):\n      energy, grad = jax.value_and_grad(Efn)(lnorm(x))\n      x_next = x - step_size * grad\n      return x_next, energy\n\n  xhat_init = lnorm(x_init)\n  final_x, energy_history = jax.lax.scan(\n      gd_step,\n      xhat_init,\n      jnp.arange(nsteps)\n  )\n  return final_x, energy_history\n\nx_init = jr.normal(key, (100, et.config.D)) # Layer normalized tokens\nfinal_x, energy_history = energy_recall(et.energy, x_init, nsteps=3000, step_size=0.5)\n\n\n\n\n\n\n\n\n\nFigure 2: Energy descent for the Energy Transformer.",
    "crumbs": [
      "tutorial",
      "Energy Transformer"
    ]
  },
  {
    "objectID": "tutorial/energy_transformer.html#inference-with-an-energy-transformer",
    "href": "tutorial/energy_transformer.html#inference-with-an-energy-transformer",
    "title": "Energy Transformer",
    "section": "Inference with an Energy Transformer",
    "text": "Inference with an Energy Transformer\nTo make the Energy Transformer described above work on real data, we need to add some necessary addendums to work with image data: the token and position embedding matrices, and some data processing code.\n\nLoading data\nEnergy Transformer was originally trained on ImageNet. We will load some example images (unseen during training) to demonstrate ET’s ability to remember images.\n\n# Load and prepare unseen images\nIMAGENET_MEAN = np.array([0.485, 0.456, 0.406]) * 255 # C, H, W\nIMAGENET_STD = np.array([0.229, 0.224, 0.225]) * 255 # C, H, W\n\ndef normalize_img(im):\n  \"\"\"Put into channel first format, normalize\"\"\"\n  x = (im - IMAGENET_MEAN) / IMAGENET_STD\n  x = rearrange(x, \"h w c-&gt; c h w\")\n  return x\n\ndef unnormalize_img(x):\n  \"\"\"Put back into channel last format, denormalize\"\"\"\n  x = rearrange(x, \"c h w -&gt; h w c\")\n  im = (x * IMAGENET_STD) + IMAGENET_MEAN\n  return im.astype(jnp.uint8)\n\n@ft.lru_cache\ndef get_normalized_imgs():\n  imgs = jnp.array(get_et_imgs())\n  imgs = jax.vmap(normalize_img)(imgs)\n  return imgs\n\n\n\n\n\n\n\n\n\n\n\n\nPatching images\nWe build a Patcher class to patchify and unpatchify images, which is mostly a simple wrapper around the rearrange function from einops.\n\n\nPatcher class\nclass Patcher(eqx.Module):\n  \"Patchify and unpatchify an image.\"\n  image_shape: Iterable[int] # (C, H, W) Image shape\n  patch_size: int # Square patch size\n  kh: int # Number of patches in the height direction\n  kw: int # Number of patches in the width direction\n\n  @property\n  def patch_shape(self): return (self.image_shape[0], self.patch_size, self.patch_size)\n\n  @property\n  def num_patch_elements(self): return ft.reduce(lambda a, b=1: a * b, self.patch_shape)\n\n  @property\n  def num_patches(self): return self.kh * self.kw\n\n  def patchify(self, img):\n    \"Turn an image (possibly batched) into a collection of patches.\"\n    return rearrange(\n      img,\n      \"... c (kh h) (kw w)-&gt; ... (kh kw) c h w\",\n      h=self.patch_size,\n      w=self.patch_size,\n    )\n\n  def unpatchify(self, patches):\n    \"Turn a collection of patches (possibly batched) back into an image.\"\n    return rearrange(\n      patches, \"... (kh kw) c h w -&gt; ... c (kh h) (kw w)\", kh=self.kh, kw=self.kw\n    )\n\n  def rasterize(self, patches):\n    \"Rasterize patches into tokens\"\n    return rearrange(patches, \"... c h w -&gt; ... (c h w)\")\n\n  def unrasterize(self, tokens):\n    \"Unrasterize tokens into patches\"\n    c,h,w = self.patch_shape\n    return rearrange(tokens, \"... (c h w) -&gt; ... c h w\", c=c, h=h, w=w)\n\n  def tokenify(self, img):\n    \"Turn img into rasterized patches\"\n    return self.rasterize(self.patchify(img))\n\n  def untokenify(self, tokens):\n    \"Untokenify tokens into original image\"\n    return self.unpatchify(self.unrasterize(tokens))\n\n  def patchified_shape(self):\n    \"The expected shape of a patchified image\"\n    return (self.num_patches, *self.patch_shape)\n\n  @classmethod\n  def from_img(cls, img, patch_size):\n    \"Create a Patcher from an example image.\"\n    return cls.from_img_shape(img.shape, patch_size)\n\n  @classmethod\n  def from_img_shape(cls, img_shape, patch_size):\n    \"Create a patcher from a specified image shape.\"\n    height, width = img_shape[-2:]\n    assert (height % patch_size) == 0\n    assert (width % patch_size) == 0\n    kh = int(height / patch_size)\n    kw = int(width / patch_size)\n    return cls(img_shape, patch_size, kh, kw)\n\n\nIt lets us do things like:\n\npatcher = Patcher.from_img_shape(imgs[0].shape, patch_size=16)\npatched_img = patcher.patchify(imgs[0])\nprint(patched_img.shape)\n\n(196, 3, 16, 16)\n\n\n\n\nRuntimeWarning: invalid value encountered in cast\n  return im.astype(jnp.uint8)\n\n\n\n\n\n\n\n\n\nPatcher.unpatchify gets us back to the original image.\n\nassert jnp.all(patcher.unpatchify(patched_img) == imgs[0])\n\nWe can also process an images and batches of imags into tokens and back.\n\ntokenified_img = patcher.tokenify(imgs[0])\nprint(\"Token pre-embedding shape: \", tokenified_img.shape)\n\nuntokenified_img = patcher.untokenify(tokenified_img)\nassert jnp.all(untokenified_img == imgs[0])\n\nbatch_tokenified_imgs = patcher.tokenify(imgs)\nprint(\"Batch token pre-embedding shape: \", batch_tokenified_imgs.shape)\n\nbatch_untokenified_imgs = patcher.untokenify(batch_tokenified_imgs)\nassert jnp.all(batch_untokenified_imgs == imgs)\n\nToken pre-embedding shape:  (196, 768)\nBatch token pre-embedding shape:  (11, 196, 768)\n\n\n\n\nImage-compatible ET\nLet’s create a full ET, complete with embeddings, model that can be used for masked-image inpainting. We say that each image has \\(N\\) total patches/tokens, where each patch as \\(Z = c \\times h \\times w\\) pixels when rasterized. We will use linear embeddings (with biases) to embed and unembed rasterized image patches to tokens.\nFirst, let’s describe the data and ET we are working with.\n\nclass ImageETConfig(eqx.Module):\n  image_shape: Tuple[int, int, int] = (3, 224, 224) # (C, H, W) Image shape\n  patch_size: int = 16 # Square patch size\n  et_conf: ETConfig = fullETConfig\n\nTo work with data, we add a few extra matrices: embedding/unembedding matrices (let’s use a bias for each), position embeddings, and CLS/MASK tokens. The position embeddings are used to encode the position of each token in the sequence, and the CLS/MASK tokens are used for interop with the original ViT. [2] Additionally, the layernorm is external to the computation of the ET so we’ll insert those parameters here.\n\nclass ImageEnergyTransformer(eqx.Module):\n  patcher: Patcher\n  W_emb: Float[Array, \"Z D\"]\n  b_emb: Float[Array, \"D\"]\n  W_unemb: Float[Array, \"D Z\"]\n  b_unemb: Float[Array, \"Z\"]\n\n  pos_embed: Float[Array, \"(N+1) D\"] # Don't forget the CLS token!\n  cls_token: jax.Array\n  mask_token: jax.Array\n  et: EnergyTransformer\n  lnorm: EnergyLayerNorm\n\n  config: ImageETConfig\n\nLet’s define some functions for converting image patches to/from tokens. These are a.k.a. “embedding” and “unembedding” operations.\n\n@patch\ndef encode(\n  self: ImageEnergyTransformer, \n  x: Float[Array, \"N Z\"]\n):\n  \"Embed rasterized patches to tokens\"\n  out = x @ self.W_emb + self.b_emb # (..., N, D)\n  return out\n\n@patch\ndef decode(\n  self: ImageEnergyTransformer, \n  x: Float[Array, \"N D\"]):\n  \"Turn x from tokens to rasterized img patches\"\n  return x @ self.W_unemb + self.b_unemb # (..., N, Z)\n\nMasking tokens is also a part of this data connection. Let’s corrupt and add the CLS register:\n\n@patch\ndef corrupt_tokens(\n  self: ImageEnergyTransformer, \n  x: Float[Array, \"N D\"],\n  mask: Float[Array, \"N\"], \n  max_n_masked: int=100):\n  \"\"\"Corrupt tokens with MASK tokens wherever `mask` is 1.\n\n  `max_n_masked` needs to be known in advance for JAX JIT to work properly\n  \"\"\"\n  maskmask = jnp.nonzero(mask == 1, size=max_n_masked, fill_value=0)\n  return x.at[maskmask].set(self.mask_token) # (..., N, D)\n\n@patch\ndef prep_tokens(\n  self: ImageEnergyTransformer, \n  x: Float[Array, \"N D\"], \n  mask: Float[Array, \"N\"]):\n  \"Add CLS+MASK tokens and POS embeddings\"\n  x = self.corrupt_tokens(x, mask)\n  x = jnp.concatenate([self.cls_token[None], x]) # (..., N+1, D)\n  return x + self.pos_embed # (..., N+1, D)\n\nThe inference process is gradient descent down the energy, and turns a full image whose patches are masked according to mask and returns predictions for the whole image.\n\n@patch\ndef __call__(\n  self: ImageEnergyTransformer, \n  img: Float[Array, \"C H W\"], \n  mask: Float[Array, \"N\"], \n  nsteps=12, \n  step_size=0.1):\n  \"A complete pipeline for masked image modeling in ET using gradient descent\"\n  x = self.patcher.tokenify(img) # (..., N, Z)\n  x = self.encode(x)\n  x = self.prep_tokens(x, mask)  # (..., N+1, D)\n\n  get_energy_info = jax.value_and_grad(self.et.energy)\n  \n  def gd_step(x, i):\n      xhat = self.lnorm(x)\n      E, dEdg = get_energy_info(xhat)\n      x_next = x - step_size * dEdg\n      return x_next, {\"energy\": E, \"xhat\": xhat}\n\n  x, traj_outputs = jax.lax.scan(gd_step, x, jnp.arange(nsteps))\n\n  xhat_final = self.lnorm(x)\n  E_final = self.et.energy(xhat_final)\n  traj_outputs['xhat'] = jnp.concatenate([traj_outputs['xhat'], xhat_final[None]], axis=0)\n  traj_outputs['energy'] = jnp.concatenate([traj_outputs['energy'], E_final[None]], axis=0)\n\n  xhat_final = xhat_final[1:]  # Discard CLS token for masked inpainting\n  x_decoded = self.decode(xhat_final)\n  return self.patcher.untokenify(x_decoded), traj_outputs\n\n\n\n\n\n\n\nRandom initialization helper\n\n\n\n\n\nFor completeness, let’s add a helper function to initialize the model with random parameters. We won’t use it in this tutorial, however.\n\n@patch(cls_method=True)\ndef rand_init(cls: ImageEnergyTransformer, key, config=ImageETConfig()):\n  key1, key2, key3, key4, key5, key6, key7, key8 = jr.split(key, 8)\n  patcher = Patcher.from_img_shape(config.image_shape, config.patch_size)\n  W_emb = jr.normal(key1, (patcher.num_patch_elements, config.et_conf.D)) / config.et_conf.D\n  b_emb = jr.normal(key2, (config.et_conf.D,))\n  W_unemb = jr.normal(key3, (config.et_conf.D, patcher.num_patch_elements)) / patcher.num_patch_elements\n  b_unemb = jr.normal(key4, (patcher.num_patch_elements,))\n  pos_embed = jr.normal(key5, (patcher.num_patches, config.et_conf.D)) / config.et_conf.D\n  cls_token = 0.002 * jr.normal(key6, (config.et_conf.D,))\n  mask_token = 0.002 * jr.normal(key7, (config.et_conf.D,))\n  pos_embed = 0.002 * jr.normal(key8, (1 + patcher.num_patches, config.et_conf.D)) / config.et_conf.D\n\n  return cls(\n    patcher=patcher,\n    W_emb=W_emb,\n    b_emb=b_emb,\n    W_unemb=W_unemb,\n    b_unemb=b_unemb,\n    pos_embed=pos_embed,\n    cls_token=cls_token,\n    mask_token=mask_token,\n    et=EnergyTransformer.rand_init(key7, config.et_conf),\n    lnorm=EnergyLayerNorm(gamma=1., delta=jnp.zeros(config.et_conf.D)),\n    config=config\n  )\n\nimageET = ImageEnergyTransformer.rand_init(key, ImageETConfig())\n\n\n\n\n\n\nLoading pretrained weights\nET has publicly available pretrained weights that can be used for masked-image inpainting. The model itself is pretty small ~20MB, with no compression tricks on the weights (everything is np.float32). We load the state dict from a saved .npz file as follows:\n\n@ft.lru_cache\ndef get_pretrained_et():\n  load_dict = {k: jnp.array(v) for k,v in get_et_checkpoint().items()}\n\n  # config from state_dict\n  H, Y, D = load_dict[\"Wk\"].shape\n  D, M = load_dict[\"Xi\"].shape\n\n  et_config = ETConfig(D=D, H=H, Y=Y, M=M, prevent_self_attention=False) # These weights were trained allowing self attention. But the arch works equally well both ways.\n  et = EnergyTransformer(\n    Wk = rearrange(load_dict[\"Wk\"], \"h y d -&gt; h d y\"),\n    Wq = rearrange(load_dict[\"Wq\"], \"h y d -&gt; h d y\"),\n    Xi = rearrange(load_dict[\"Xi\"], \"d m -&gt; m d\"),\n    config = et_config\n  )\n\n  image_config = ImageETConfig(image_shape=(3, 224, 224), patch_size=16, et_conf=et_config)\n  patcher = Patcher.from_img_shape(image_config.image_shape, image_config.patch_size)\n  iet = ImageEnergyTransformer(\n    patcher = patcher,\n    W_emb = load_dict[\"Wenc\"],\n    b_emb = load_dict[\"Benc\"],\n    W_unemb = load_dict[\"Wdec\"],\n    b_unemb = load_dict[\"Bdec\"],\n    pos_embed = load_dict[\"POS_embed\"],\n    cls_token = load_dict[\"CLS_token\"],\n    mask_token = load_dict[\"MASK_token\"],\n    et = et,\n    lnorm = EnergyLayerNorm(gamma=load_dict[\"LNORM_gamma\"], delta=load_dict[\"LNORM_bias\"]),\n    config = image_config\n  )\n\n  return iet\n\nWe can inpaint images with ET.\n\ndef inpaint_image(\n  iet: ImageEnergyTransformer, \n  img: Float[Array, \"C H W\"], \n  n_mask: int, \n  key: jax.random.PRNGKey, \n  nsteps: int=12, \n  step_size: float=0.1):\n    \" Perform masked image inpainting with Energy Transformer\"\n    # Create random mask\n    mask_idxs = jr.choice(\n        key, np.arange(iet.patcher.num_patches), shape=(n_mask,), replace=False\n    )\n    mask = jnp.zeros(iet.patcher.num_patches).at[mask_idxs].set(1)\n    \n    x = iet.patcher.tokenify(img)\n    x = iet.encode(x)  # Img to embedded tokens\n    x = iet.prep_tokens(x, mask)[1:]  # N,D (remove CLS token)\n    masked_img = iet.decode(iet.lnorm(x))\n    masked_img = iet.patcher.untokenify(masked_img)\n    \n    # Reconstruct image using Energy Transformer\n    recons_img, traj_outputs = iet(img, mask, nsteps=nsteps, step_size=step_size)\n    \n    return masked_img, recons_img, traj_outputs\n\niet = get_pretrained_et()\nnh, nw = 2, 5\nN = nh*nw\nog_imgs = get_normalized_imgs()[:N]\n\nkeys = jr.split(jr.PRNGKey(0), len(og_imgs))\nmasked_imgs, recons_imgs, traj_outputs = jax.vmap(inpaint_image, in_axes=(None, 0, None, 0))(iet, og_imgs, 100, keys)\n\nvunnormalize_img = jax.vmap(unnormalize_img)\nog_imgs_show, masked_imgs_show, recons_imgs_show = [vunnormalize_img(im) for im in (og_imgs, masked_imgs, recons_imgs)]\n\n\n\n\n\n\n\n\n\n\nWe can also animate the retrieval.\n\n\nAnimation dependencies\nfrom pathlib import Path\nimport matplotlib.animation as animation\nfrom IPython.display import Video, Markdown\nfrom moviepy.editor import ipython_display\nimport os\n\nCACHE_DIR = Path(\"./cache\") / \"01_energy_transformer\"\nCACHE_DIR.mkdir(exist_ok=True, parents=True)\nCACHE_VIDEOS = True\n\n\n\n\n\n\nVideo\n\n\n\n\nThese images are fully reconstructed using autograd down the parameterized energy function. You may notice the reconstructions are not perfect, e.g., the right eye of the white dog is missing.\n\n\n\n\n\n\nThe energy is still decreasing! Shouldn’t the images get better if we run longer?\n\n\n\n\n\nUnfortunately, these weights were only trained to 12 steps at a fixed step size. Running longer will still cause the energy to decrease, but our image reconstruction quality will not improve. This reflects that our model has learned a kind of ‘metastable state’ at which nice reconstructions are retrieved, but these reconstructions are not “memories” in the formal definition of the term.\n\nmasked_imgs, recons_imgs, traj_outputs = jax.vmap(ft.partial(inpaint_image, nsteps=40), in_axes=(None, 0, None, 0))(iet, og_imgs, 100, keys)\nvideo, video_fname = show_et_recall_animation(iet, traj_outputs, \"et_reconstruction_long\", \n                                             steps_per_sample=1, force_remake=True)\n\n\n\n\n\nVideo",
    "crumbs": [
      "tutorial",
      "Energy Transformer"
    ]
  },
  {
    "objectID": "tutorial/energy_transformer.html#interpreting-et",
    "href": "tutorial/energy_transformer.html#interpreting-et",
    "title": "Energy Transformer",
    "section": "Interpreting ET",
    "text": "Interpreting ET\nThe representations learned by ET are attractors of the dynamics. That is, the weights of the Hofield Network in ET are not arbitrary linear transformations — they are actual stored data patterns. Visualizing the weights reveals what the model has actually learned.\n\ndef decode_stored_pattern(iet, xi):\n  c,h,w = iet.patcher.patch_shape\n  decoded = iet.decode(iet.lnorm(xi))\n  patches = rearrange(decoded, '... (c h w) -&gt; ... c h w', c=c, h=h, w=w)\n  return unnormalize_img(patches) \n\nXi_show = jax.vmap(ft.partial(decode_stored_pattern, iet))(iet.et.Xi)\n\n\n\n\n\n\nSampling the stored patterns in the Hopfield Network, sorted by frequency content\n\n\n\n\n\n\n\n\n\n\nInterpretability by design\n\n\n\nYou can think of the Hopfield Network like an SAE that is integrated into the core computation of the model. Interpretability is a natural byproduct of good architecture design.",
    "crumbs": [
      "tutorial",
      "Energy Transformer"
    ]
  },
  {
    "objectID": "tutorial/dense_storage.html",
    "href": "tutorial/dense_storage.html",
    "title": "Binary Dense Storage",
    "section": "",
    "text": "Notebook Execution Settings\nCACHE_DIR = \"cache/00_dense_storage\"\nCACHE_RECALL = True # If False, regenerate all saved results even if files exist.\nSHOW_FULL_ANIMATIONS = True # If True, render videos instead of gifs. This is slower than gifs and relies on `ffmpeg` to save the animation, but it lets us see the energy descent alongside the frame evolution.",
    "crumbs": [
      "tutorial",
      "Binary Dense Storage"
    ]
  },
  {
    "objectID": "tutorial/dense_storage.html#the-classical-hopfield-network",
    "href": "tutorial/dense_storage.html#the-classical-hopfield-network",
    "title": "Binary Dense Storage",
    "section": "The Classical Hopfield Network",
    "text": "The Classical Hopfield Network\nLet’s revisit our task to store \\(K\\) binary patterns each of dimension \\(D\\) into an energy function. Let’s keep things simple and fast for the first part of this notebook and focus on storing and retrieving \\(K=2\\) patterns: an eevee and pichu, where each (48,48) image is rasterized to a vector dimension of \\(D=2304\\).\n\ndesired_names = [\"eevee\", \"pichu\"]\neevee_pichu_idxs = [poke_names.index(name) for name in desired_names]\nXi = data[eevee_pichu_idxs]\n\nfig, ax = show_im(Xi, figsize=(6,3));\nax.set_title(\"Stored patterns\")\nplt.show()\n\nprint(f\"K={Xi.shape[0]}, D={Xi.shape[1]}\")\n\nK=2, D=2304\n\n\n\n\n\n\n\n\n\nThe Classical Hopfield Network (CHN) [1] defines an energy function for this collection of patterns, putting the \\(\\mu\\)-th stored pattern \\(\\xi^\\mu\\) at a low value of energy. The CHN energy is a quadratic function described by dot-product correlations:\n\\[\nE_\\text{CHN}(\\sigma) = -\\frac{1}{2} \\sum_\\mu \\left(\\sum_{i} \\xi^\\mu_i \\sigma_i\\right)^2 = -\\frac{1}{2} \\sum_{i,j} T_{ij} \\sigma_i \\sigma_j.\n\\tag{2}\\]\nWe see the familiar equation for CHN energy on the RHS if we expand the quadratic function, where \\(T_{ij} := \\sum_{\\mu=1}^K \\xi^\\mu_i \\xi^\\mu_j\\) is the matrix of symmetric synapses. Learned patterns \\(\\xi^\\mu\\) are stored in \\(T\\) via a simple, Hebbian learning rule.\nThe CHN can be easily implemented in code via\n\nclass CHN(BinaryAM):\n    def energy(\n        self, \n        sigma: Float[Array, \"D\"] # Possibly noisy query pattern\n        ): \n        \"Quadratic energy function for the CHN\"\n        return -0.5 * jnp.sum((self.Xi @ sigma)**2, axis=0)\n\nchn = CHN(Xi)\n\nThe asynchronous update rule of Equation 1 uses the energy difference of a flipped bit to determine whether to keep the flip or not. That update rule is equivalent to the following, arguably more familiar update rule, which describes the next state based on the sign of the total input current to the neuron \\(\\sigma_i\\).\n\\[\n\\begin{align*}\n\\sigma_i^{(t+1)} &\\leftarrow \\text{sgn}\\left(\\sum_{\\mu} \\xi^\\mu_i \\sum_{j \\neq i} \\left(\\xi^\\mu_j \\sigma_j^{(t)}\\right) \\right)\\\\\n\\text{sgn}(x) &:= \\begin{cases}\n1 & \\text{if } x \\geq 0 \\\\\n-1 & \\text{if } x &lt; 0\n\\end{cases}\\quad.\n\\end{align*}\n\\]\nThis update rule also ensures the network always moves toward lower energy states. Because the \\(E_\\text{CHN}\\) is bounded from below, the network will eventually converge to a local minimum that (ideally) corresponds to one of the stored patterns.\nLet’s observe the recall process! We’ll start with a noisy version of the first pattern and see if we can recover it.\n\ndef flip_some_bits(key, x, p=0.1):\n    \"Flip `p` fraction of bits in `x`\"\n    prange = np.array([p, 1-p])\n    return x * jr.choice(key, np.array([-1, 1]), p=prange, shape=x.shape)\n\nsigma_og = Xi[0] \nsigma_noisy = flip_some_bits(jr.PRNGKey(0), sigma_og, 0.2)\n\nshow_im(jnp.stack([sigma_og, sigma_noisy]), figsize=(6, 3));\n\n\n\n\n\n\n\n\nFor the pedagogical purpose of this notebook, we’ll cache the recall process and results so we don’t have to run it every time.\n\n@delegates(BinaryAM.async_recall)\ndef cached_recall(am, cache_name, sigma_noisy, key=jr.PRNGKey(0), save=True, **kwargs):\n    \"Cache the recall process using key `cache_name`\"\n    npz_fname = Path(CACHE_DIR) / (cache_name + '.npz')\n    if npz_fname.exists() and CACHE_RECALL: \n        npz_data = np.load(npz_fname)\n        sigma_final, frames, energies = npz_data['sigma_final'], npz_data['frames'], npz_data['energies']\n        print(\"Loading cached recall data\")\n    else: \n        sigma_final, (frames, energies) = am.async_recall(sigma_noisy, key=key, **kwargs)\n        if save: jnp.savez(npz_fname, sigma_final=sigma_final, frames=frames, energies=energies)\n    return sigma_final, frames, energies\n\ncache_name = 'basic_hopfield_recovery'\nsigma_final, frames, energies = cached_recall(chn, cache_name, sigma_noisy, nsteps=12000, key=jr.PRNGKey(5))\n\nLoading cached recall data\n\n\n\n\n\n\n\n\n\n\n\nWe can animate the recall process to view the “thinking” process of the CHN.\n\n\n\n\nVideo\n\n\n\n\n\nRetrieving “inverted” images\nIf we initialize a query with too much noise, it’s possible to retrieve the negative of a stored pattern or an “inverted image”. Because the energy is quadratic, both \\(\\sigma\\) and \\(-\\sigma\\) produce the same small value of energy. Whether we retrieve the original \\(\\sigma\\) or the inverted \\(-\\sigma\\) is dependent on whether we initialize our query closer to the original or inverted pattern.\n\\[\nE_\\text{CHN}(-\\sigma) = -\\frac{1}{2} \\left(\\sum_{\\mu} \\xi^\\mu_i (-\\sigma_i)\\right)^2 = E_\\text{CHN}(\\sigma)\n\\]\n\n\nLoading cached recall data\nAccidentally retrieved the inverted pattern!\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\nMemory retrieval failure\nUnfortunately, the CHN is terrible at storing and retrieving multiple patterns. If we add even four more patterns into the synaptic memory, our network will fail to retrieve our eevee.\n\nXi = data[eevee_pichu_idxs]\nXi = jnp.concatenate([Xi, jr.choice(jr.PRNGKey(10), data, shape=(4,), replace=False)])\nfig, ax = show_im(Xi, figsize=(6, 4));\nax.set_title(f\"Stored patterns (K={Xi.shape[0]})\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nLoading cached recall data\nCHN failed to retrieve the correct pattern!\n\n\n\n\n\n\n\n\n\n\n\nVideo",
    "crumbs": [
      "tutorial",
      "Binary Dense Storage"
    ]
  },
  {
    "objectID": "tutorial/dense_storage.html#dense-associative-memory",
    "href": "tutorial/dense_storage.html#dense-associative-memory",
    "title": "Binary Dense Storage",
    "section": "Dense Associative Memory",
    "text": "Dense Associative Memory\nThe CHN has a quadratic energy, which is a special case of a more general class of models called Dense Associative Memory (DenseAM) [2]. If we increase the degree of the polynomial used in the energy function, we strengthen the coupling between neurons and can store more patterns into the same synaptic matrix.\nThe new energy function, written in terms of polynomials of degree \\(n\\) and using the same notation for stored patterns \\(\\xi^\\mu_i\\), is\n\\[\n\\begin{align*}\nE_\\text{DAM}(\\sigma) &= -\\sum_{\\mu=1}^K F_n\\left(\\sum_{i=1}^D \\xi^\\mu_i \\sigma_i\\right),\\\\\n\\text{where}\\;F_n(x) &= \\begin{cases} \\frac{x^n}{n} & \\text{if } x \\geq 0 \\\\ 0 & \\text{if } x &lt; 0 \\end{cases}.\n\\end{align*}\n\\tag{3}\\]\n\n\n\n\n\n\nNote\n\n\n\nWe need \\(F_n\\) to be convex for all \\(n\\), which is why we perform the rectification. We could alternatively limit ourselves to only even values of \\(n\\).\nFun fact, rectified polynomials remove the “inverted” retrieval phenomenon seen in Section 1.1.\n\n\nEquation 3 admits the following manual update rule for a single neuron \\(i\\):\n\\[\n\\begin{align*}\n\\sigma_i^{(t+1)} &\\leftarrow \\text{sgn}\\left( \\sum_{\\mu} \\xi^\\mu_i f_n\\left( \\sum_{j \\neq i} \\xi^\\mu_j \\sigma_j^{(t)}\\right)\\right)\\\\\n\\end{align*}.\n\\tag{4}\\]\nHere we introduced an activation function \\(f_n(\\cdot) = F_n'(\\cdot)\\) that is the derivative of the rectified polynomial used to define the energy. This update can be viewed as the negative gradient of the energy function, ensuring that the network always moves toward lower energy states. Like before, this energy is bounded from below and we will eventually converge to a local minimum that corresponds to one of the stored patterns.\nLet’s implement the DenseAM model. The primary difference from the CHN is that now we generalize the quadratic energy to a (possibly rectified) polynomial energy.\n\nclass PolynomialDenseAM(BinaryAM):\n    Xi: jax.Array # (K, D) Memory patterns \n    n: int # Power of polynomial F\n    rectified: bool = True # Whether to rectify inputs to F\n\n    def F_n(self, sims): \n        \"\"\"Rectified polynomial of degree `n` for energy\"\"\"\n        sims = sims.clip(0) if self.rectified else sims\n        return 1 / self.n * sims ** self.n\n\n    def energy(self, sigma): \n        return -jnp.sum(self.F_n(self.Xi @ sigma))\n\nA simple change to using a polynomial of degree \\(6\\) instead of the CHN’s quadratic energy function allows us to store and retrieve our desired eevee even with up to \\(K=100\\) patterns.\n\n# Increase the number of stored patterns!\nXi = data[eevee_pichu_idxs]\nXi = jnp.concatenate([Xi, jr.choice(jr.PRNGKey(10), data, shape=(98,), replace=False)])\nfig1, ax1 = show_im(Xi, figsize=(7,7));\nax1.set_title(\"Stored patterns\")\ndam = PolynomialDenseAM(Xi, n=6, rectified=True)\n\nfname = f'dam_recovery_n_{dam.n}_K_{Xi.shape[0]}'\n\nsigma_og = Xi[0]\nsigma_noisy = flip_some_bits(jr.PRNGKey(0), sigma_og, 0.2)\nsigma_final, frames, energies = cached_recall(dam, fname, sigma_noisy, nsteps=20000, key=jr.PRNGKey(5))\n\nfig2, axes2 = show_recall_output(sigma_og, sigma_noisy, sigma_final, energies, show_original=False)\nfig2.suptitle(f\"DenseAM(n={dam.n}, K={Xi.shape[0]})\")\nplt.subplots_adjust(top=0.75)\nplt.show()\n\nvideo, video_fname = show_cached_recall_animation(fname, steps_per_sample=32)\nMarkdown(f\"![]({video_fname})\")\n\nLoading cached recall data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\n\nA higher degree polynomial gives us more storage capacity, which means that it is easier to retrieve the patterns we have stored in the network. Note that the higher the degree \\(n\\), the narrower the basins of attraction, which makes it easier to pack more patterns into the energy landscape.",
    "crumbs": [
      "tutorial",
      "Binary Dense Storage"
    ]
  },
  {
    "objectID": "tutorial/dense_storage.html#sec-gotta-catch-em-all",
    "href": "tutorial/dense_storage.html#sec-gotta-catch-em-all",
    "title": "Binary Dense Storage",
    "section": "Gotta catch ’em all!",
    "text": "Gotta catch ’em all!\nLet’s try to store and retrieve all 1024 pokemon patterns into our network (though we will only show retrieval for a subset of them for computational reasons). To do this, we’ll need very large values of \\(n\\), which is bad for numeric overflow (computers don’t like working in really really large numbers i.e., inf energy regimes).\nWe’ll implement an exponential version of the DenseAM [3]. Specifically, we will use a numerically stable logsumexp version [4].\n\\[\n\\begin{align*}\nE_\\text{eDAM}(\\sigma) &= -\\log \\sum_{\\mu=1}^K \\exp \\left(\\beta \\sum_{i=1}^D \\xi^\\mu_i \\sigma_i\\right)\n\\end{align*}\n\\tag{5}\\]\nwhere increasing the inverse temperature \\(\\beta\\) has a similar effect to increasing \\(n\\) in the DenseAM polynomial energy function. Because the log is a monotonically increasing function, the energy minima of the original energy function are preserved, while simultaneously making the energy function more numerically stable.\n\nclass ExponentialDenseAM(BinaryAM):\n    Xi: jax.Array # (K, D) Memory patterns \n    beta: float = 1.0 # Temperature parameter\n\n    def energy(self, sigma):\n        return -jax.nn.logsumexp(self.beta * self.Xi @ sigma, axis=-1)\n\n\n# Show larger batch retrieval\nXi = data[:1024]\nNshow = 255\nXi_show = jnp.concatenate([data[eevee_pichu_idxs], jr.choice(jr.PRNGKey(10), Xi, shape=(Nshow - len(eevee_pichu_idxs),), replace=False)])\nfig1, ax1 = show_im(Xi_show, figsize=(8,8));\nax1.set_title(f\"Random sample of {Nshow} stored patterns\")\nprint(f\"Storing {Xi.shape[0]} patterns\")\n\nStoring 1024 patterns\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMemory usage warning\n\n\n\n\n\nDepending on your RAM availability, the following cell may crash your session. Decrease to e.g., nh = nw = 5 to avoid this (or upgrade your runtime on Colab for more resources).\n\n\n\n\n\n\nCode\nkey1, key2 = jr.split(jr.PRNGKey(3))\nnh = nw = 10\nN = nh * nw # Sample N patterns to show in grid\nsigma_og = jnp.concatenate([\n    data[eevee_pichu_idxs], \n    jr.choice(jr.PRNGKey(10), data, shape=(N - len(eevee_pichu_idxs),), replace=False)])\nsigma_noisy = flip_some_bits(key2, sigma_og, 0.25)\n\nedam = ExponentialDenseAM(Xi, beta=50.)\n\ncache_name = \"logsumexp_batched\"\nkeys = jr.split(key2, sigma_noisy.shape[0])\nnpz_fname = Path(CACHE_DIR) / (cache_name + \".npz\")\nif os.path.exists(npz_fname) and CACHE_RECALL:\n    npz_data = np.load(npz_fname)\n    sigma_final, frames, energies = npz_data['sigma_final'], npz_data['frames'], npz_data['energies']\nelse:\n    sigma_final, frames, energies = jax.vmap(ft.partial(cached_recall, nsteps=16000, save=False), in_axes=(None, None, 0,0))(edam, cache_name, sigma_noisy, keys)\n    np.savez(npz_fname, sigma_final=sigma_final, frames=frames, energies=energies)\n\n\nAnd of course, what’s the fun if we can’t animate the retrieval process?\n&lt;IPython.core.display.Image object&gt;",
    "crumbs": [
      "tutorial",
      "Binary Dense Storage"
    ]
  },
  {
    "objectID": "tutorial/diffusion_as_memory.html",
    "href": "tutorial/diffusion_as_memory.html",
    "title": "Memory and Diffusion",
    "section": "",
    "text": "This notebook is a simplified, step-by-step walkthrough of the 2D toy example from the paper: “Memorization to Generalization: Emergence of Diffusion Models from Associative Memory”.\nWe will train a score-based diffusion model on a small dataset of points lying on a circle. Our goal is to understand how the model learns the data distribution and to visualize its learned “energy landscape,” which reveals how it behaves like an Associative Memory system initially to later transition into a generative model.\nFor more details, please read the paper and the code repository.\nImports and Setup\n# --- Essential Libraries ---\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nfrom itertools import cycle\nfrom PIL import Image\n\nfrom copy import deepcopy\n\n# --- SciPy for specific math functions ---\nfrom scipy.special import i0, i1 # Modified Bessel functions for analytical energy\nimport scipy.integrate as integrate # For ODE solving (likelihood calculation)\n\n# --- Scikit-learn for Clustering ---\nfrom sklearn.cluster import AgglomerativeClustering\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Set a nice plot style\nsns.set_theme(style=\"whitegrid\")\n\n# turn off warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom collections import OrderedDict\nCaching models\n# For caching trained models\nfrom pathlib import Path\nCACHE_DIR = Path(\"./cache/02_diffusion_as_memory\")\nCACHE_DIR.mkdir(parents=True, exist_ok=True)\nCACHE_MODELS = True",
    "crumbs": [
      "tutorial",
      "Memory and Diffusion"
    ]
  },
  {
    "objectID": "tutorial/diffusion_as_memory.html#data-generation-and-visualization",
    "href": "tutorial/diffusion_as_memory.html#data-generation-and-visualization",
    "title": "Memory and Diffusion",
    "section": "Data Generation and Visualization",
    "text": "Data Generation and Visualization\nThe paper uses a simple dataset: points sampled from the circumference of a unit circle. This helps us easily visualize how the model learns.\nWe’ll define a function to generate these points and a PyTorch Dataset class to handle them.\n\ndef generate_circle_data(num_samples=60_000, radius=1, seed=59):\n    \"\"\"Generates data points that lie on a unit circle.\"\"\"\n    np.random.seed(seed)\n    # Sample angles uniformly from 0 to 2*pi\n    angles = np.random.uniform(0, 2 * np.pi, num_samples)\n\n    # Convert polar coordinates (angles, radius) to Cartesian (x, y)\n    x = radius * np.cos(angles)\n    y = radius * np.sin(angles)\n    return np.stack([x, y], axis=1)\n\nclass CircleDataset(Dataset):\n    \"\"\"A PyTorch Dataset to wrap our circle data.\"\"\"\n    def __init__(self, num_samples=60_000, radius=1, seed=9):\n        # Generate and store the data as a torch tensor\n        self.data = torch.from_numpy(generate_circle_data(num_samples, radius, seed)).float()\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\ndef create_subset(dataset, sample_size, seed=42):\n    \"\"\"Create a subset of the dataset based on the specified sample size. \"\"\"\n    max_size = len(dataset)\n    generator = torch.Generator().manual_seed(seed)\n    if not 1 &lt;= sample_size &lt;= len(dataset):\n        raise ValueError(\"Sample size must be between 1 and the size of the dataset inclusive.\")\n    subset, _ = torch.utils.data.random_split(\n        dataset, [sample_size, max_size - sample_size], generator=generator\n    )\n    return subset\n\n\nCreating a Small Training Set\nDiffusion models can learn from very few samples. In the paper, this is referred to as memorizing “patterns”. Let’s create a tiny dataset with just 9 data points (patterns) to train on.\n\n# --- Configuration ---\nSAMPLE_SIZE = 9 # The number of data points (patterns) to memorize\nBATCH_SIZE = min(500, SAMPLE_SIZE)  # Use all data points in each batch\nSEED = 9       # For reproducibility\n\n# Create Dataset\ndataset = CircleDataset(60_000, seed=SEED)\n\n# Split Dataset\ntrain_subset = create_subset(dataset, SAMPLE_SIZE, SEED)\n\n# Create a DataLoader\ntrain_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n\n# Extract the training data points for visualization\npatterns = train_subset.dataset[train_subset.indices]\n\nLet’s plot our small dataset. These are the specific points we want our model to learn and remember.",
    "crumbs": [
      "tutorial",
      "Memory and Diffusion"
    ]
  },
  {
    "objectID": "tutorial/diffusion_as_memory.html#the-diffusion-model",
    "href": "tutorial/diffusion_as_memory.html#the-diffusion-model",
    "title": "Memory and Diffusion",
    "section": "The Diffusion Model",
    "text": "The Diffusion Model\nThe diffusion model is simply a model \\(s_\\theta(\\mathbf{x}_t, t)\\) which approximates the score function: \\[\ns_\\theta(\\mathbf{x}_t, t) ≈ \\nabla_{\\mathbf{x}_t} \\log p_t (\\mathbf{x}_t)\n\\] over a series of timesteps.\nIn this tutorial, we will be using Variance Exploding (VE) SDE, which defines how data is gradually noised over time ranging from \\(t \\in [\\epsilon, 1]\\): \\[\n  \\mathrm{d} \\mathbf{x}_t = \\sigma  \\mathrm{d} \\mathbf{w}_t\n\\] and the corresponding reverse process: \\[\n  \\mathrm{d} \\mathbf{x}_t = \\big [ -\\sigma^2 \\nabla_{\\mathbf{x}_t} \\log p_t (\\mathbf{x}_t) \\big ] \\mathrm{d}t + \\sigma^2 \\mathrm{d} \\mathbf{w}_t\n\\] where \\(g(t) = \\sigma\\) is the diffusion coefficient and \\(\\mathbf{w}_t\\) is brownian motion.\n\nclass VESDETerms:\n    \"\"\"Defines the terms for the Variance Exploding SDE.\"\"\"\n    def __init__(self, sigma_max, device=None):\n        self.sigma = sigma_max\n        self.device = device\n\n    def marginal_prob_std(self, t):\n        t = torch.as_tensor(t, device=self.device, dtype=torch.float32)\n        return self.sigma * torch.sqrt(t)\n\n    def diffusion_coeff(self, t):\n        t = torch.as_tensor(t, device=self.device, dtype=torch.float32)\n        return self.sigma * torch.ones_like(t)\n\n\nScoreNet Architecture\nOur score network is a simple Multi-Layer Perceptron (MLP). It takes a noisy data point x and a time step t as inputs, and returns the estimated score. The conditioning on time step t is performed via the Fourier embedding, a standard method of time conditioning in diffusion models.\n\n\nPyTorch ScoreNet\n@torch.no_grad()\ndef update_ema(ema_model, model, decay=0.9999):\n    \"\"\"\n    Step the EMA model towards the current model.\n    \"\"\"\n    ema_params = OrderedDict(ema_model.named_parameters())\n    model_params = OrderedDict(model.named_parameters())\n\n    for name, param in model_params.items():\n        if param.requires_grad == True:\n            ema_params[name].mul_(decay).add_(param.data, alpha=1. - decay)\n\nclass FourierEmbedding(torch.nn.Module):\n    \"\"\"Embeds time `t` into a high-dimensional feature space.\"\"\"\n    def __init__(self, embed_dim, scale=16):\n        super().__init__()\n        self.register_buffer('freqs', torch.randn(embed_dim // 2) * scale)\n\n\n    def forward(self, x):\n        x = x.ger((2. * torch.pi * self.freqs).to(x.dtype))\n        x = torch.cat([x.cos(), x.sin()], dim=1)\n        return x\n\nclass ScoreNet(nn.Module):\n    \"\"\"The score-based model (a simple MLP).\"\"\"\n    def __init__(\n        self,\n        input_dim=2,\n        num_layers=4,\n        hidden_dim=128,\n        embed_dim=128,\n        marginal_prob_std=None\n    ):\n        super().__init__()\n\n        self.act = nn.SiLU()\n        self.marginal_prob_std = marginal_prob_std\n\n        # Time embedding\n        self.time_embed = nn.Sequential(\n            FourierEmbedding(embed_dim=embed_dim),\n            nn.Linear(embed_dim, embed_dim),\n            nn.SiLU(),\n            nn.Linear(embed_dim, embed_dim),\n            nn.SiLU()\n        )\n\n        # Project combined (x + time-embedding) to hidden dimension\n        self.input_proj = nn.Linear(input_dim + embed_dim, hidden_dim)\n\n        # Hidden MLP layers\n        layers = []\n        for _ in range(num_layers - 1):\n            layers.append(nn.Linear(hidden_dim, hidden_dim))\n\n            if _ == num_layers - 2:\n                layers.append(nn.LayerNorm(hidden_dim))\n                layers.append(nn.SiLU())\n            else:\n                layers.append(nn.SiLU())\n\n        self.hidden = nn.Sequential(*layers)\n\n        # Final output to 2 dimensions\n        self.output = nn.Linear(hidden_dim, 2)\n\n    def forward(self, x, t):\n        # Generate time embedding and concatenate with x\n        t_emb = self.time_embed(t)\n        h = torch.cat([x, t_emb], dim=1)\n\n        # Pass through MLP\n        h = self.input_proj(h)\n        h = self.hidden(h)\n        h = self.output(h)\n\n        # Scale by 1 / marginal_prob_std(t)\n        return h / self.marginal_prob_std(t)[:, None]",
    "crumbs": [
      "tutorial",
      "Memory and Diffusion"
    ]
  },
  {
    "objectID": "tutorial/diffusion_as_memory.html#training",
    "href": "tutorial/diffusion_as_memory.html#training",
    "title": "Memory and Diffusion",
    "section": "Training",
    "text": "Training\nWe use the denoising score matching (DSM) loss. The goal is to train the ScoreNet model so that its output, the score, matches the direction of the noise z that was added to the clean data x at each time step t. \\[\n\\mathcal{L} = \\mathbb{E}_{\\mathbf{x}_0, \\mathbf{x}_t, t} \\, \\bigg  [ \\lambda(t) \\lVert s_\\theta (\\mathbf{x}_t, t) -  \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t | \\mathbf{x}_0) \\rVert^2 \\bigg ]\n\\] where \\(\\mathbf{x}_0\\) denotes the clean data point and \\(\\mathbf{x}_t\\) is the perturbed data point.\nFor example, assume \\(\\tilde{\\mathbf{x}} \\sim \\mathcal{N} (\\tilde{\\mathbf{x}} | \\mathbf{x}, \\sigma^2 \\mathbf{I})\\) for the simple case of DSM. We have the following: \\[\n\\nabla_{\\tilde{\\mathbf{x}}} \\log p(\\tilde{\\mathbf{x}} | \\mathbf{x}) = \\nabla_\\tilde{\\mathbf{x}} \\bigg ( -\\frac{1}{2\\sigma^2} (\\tilde{\\mathbf{x}} - \\mathbf{x})^2 \\bigg ) = -\\frac{\\tilde{\\mathbf{x}} - \\mathbf{x}}{\\sigma^2} = -\\frac{\\mathbf{z}}{\\sigma}\n\\] as the score function, which we have to learn for a single timestep of denoising. \\(\\mathbf{z} \\sim \\mathcal{N}(0, \\mathbf{I})\\).\n\n\nTraining Loop and Loss Function\ndef loss_fn(model, x, marginal_prob_std, eps=1e-5):\n    \"\"\"The denoising score matching loss function.\"\"\"\n    # Sample a random time t\n    random_t = torch.rand(x.shape[0], device=x.device) * (1. - eps) + eps\n\n    # Sample a random noise vector\n    z = torch.randn_like(x)\n    std = marginal_prob_std(random_t)[:, None]\n\n    # Create the noisy data point\n    perturbed_x = x + z * std\n\n    # Get the model's score prediction: -z / 𝜎\n    score = model(perturbed_x, random_t)\n\n    # Calculate the loss\n    loss = torch.mean(torch.square(score * std + z))\n    return loss\n\ndef train_loop(train_loader, vesde, iterations=100_000, lr=1e-4, device='cuda', log_freq=10_000):\n    # create our score model\n    score_model = ScoreNet(marginal_prob_std=vesde.marginal_prob_std)\n\n    # create an exponential moving average version of the model\n    ema = deepcopy(score_model).to(device)\n    score_model = score_model.to(device)\n\n    # create optimizer\n    optimizer = torch.optim.Adam(score_model.parameters(), lr=lr)\n\n    # Use an infinite data loader to cycle through our small dataset\n    infinite_loader = iter(cycle(train_loader))\n\n    # --- Training ---\n    score_model.train()\n    running_loss = 0.\n    pbar = tqdm(range(iterations))\n    for iteration in pbar:\n        # Get a batch of data\n        x = next(infinite_loader).to(device)\n\n        # Calculate loss\n        loss = loss_fn(score_model, x, vesde.marginal_prob_std)\n        running_loss += loss.item()\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        # udpate ema\n        update_ema(ema, score_model)\n\n        # Log progress\n        if iteration % log_freq == 0 and iteration &gt; 0:\n            pbar.set_description(f\"Loss: {running_loss / log_freq:.4f}\")\n            running_loss = 0.\n\n    # return the exponential moving average model\n    ema.eval()\n    return ema\n\n\nTime to train! The following code takes a few minutes to run, but the results are cached after the first run.\n\n# Train our SDE-based diffusion models for training data sizes: 2, 9, and 1000.\nSEED = 9       # For reproducibility\nLR = 1e-4\nSIGMA_MAX = 1.\nITERATIONS = 50_000\n\n# Instantiate the SDE and the Model\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\" \nvesde = VESDETerms(sigma_max=SIGMA_MAX, device=device)\n\nCACHE_MODELS = True\n\ndef make_cache_name(sample_size, seed, lr, sigma_max, iterations):\n    return f\"ema_model_{sample_size}_{seed}_{lr}_{sigma_max}_{iterations}.pth\"\n\ndef get_ema_model(cache_name):\n    cache_path = CACHE_DIR / cache_name\n    if cache_path.exists():\n        model = ScoreNet(marginal_prob_std=vesde.marginal_prob_std)\n        state_dict = torch.load(cache_path)\n        model.load_state_dict(state_dict)\n        return model\n    else:\n        return None # Will need to train it\n\ndata_sizes = [2, 9, 1000] # Takes ~5 min on an M1 Pro CPU\nema_set, pattern_set = [], [] # store our ema models and training patterns into two separate lists\n\nfor sample_size in data_sizes:\n    ema = None        \n    cache_name = make_cache_name(sample_size, SEED, LR, SIGMA_MAX, ITERATIONS)\n    # Extract the training data points for visualization\n    batch_size = min(500, sample_size)  # Use all data points in each batch\n    train_subset = create_subset(dataset, sample_size, SEED)\n    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n\n    if CACHE_MODELS:\n        ema = get_ema_model(cache_name)\n    if ema is None or not CACHE_MODELS: \n        # Train the model if no cache exists\n        torch.manual_seed(SEED)\n        ema = train_loop(train_loader, vesde, ITERATIONS, LR, device=device)\n        torch.save(ema.state_dict(), CACHE_DIR / f\"{cache_name}\")\n        \n    patterns = train_subset.dataset[train_subset.indices]\n    pattern_set.append(patterns)\n    ema_set.append(ema)",
    "crumbs": [
      "tutorial",
      "Memory and Diffusion"
    ]
  },
  {
    "objectID": "tutorial/diffusion_as_memory.html#sampling-from-the-trained-model",
    "href": "tutorial/diffusion_as_memory.html#sampling-from-the-trained-model",
    "title": "Memory and Diffusion",
    "section": "Sampling from the Trained Model",
    "text": "Sampling from the Trained Model\nTo generate new samples, we run the diffusion process in reverse. We start with pure random noise (sampled at t=1) and use our trained score model to guide it back towards the data distribution (towards t=0). This is done using a numerical SDE solver, like the Euler-Maruyama method.\n\n\nEuler-Maruyama Sampler\ndef Euler_Maruyama_sampler(score_model,\n                           sde,\n                           batch_size=64,\n                           num_steps=1000,\n                           device='cuda',\n                           eps=1e-5):\n    \"\"\"Generate samples from the score-based model using the Euler-Maruyama solver.\"\"\"\n    score_model.eval()\n    t_end = torch.ones(batch_size, device=device)\n\n    # Start with orthogonalized random noise ~ N(0, sigma_max^2 * I)\n    init_x = torch.randn(batch_size, 2, device=device)\n    init_x = init_x  / torch.norm(init_x, dim = (1), keepdim=True)\n    init_x = init_x * sde.marginal_prob_std(t_end)[:, None]\n\n    time_steps = torch.linspace(1., eps, num_steps, device=device)\n    step_size = time_steps[0] - time_steps[1] #dt\n    x = init_x\n\n    with torch.no_grad():\n        for time_step in tqdm(time_steps, desc=\"Sampling\"):\n            batch_time_step = torch.ones(batch_size, device=device) * time_step\n            g = sde.diffusion_coeff(batch_time_step)\n            # This is the reverse SDE update step\n            mean_x = x + (g**2)[:, None] * score_model(x, batch_time_step) * step_size\n            eps = torch.randn_like(x)\n            noise = torch.sqrt(step_size) * g[:, None] * eps\n            x = mean_x + noise\n\n    score_model.train()\n    return mean_x # Return the final denoised sample\n\n\nWe can now sample from the trained models.\n\n\nSampling from the Trained Models\ngenerated_set = []\n\nfor ema in ema_set:\n    generated_samples = Euler_Maruyama_sampler(ema, vesde, batch_size=1_500, device=device)\n    generated_samples = generated_samples.detach().cpu().numpy()\n    generated_set.append(generated_samples)",
    "crumbs": [
      "tutorial",
      "Memory and Diffusion"
    ]
  },
  {
    "objectID": "tutorial/diffusion_as_memory.html#potential-energy-of-generated-and-data-samples",
    "href": "tutorial/diffusion_as_memory.html#potential-energy-of-generated-and-data-samples",
    "title": "Memory and Diffusion",
    "section": "Potential Energy of Generated and Data Samples",
    "text": "Potential Energy of Generated and Data Samples\nRecall the relationship between energy and probability denoted by the Boltzmann distribution: \\[\n    p_\\theta(\\mathbf{x}) = \\frac{\\exp{(-E_\\theta(\\mathbf{x}))} }{Z_\\theta}\n\\] This indicates that our energy (up to a constant) is obtained by computing the negative log-likelihood: \\[\n    -\\log p_\\theta(\\mathbf{x}) = E_\\theta(\\mathbf{x}) + C\n\\]\nSince we are dealing with a non-equilibrium system, that is our diffusion model, we follow the formulations and codes provided in Song et al. (2021) to compute the log-likelihood: \\[\n    \\log p_0(\\mathbf{x}_0;\\mathbf{\\theta}) = \\log p_T(\\mathbf{x}_T; \\theta) + \\int_0^T \\nabla \\cdot \\tilde{\\mathbf{f}}(\\mathbf{x}_t, t)  \\mathrm{d}t\n\\] where \\[\n    \\tilde{\\mathbf{f}}(\\mathbf{x}_t, t) =-\\frac{1}{2}\\sigma^2 \\nabla_{\\mathbf{x}_{t}} \\log p_t(\\mathbf{x}_{t}; \\theta)\n\\] for this setting. Keep in mind, \\(\\nabla \\cdot ()\\) denotes the laplacian operation.\nTo derive the above equation, we start with the Fokker-Planck equation, as did in Chen et al. (2018) and Song et al. (2021), which yields the following general probability flow ODE (derived from the forward process SDE): \\[\n    \\mathrm{d} \\mathbf{x}_t = \\tilde{\\mathbf{f}}(\\mathbf{x}_t, t)\\mathrm{d} t + \\tilde{\\mathbf{g}} (\\mathbf{x}_t, t) \\mathrm{d} \\mathbf{w}_t\n\\] where \\[\\tilde{\\mathbf{f}}(\\mathbf{x}_t, t) = \\mathbf{f} (\\mathbf{x}_t, t) - \\frac{1}{2} \\nabla \\cdot \\big[\\mathbf{g} (\\mathbf{x}_t, t) \\mathbf{g} (\\mathbf{x}_t, t)^\\top \\big] - \\frac{1}{2}  \\big [ \\mathbf{g} (\\mathbf{x}_t, t) \\mathbf{g} (\\mathbf{x}_t, t)^\\top \\big ] \\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x}_t)\\] and \\(\\tilde{\\mathbf{g}} (\\mathbf{x}_t, t) = 0\\).\nHere, \\(\\mathbf{f}(\\mathbf{x}_t, t) = 0\\) denotes the drift term which vanishes in the VE setting while \\(\\mathbf{g} (\\mathbf{x}_t, t) = \\sigma\\) which is constant in this setting. Thus, the probability flow ODE for our setting is simply: \\[\n    \\mathrm{d} \\mathbf{x}_t = \\tilde{\\mathbf{f}}(\\mathbf{x}_t, t)\\mathrm{d} t = -\\frac{1}{2}\\sigma^2 \\nabla_{\\mathbf{x}_{t}} \\log p_t(\\mathbf{x}_{t}) \\, \\mathrm{d}t\n\\]\n\n\nComputing the Laplacian and Log-Likelihood\n# @title Code to Compute Laplacian and Log-Likelihood\ndef compute_laplacian(score_fn, x, t):\n    \"\"\"\n    Compute the Laplacian of the score function using torch.func utilities.\n    This is a more efficient, vectorized alternative to the loop-based approach.\n    \"\"\"\n\n    def get_laplacian_for_single_sample(x_sample, t_sample):\n        \"\"\"\n        A helper function that computes the Laplacian for one sample.\n        This is the function we will vectorize using vmap (like Jax).\n        \"\"\"\n        # We have to unsqueeze since torch module requires a batch dim.\n        f = lambda x_inp: score_fn(x_inp[None], t_sample[None]).squeeze(0)\n\n        # Compute the Jacobian of the score function w.r.t. x.\n        # This gives a D x D matrix where D is the dimension of x.\n        J = torch.func.jacrev(f)(x_sample)\n\n        # The Laplacian is the trace of the Jacobian matrix (sum of its diagonal).\n        return torch.trace(J)\n\n    # Apply vmap over the first dimension for both inputs\n    return torch.func.vmap(get_laplacian_for_single_sample, in_dims=(0, 0))(x, t)\n\n\ndef ode_likelihood_with_laplacian(x, score_model, sde, device='cuda', eps=1e-5):\n    \"\"\"Compute the log-likelihood of x by solving the probability flow ODE.\"\"\"\n    shape = x.shape\n\n    def score_eval_wrapper(sample, time_steps):\n        \"\"\"A wrapper for evaluating the score-based model for the ODE solver.\"\"\"\n        sample = torch.tensor(sample, device=device, dtype=torch.float32).reshape(shape)\n        time_steps = torch.tensor(time_steps, device=device, dtype=torch.float32).reshape((sample.shape[0], ))\n        with torch.no_grad():\n            score = score_model(sample, time_steps)\n        return score.cpu().numpy().reshape((-1, shape[1])).astype(np.float64)\n\n    def laplacian_eval_wrapper(sample, time_steps):\n        \"\"\"A wrapper for evaluating the Laplacian of the score function.\"\"\"\n        sample = torch.tensor(sample, device=device, dtype=torch.float32).reshape(shape)\n        time_steps = torch.tensor(time_steps, device=device, dtype=torch.float32).reshape((sample.shape[0], ))\n        laplacian = compute_laplacian(score_model, sample, time_steps)\n        return laplacian.cpu().numpy().reshape((-1,)).astype(np.float64)\n\n    def ode_func(t, x_and_logp):\n        time_steps = torch.from_numpy(np.ones((shape[0],)) * t).to(device, torch.float32)\n        sample = torch.from_numpy(x_and_logp[:-shape[0]].reshape(shape)).to(device, torch.float32)\n\n        with torch.no_grad():\n            g = sde.diffusion_coeff(time_steps).cpu().numpy()\n\n            score = score_eval_wrapper(sample, time_steps)\n            laplacian = laplacian_eval_wrapper(sample, time_steps)\n\n        drift = -0.5 * g[:, None]**2 * score\n        logp_grad = -0.5 * g**2 * laplacian\n        return np.concatenate([drift.flatten(), logp_grad], axis = 0)\n\n    init = np.concatenate([x.cpu().numpy().flatten(), np.zeros((shape[0],))])\n    res = integrate.solve_ivp(ode_func, (eps, 1.), init, rtol=1e-5, atol=1e-5, method='RK45')\n    zp = torch.tensor(res.y[:, -1], device=device)\n    z = zp[:-shape[0]].reshape(shape)\n    delta_logp = zp[-shape[0]:]\n\n    sigma_max = sde.marginal_prob_std(torch.tensor(1.))\n\n    prior_logp = -shape[1] / 2. * torch.log(2 * np.pi * sigma_max ** 2)\n    prior_logp = prior_logp - torch.sum(z ** 2, dim=-1) / (2 * sigma_max ** 2)\n    return (prior_logp + delta_logp)\n\n\n\nVisualizing the Energy\nSince we are generating quite a lot of synthetic data points, we use hierarchical clustering to get a sense of where the concentrations of these new points are at. To be more informative, we are also displaying the energy profile of these concentrations alongside that of the training data points.\nAt K = 2, we can see that the concentrations of generated points are pretty much surrounding the data points and their energy profile are similar to that of the training data points.\nMeanwhile, at K = 9, we now see local minima of the energy that devitate drastically from the training points. These new local minima of the energy are called spurious patterns.\nFinally, when K = 1000, the energy now very closely matches that of the DenseAM’s derived exact energy, see below.\n\n\nPlot Functions for Energy Landscape in 2D and 3D\n# @title Plot Functions for Energy Landscape in 2D and 3D\n\ndef to_energy(loglikelihood, normalize=True):\n    # nll = E + C\n    energy = -loglikelihood\n    if normalize:\n         # normalize energy by its minimum\n        return energy - energy.min()\n    return energy\n\n\ndef plot_combined_landscape(fig, ax, score_model, sde, patterns, samples, labels, centers, t_eval=1e-5, device='cpu', annotate=True, quiver_stride=10):\n    \"\"\"Visualize the energy landscape with annotated points.\"\"\"\n    score_model.eval()\n\n    # 1. Calculate Energy for Patterns and Centers\n    # Energy for original patterns\n    patterns_tensor = patterns.to(device)\n    logp_patterns = ode_likelihood_with_laplacian(patterns_tensor, score_model, sde, device, eps=t_eval)\n    energy_patterns = to_energy(logp_patterns).cpu().numpy()\n\n    # Energy for found cluster centers\n    centers_tensor = torch.from_numpy(centers).float().to(device)\n    logp_centers = ode_likelihood_with_laplacian(centers_tensor, score_model, sde, device, eps=t_eval)\n    energy_centers = to_energy(logp_centers).cpu().numpy()\n\n    # 2. Create a grid of points\n    bounds=(-1.5, 1.5); resolution=75\n    x_ = torch.linspace(bounds[0], bounds[1], resolution)\n    y_ = torch.linspace(bounds[0], bounds[1], resolution)\n    X, Y = torch.meshgrid(x_, y_, indexing='ij')\n    grid_tensor = torch.stack([X.flatten(), Y.flatten()], dim=1).to(device)\n\n    # 3. Calculate the energy and score for the grid\n    logp_grid = ode_likelihood_with_laplacian(grid_tensor, score_model, sde, device, eps=t_eval)\n    energy_grid = to_energy(logp_grid).cpu().numpy().reshape(resolution, resolution)\n\n    with torch.no_grad():\n        vec_t = torch.ones(grid_tensor.shape[0], device=device) * t_eval\n        scores = score_model(grid_tensor, vec_t).cpu().numpy()\n\n    # 4. Create the comprehensive plot\n    ax.set_title(f'K = {len(patterns)}', fontsize=7.5, pad=-0.025)\n    ax.set_aspect('equal'); ax.grid(False)\n\n    # Plot energy contour\n    contour = ax.contourf(X.cpu(), Y.cpu(), energy_grid, levels=100, cmap='inferno', zorder=0)\n\n    # Plot the unit circle\n    theta = np.linspace(0, 2 * np.pi, 200)\n    ax.plot(np.cos(theta), np.sin(theta), color='white', linestyle='--', alpha=0.6, zorder=1, label='Unit Circle')\n\n    # Plot score field\n    ax.quiver(grid_tensor[:, 0].cpu()[::quiver_stride], grid_tensor[:, 1].cpu()[::quiver_stride],\n              scores[:, 0][::quiver_stride], scores[:, 1][::quiver_stride], color='white', alpha=0.5,\n              width=0.005, headwidth=3, zorder=2)\n\n    # Plot original patterns\n    ax.scatter(patterns[:, 0].cpu(), patterns[:, 1].cpu(), marker=\"*\", alpha=0.75,\n               s=100, color=\"deeppink\", label=\"Original Patterns\", edgecolor='black', zorder=5)\n\n    if annotate: # turn off annotation and plotting center since there are too many patterns at this point\n        # Plot generated samples\n        ax.scatter(samples[:, 0], samples[:, 1], c=labels, cmap='viridis',\n                   s=7.5, alpha=0.2, zorder=3, label='Generated Samples')\n\n        # Plot found cluster centers\n        ax.scatter(centers[:, 0], centers[:, 1], marker='X', s=40,\n                    color='aqua', edgecolor='black', zorder=4, label='Cluster Centers')\n\n        # Add energy annotations for patterns\n        for i, p in enumerate(patterns.cpu().numpy()):\n            ax.annotate(f'{energy_patterns[i]:.2f}', (p[0], p[1]),\n                        xytext=(10, 0), textcoords='offset points', color='deeppink', fontsize=4,\n                        weight='bold', bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"black\", ec=\"lime\", lw=1, alpha=0.6))\n\n        # Add energy annotations for cluster centers\n        for i, c in enumerate(centers):\n            ax.annotate(f'{energy_centers[i]:.2f}', (c[0], c[1]),\n                        xytext=(-15, 0), textcoords='offset points', color='aqua', fontsize=4,\n                        weight='bold', bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"black\", ec=\"yellow\", lw=1, alpha=0.6))\n\n    #ax.set_xlabel('x', fontsize=5.5); ax.set_ylabel('y', fontsize=5.5);\n    ax.set_xticks([]); ax.set_yticks([]);\n    ax.set_xlim(bounds); ax.set_ylim(bounds);\n    return contour\n\n\ndef plot_energy_surface_3d(fig, ax, X, Y, energy, view_angle=(60, -60), title=None):\n    \"\"\"\n    Creates a 3D surface plot of the energy landscape.\n\n    Args:\n        X (np.ndarray): Meshgrid for X coordinates.\n        Y (np.ndarray): Meshgrid for Y coordinates.\n        energy (np.ndarray): 2D array of energy values.\n        view_angle (tuple): Tuple of (elevation, azimuth) for the plot's camera angle.\n    \"\"\"\n    # Plot the 3D surface\n    surface = ax.plot_surface(X, Y, energy, cmap='inferno', rstride=1, cstride=1,\n                              linewidth=0, antialiased=True, alpha=0.9)\n\n    # Set labels and title\n    ax.set_xlabel('x', fontsize=5, labelpad=-18)\n    ax.set_ylabel('y', fontsize=5, labelpad=-18)\n    ax.set_zlabel('Energy', fontsize=5, labelpad=-18, rotation=-90)\n    ax.set_title(title, fontsize=7.5, y=-0.08)\n\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_zticks([])\n\n    # Set a nice viewing angle\n    ax.view_init(elev=view_angle[0], azim=view_angle[1])\n    return surface\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualization of the Energy Landscape Across K training Sizes in 2D\n# This threshold determines how close points need to be to be considered in the same cluster.\n# You may need to tune this value based on your results.\ndist_thresholds = [3, 2, 0.45]\nannotates = [True, True, False]\nt_evals = [0.15] * len(dist_thresholds) # evaluate potentials at eps = 0.15\n\nfig, axs = plt.subplots(1, 3, figsize=(8, 8), constrained_layout=True)\nfor i, (patterns, generated_samples, ema, threshold, t_eval, annotate) in enumerate(zip(pattern_set, generated_set, ema_set, dist_thresholds, t_evals, annotates)):\n    # Perform Agglomerative (Hierarchical) Clustering\n    clustering = AgglomerativeClustering(\n        n_clusters=None,                                      # We let the algorithm find the clusters based on the threshold\n        distance_threshold=threshold\n    ).fit(generated_samples)\n\n    # Find the center of each identified cluster\n    cluster_labels = clustering.labels_\n    n_clusters_found = len(np.unique(cluster_labels))\n\n    cluster_centers = np.array([\n        generated_samples[cluster_labels == i].mean(axis=0)\n        for i in range(n_clusters_found)\n    ])\n\n    contour = plot_combined_landscape(\n        fig, axs[i],\n        ema,\n        vesde,\n        patterns,\n        generated_samples,\n        cluster_labels,\n        cluster_centers,\n        device=device,\n        t_eval=t_eval,\n        annotate=annotate,\n    )\n\naxs[0].legend(loc='lower left', fontsize=4)\n\n# Set the ticks to only be at the min and max\ncbar = fig.colorbar(contour, ax=axs[-1], shrink=0.3)\ncbar.set_label('Energy (Lower is Better)', fontsize=8, labelpad=-10)\nvmin, vmax = contour.get_clim()\ncbar.set_ticks([vmin, vmax])\ncbar.set_ticklabels(['Low', 'High'], fontsize=7.5)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nVisualization of the Energy Landscape Across K training Sizes in 3D\n# This code is slow to run, so we cache the figure\nCACHE_FIG = True\n\nbounds = (-1.5, 1.5)\nresolution = 100\nX_grid, Y_grid = torch.meshgrid(\n    torch.linspace(bounds[0], bounds[1], resolution),\n    torch.linspace(bounds[0], bounds[1], resolution),\n    indexing='ij'\n)\ngrid_tensor = torch.stack([X_grid.ravel(), Y_grid.ravel()], dim=1).to(device)\n\nfig_fname = CACHE_DIR / \"slow_fig.png\"\nif CACHE_FIG and fig_fname.exists():\n    img = Image.open(str(fig_fname))\n    display(img)\nelse:\n    fig, axs = plt.subplots(1, 3, figsize=(8, 6), constrained_layout=True, subplot_kw={\"projection\": \"3d\"})\n\n    for i, (patterns, score_model, annotate, eps) in enumerate(zip(pattern_set, ema_set, annotates, t_evals)):\n        # Calculate the energy for the grid (this is the slow step)\n        logp_grid = ode_likelihood_with_laplacian(grid_tensor, score_model, vesde, device=device, eps=eps)\n        energy_grid = to_energy(logp_grid).cpu().numpy()\n        energy_grid = energy_grid.reshape(resolution, resolution)\n\n        # Create the plot\n        contour = plot_energy_surface_3d(fig, axs[i], X_grid.cpu().numpy(), Y_grid.cpu().numpy(), energy_grid, title=f'K = {len(patterns)}')\n\n        # show the data points again...\n        if annotate:\n            axs[i].scatter(\n                patterns[:, 0].cpu(), patterns[:, 1].cpu(), marker=\"*\", alpha=0.75,\n                s=35, color=\"deeppink\", label=\"Original Patterns\", edgecolor='black', zorder=5\n            )\n\n    # Set the ticks to only be at the min and max\n    cbar = fig.colorbar(contour, ax=axs[-1], shrink=0.25)\n    cbar.set_label('Energy (Lower is Better)', fontsize=8, labelpad=-15)\n\n    vmin, vmax = contour.get_clim()\n    cbar.set_ticks([vmin, vmax])\n\n    cbar.set_ticklabels(['Low', 'High'], fontsize=7)\n    plt.savefig(fig_fname)\n\nplt.show()",
    "crumbs": [
      "tutorial",
      "Memory and Diffusion"
    ]
  },
  {
    "objectID": "tutorial/diffusion_as_memory.html#the-empirical-energy-and-score-function-for-the-toy-model",
    "href": "tutorial/diffusion_as_memory.html#the-empirical-energy-and-score-function-for-the-toy-model",
    "title": "Memory and Diffusion",
    "section": "The Empirical Energy and Score Function for the Toy Model",
    "text": "The Empirical Energy and Score Function for the Toy Model\n\ndef cartesian_to_polar(samples):\n    x, y = samples[:, 0], samples[:, 1]\n    r = np.sqrt(x ** 2 + y ** 2)\n    angles = np.arctan2(y, x)\n    return r, angles\n\ndef energy_am(samples, beta, normalize=True):\n    \"\"\"\n    Computes the energy function\n        E^AM(R, phi) = R^2 + 1 - (1 / beta) * log(I_0(2 * beta * R))\n\n    See Eq. (13) in the paper.\n    \"\"\"\n    r, _ = cartesian_to_polar(samples)\n    energy = r**2 + 1 - (1 / beta) * np.log(i0(2 * beta * r))\n\n    # Shift so that the lowest energy is\n    if normalize:\n        energy = energy - energy.min()\n    return energy\n\ndef score_am(samples, beta, epsilon=1e-6):\n    \"\"\"\n    Computes the score function\n        S^AM(R, phi) = -2 * R - (2 / beta) * I_1(2 * beta * R) / I_0(2 * beta * R)\n\n    See Eq. (18) in the paper.\n    \"\"\"\n    r, _ = cartesian_to_polar(samples)\n\n    # Ensure r is not zero to avoid division by zero\n    r = np.clip(r, epsilon, np.inf)\n\n    bessel_ratio = i1(2 * beta * r) / i0(2 * beta * r)\n\n    score_r = 2 * (bessel_ratio - r)\n    score = score_r[:, None] * samples / r[:, None]\n\n    return score\n\n\n\nVisualization of the Empirical Energy Landscape and its Score Function\n# @title Visualization of the Empirical Energy Landscape and its Score Function\n# Inverse Temperature -- Beta\nbeta = 1 / 0.05\n\ngrid_square = torch.stack([X_grid, Y_grid], dim=1)\nscores = score_am(grid_square, beta)\nenergy = energy_am(grid_square, beta)\nenergy_grid = energy.reshape(resolution, resolution)\n\nfig = plt.figure(figsize=(8, 8), constrained_layout=True)\n\nax = fig.add_subplot(121)\nax.set_title('Exact', fontsize=8, y=-0.1)\nax.set_aspect('equal'); ax.grid(False)\n\n# Plot energy contour\ncontour = ax.contourf(X_grid.cpu(), Y_grid.cpu(), energy_grid, levels=100, cmap='inferno', zorder=0)\n\n# Plot the unit circle\ntheta = np.linspace(0, 2 * np.pi, 200)\nax.plot(np.cos(theta), np.sin(theta), color='white', linestyle='--', alpha=0.6, zorder=1, label='Unit Circle')\n\n\nstride = 5\n# Plot score field\nax.quiver(grid_square[:, 0].cpu()[::stride], grid_square[:, 1].cpu()[::stride],\n          scores[:, 0][::stride], scores[:, 1][::stride], color='white', alpha=0.5,\n          width=0.002, headwidth=3, zorder=2)\n\nax.set_xlabel('x', fontsize=5)\nax.set_ylabel('y', fontsize=5)\nax.set_xticks([]); ax.set_yticks([]);\n\nax = fig.add_subplot(122, projection=\"3d\")\ncontour = plot_energy_surface_3d(fig, ax, X_grid, Y_grid, energy_grid, view_angle=(60, -60), title='Exact')\n\ncbar = fig.colorbar(contour, ax=ax, shrink=0.5)\ncbar.set_label('Energy (Lower is Better)', fontsize=10, labelpad=-18)\n\nvmin, vmax = contour.get_clim()\ncbar.set_ticks([vmin, vmax])\ncbar.set_ticklabels(['Low', 'High'], fontsize=10)\nplt.show()",
    "crumbs": [
      "tutorial",
      "Memory and Diffusion"
    ]
  },
  {
    "objectID": "tutorial/index.html",
    "href": "tutorial/index.html",
    "title": "Tutorials",
    "section": "",
    "text": "Dense Storage in binary networks. A whirlwind tour of the Classical Hopfield Network to Dense Associative Memory.\nEnergy Transformer. Rederiving the Transformer as a pure associative memory\nDiffusion as Memory. Diffusion models try to learn implicitly the energy of data explicitly modeled by AMs.\nDistributed memory. A way to encode dense storage capacity in constant parameters.\n\n\n\n\n\n\n\nNote\n\n\n\nThese tutorials are under construction until July 14, 2025, when this tutorial will be presented at ICML 2025.",
    "crumbs": [
      "tutorial"
    ]
  },
  {
    "objectID": "lib/data_utils.html",
    "href": "lib/data_utils.html",
    "title": "Pokemon Sprites",
    "section": "",
    "text": "We will use a cache directory to store the downloaded, processed data. This defaults to ~/.cache/amtutorial, but can be overridden by setting the AMTUTORIAL_CACHE_DIR environment variable.\n\n\n\n\n get_cache_dir (subfolder:Optional[str]=None)\n\nGet a cross-platform cache directory that works locally and in Google Colab.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsubfolder\nOptional\nNone\nSubdir of cache dir\n\n\nReturns\nPath",
    "crumbs": [
      "lib",
      "Pokemon Sprites"
    ]
  },
  {
    "objectID": "lib/data_utils.html#cache-directory",
    "href": "lib/data_utils.html#cache-directory",
    "title": "Pokemon Sprites",
    "section": "",
    "text": "We will use a cache directory to store the downloaded, processed data. This defaults to ~/.cache/amtutorial, but can be overridden by setting the AMTUTORIAL_CACHE_DIR environment variable.\n\n\n\n\n get_cache_dir (subfolder:Optional[str]=None)\n\nGet a cross-platform cache directory that works locally and in Google Colab.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsubfolder\nOptional\nNone\nSubdir of cache dir\n\n\nReturns\nPath",
    "crumbs": [
      "lib",
      "Pokemon Sprites"
    ]
  },
  {
    "objectID": "lib/data_utils.html#preparing-pokemon",
    "href": "lib/data_utils.html#preparing-pokemon",
    "title": "Pokemon Sprites",
    "section": "Preparing Pokemon",
    "text": "Preparing Pokemon\nFor cutesies and funsies, let’s build our own black and white pokemon dataset from the Pokemon Database. Let’s use a little help from AI to download and process our Pokemon sprites to be binary.\n\nDownloading Pokemon Sprites\n\n\n\ndownload_pokemon_sprites\n\n download_pokemon_sprites (delay:float=0.1, verbose:bool=True)\n\nDownload Pokemon sprite images from pokemondb.net\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndelay\nfloat\n0.1\nDelay between downloads (seconds) to be nice to server\n\n\nverbose\nbool\nTrue\nPrint progress messages\n\n\n\n\n\nProcessing Pokemon Sprites\n\n\n\nscale_to_fit\n\n scale_to_fit (img, target_size)\n\nScale image to fit within target_size while maintaining aspect ratio\n\n\n\nget_sprite_bbox\n\n get_sprite_bbox (img)\n\nGet bounding box of non-transparent pixels\n\n\n\nfloyd_steinberg_dither\n\n floyd_steinberg_dither (gray, strength=1.0)\n\nApply Floyd-Steinberg dithering with adjustable strength\n\n\n\nbinarize_image\n\n binarize_image (img, method='adaptive', invert=False,\n                 dither_strength=1.0)\n\nApply binarization to convert image to black and white\n\n\n\nprocess_pokemon_sprites\n\n process_pokemon_sprites (ds_name:str, canvas_size:int=48,\n                          sprite_percentage:float=0.95,\n                          binarize_method:str='adaptive',\n                          dither_strength:float=1.0,\n                          invert_colors:bool=False, verbose:bool=True,\n                          force:bool=False)\n\n*Process Pokemon sprites: upscale non-transparent pixels, convert to white bg, resize to specified dimensions\nThis script processes Pokemon sprite images by: 1. Cropping excess transparency around sprites 2. Scaling sprites to occupy a specified percentage of the canvas 3. Converting transparent backgrounds to white 4. Optionally converting to black and white using adaptive thresholding or dithering 5. Optionally inverting the binary result (black background, white sprites) 6. Centering sprites on a square canvas*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nds_name\nstr\n\nDataset name\n\n\ncanvas_size\nint\n48\nFinal image size\n\n\nsprite_percentage\nfloat\n0.95\nPct of canvas the sprite should occupy (0.0-1.0)\n\n\nbinarize_method\nstr\nadaptive\nBinarization method: ‘adaptive’ or ‘dithered’\n\n\ndither_strength\nfloat\n1.0\nDithering intensity (0.0=none, 1.0=full Floyd-Steinberg)\n\n\ninvert_colors\nbool\nFalse\nInvert binary colors (only works with binarize=True)\n\n\nverbose\nbool\nTrue\nPrint progress messages\n\n\nforce\nbool\nFalse\nForce re-processing\n\n\n\nWe can experiment with different binarization methods.\n\n# Experimenting with different binarization methods\nprocess_pokemon_sprites(\"processed_adaptive\", binarize_method='adaptive') # Best\n\nProcessing 1025 Pokemon sprites...\nCanvas size: 48x48\nSprite will occupy 95.0% of canvas (45 pixels max)\nBinarization using 'adaptive' method\nProcessed 1/1025: parasect.png\nProcessed 2/1025: sobble.png\nProcessed 3/1025: lumineon.png\nProcessed 4/1025: raikou.png\nProcessed 5/1025: runerigus.png\nProcessed 6/1025: dedenne.png\nProcessed 7/1025: pyroar.png\nProcessed 8/1025: pawmi.png\nProcessed 9/1025: articuno.png\nProcessed 10/1025: meowstic.png\nProcessed 11/1025: magmortar.png\nProcessed 12/1025: bulbasaur.png\nProcessed 13/1025: banette.png\nProcessed 14/1025: staraptor.png\nProcessed 15/1025: pidove.png\nProcessed 16/1025: morgrem.png\nProcessed 17/1025: comfey.png\nProcessed 18/1025: taillow.png\nProcessed 19/1025: charizard.png\nProcessed 20/1025: infernape.png\nProcessed 21/1025: sandshrew.png\nProcessed 22/1025: marshadow.png\nProcessed 23/1025: alakazam.png\nProcessed 24/1025: lickitung.png\nProcessed 25/1025: starmie.png\nProcessed 26/1025: wishiwashi.png\nProcessed 27/1025: yanmega.png\nProcessed 28/1025: yveltal.png\nProcessed 29/1025: zigzagoon.png\nProcessed 30/1025: petilil.png\nProcessed 31/1025: torterra.png\nProcessed 32/1025: purugly.png\nProcessed 33/1025: tandemaus.png\nProcessed 34/1025: cresselia.png\nProcessed 35/1025: regigigas.png\nProcessed 36/1025: palkia.png\nProcessed 37/1025: impidimp.png\nProcessed 38/1025: froakie.png\nProcessed 39/1025: kilowattrel.png\nProcessed 40/1025: arrokuda.png\nProcessed 41/1025: munna.png\nProcessed 42/1025: bellossom.png\nProcessed 43/1025: gabite.png\nProcessed 44/1025: tapu-koko.png\nProcessed 45/1025: mareep.png\nProcessed 46/1025: shieldon.png\nProcessed 47/1025: sneasler.png\nProcessed 48/1025: stoutland.png\nProcessed 49/1025: corviknight.png\nProcessed 50/1025: walking-wake.png\nProcessed 51/1025: accelgor.png\nProcessed 52/1025: makuhita.png\nProcessed 53/1025: seviper.png\nProcessed 54/1025: tirtouga.png\nProcessed 55/1025: lampent.png\nProcessed 56/1025: chi-yu.png\nProcessed 57/1025: ceruledge.png\nProcessed 58/1025: nidoking.png\nProcessed 59/1025: huntail.png\nProcessed 60/1025: nosepass.png\nProcessed 61/1025: carracosta.png\nProcessed 62/1025: nidoqueen.png\nProcessed 63/1025: inkay.png\nProcessed 64/1025: archeops.png\nProcessed 65/1025: avalugg.png\nProcessed 66/1025: grubbin.png\nProcessed 67/1025: sentret.png\nProcessed 68/1025: golbat.png\nProcessed 69/1025: skorupi.png\nProcessed 70/1025: conkeldurr.png\nProcessed 71/1025: jellicent.png\nProcessed 72/1025: yungoos.png\nProcessed 73/1025: meltan.png\nProcessed 74/1025: arctovish.png\nProcessed 75/1025: barraskewda.png\nProcessed 76/1025: luxio.png\nProcessed 77/1025: duraludon.png\nProcessed 78/1025: binacle.png\nProcessed 79/1025: galvantula.png\nProcessed 80/1025: crawdaunt.png\nProcessed 81/1025: hippopotas.png\nProcessed 82/1025: swirlix.png\nProcessed 83/1025: finneon.png\nProcessed 84/1025: kleavor.png\nProcessed 85/1025: medicham.png\nProcessed 86/1025: yamper.png\nProcessed 87/1025: poliwrath.png\nProcessed 88/1025: deoxys.png\nProcessed 89/1025: magnemite.png\nProcessed 90/1025: florges.png\nProcessed 91/1025: serperior.png\nProcessed 92/1025: coalossal.png\nProcessed 93/1025: shinx.png\nProcessed 94/1025: quaquaval.png\nProcessed 95/1025: budew.png\nProcessed 96/1025: shellos.png\nProcessed 97/1025: gothita.png\nProcessed 98/1025: overqwil.png\nProcessed 99/1025: porygon.png\nProcessed 100/1025: alomomola.png\nProcessed 101/1025: urshifu.png\nProcessed 102/1025: electrike.png\nProcessed 103/1025: dratini.png\nProcessed 104/1025: mienfoo.png\nProcessed 105/1025: reshiram.png\nProcessed 106/1025: sawsbuck.png\nProcessed 107/1025: blaziken.png\nProcessed 108/1025: flareon.png\nProcessed 109/1025: gliscor.png\nProcessed 110/1025: heatran.png\nProcessed 111/1025: gholdengo.png\nProcessed 112/1025: thievul.png\nProcessed 113/1025: beedrill.png\nProcessed 114/1025: butterfree.png\nProcessed 115/1025: tadbulb.png\nProcessed 116/1025: fletchinder.png\nProcessed 117/1025: tsareena.png\nProcessed 118/1025: ludicolo.png\nProcessed 119/1025: phanpy.png\nProcessed 120/1025: toxicroak.png\nProcessed 121/1025: cyndaquil.png\nProcessed 122/1025: swellow.png\nProcessed 123/1025: machop.png\nProcessed 124/1025: eternatus.png\nProcessed 125/1025: fletchling.png\nProcessed 126/1025: naganadel.png\nProcessed 127/1025: ariados.png\nProcessed 128/1025: mr-mime.png\nProcessed 129/1025: passimian.png\nProcessed 130/1025: frogadier.png\nProcessed 131/1025: roggenrola.png\nProcessed 132/1025: shroodle.png\nProcessed 133/1025: shedinja.png\nProcessed 134/1025: duosion.png\nProcessed 135/1025: scraggy.png\nProcessed 136/1025: gumshoos.png\nProcessed 137/1025: latias.png\nProcessed 138/1025: shelmet.png\nProcessed 139/1025: sawk.png\nProcessed 140/1025: pawmo.png\nProcessed 141/1025: venusaur.png\nProcessed 142/1025: azelf.png\nProcessed 143/1025: sinistea.png\nProcessed 144/1025: tornadus.png\nProcessed 145/1025: quaxwell.png\nProcessed 146/1025: obstagoon.png\nProcessed 147/1025: musharna.png\nProcessed 148/1025: drampa.png\nProcessed 149/1025: liepard.png\nProcessed 150/1025: aipom.png\nProcessed 151/1025: seaking.png\nProcessed 152/1025: arboliva.png\nProcessed 153/1025: xurkitree.png\nProcessed 154/1025: meditite.png\nProcessed 155/1025: heatmor.png\nProcessed 156/1025: chandelure.png\nProcessed 157/1025: ivysaur.png\nProcessed 158/1025: calyrex.png\nProcessed 159/1025: gible.png\nProcessed 160/1025: corphish.png\nProcessed 161/1025: octillery.png\nProcessed 162/1025: cubone.png\nProcessed 163/1025: crocalor.png\nProcessed 164/1025: dusclops.png\nProcessed 165/1025: koffing.png\nProcessed 166/1025: wo-chien.png\nProcessed 167/1025: gengar.png\nProcessed 168/1025: larvitar.png\nProcessed 169/1025: rampardos.png\nProcessed 170/1025: tyrogue.png\nProcessed 171/1025: wigglytuff.png\nProcessed 172/1025: oricorio.png\nProcessed 173/1025: castform.png\nProcessed 174/1025: quaxly.png\nProcessed 175/1025: dusknoir.png\nProcessed 176/1025: chinchou.png\nProcessed 177/1025: litleo.png\nProcessed 178/1025: floragato.png\nProcessed 179/1025: manectric.png\nProcessed 180/1025: grapploct.png\nProcessed 181/1025: ditto.png\nProcessed 182/1025: scrafty.png\nProcessed 183/1025: great-tusk.png\nProcessed 184/1025: kakuna.png\nProcessed 185/1025: gastrodon.png\nProcessed 186/1025: wingull.png\nProcessed 187/1025: flaaffy.png\nProcessed 188/1025: sunflora.png\nProcessed 189/1025: iron-jugulis.png\nProcessed 190/1025: gastly.png\nProcessed 191/1025: tarountula.png\nProcessed 192/1025: delcatty.png\nProcessed 193/1025: granbull.png\nProcessed 194/1025: iron-leaves.png\nProcessed 195/1025: whismur.png\nProcessed 196/1025: gourgeist.png\nProcessed 197/1025: cleffa.png\nProcessed 198/1025: growlithe.png\nProcessed 199/1025: sigilyph.png\nProcessed 200/1025: cascoon.png\nProcessed 201/1025: pecharunt.png\nProcessed 202/1025: rapidash.png\nProcessed 203/1025: hydreigon.png\nProcessed 204/1025: cryogonal.png\nProcessed 205/1025: stunfisk.png\nProcessed 206/1025: stantler.png\nProcessed 207/1025: dracovish.png\nProcessed 208/1025: tapu-fini.png\nProcessed 209/1025: steenee.png\nProcessed 210/1025: slither-wing.png\nProcessed 211/1025: snubbull.png\nProcessed 212/1025: crabominable.png\nProcessed 213/1025: surskit.png\nProcessed 214/1025: ampharos.png\nProcessed 215/1025: monferno.png\nProcessed 216/1025: terapagos.png\nProcessed 217/1025: rhydon.png\nProcessed 218/1025: orthworm.png\nProcessed 219/1025: clodsire.png\nProcessed 220/1025: arbok.png\nProcessed 221/1025: amaura.png\nProcessed 222/1025: lucario.png\nProcessed 223/1025: hatterene.png\nProcessed 224/1025: pupitar.png\nProcessed 225/1025: elekid.png\nProcessed 226/1025: goomy.png\nProcessed 227/1025: vullaby.png\nProcessed 228/1025: venipede.png\nProcessed 229/1025: doublade.png\nProcessed 230/1025: bellibolt.png\nProcessed 231/1025: regieleki.png\nProcessed 232/1025: tangela.png\nProcessed 233/1025: pignite.png\nProcessed 234/1025: sandile.png\nProcessed 235/1025: brionne.png\nProcessed 236/1025: machamp.png\nProcessed 237/1025: wailord.png\nProcessed 238/1025: rhyhorn.png\nProcessed 239/1025: ninjask.png\nProcessed 240/1025: salandit.png\nProcessed 241/1025: grotle.png\nProcessed 242/1025: dragonair.png\nProcessed 243/1025: mime-jr.png\nProcessed 244/1025: zebstrika.png\nProcessed 245/1025: zarude.png\nProcessed 246/1025: pikachu.png\nProcessed 247/1025: typhlosion.png\nProcessed 248/1025: sizzlipede.png\nProcessed 249/1025: aromatisse.png\nProcessed 250/1025: chimecho.png\nProcessed 251/1025: diggersby.png\nProcessed 252/1025: trevenant.png\nProcessed 253/1025: audino.png\nProcessed 254/1025: dodrio.png\nProcessed 255/1025: azumarill.png\nProcessed 256/1025: perrserker.png\nProcessed 257/1025: aggron.png\nProcessed 258/1025: dialga.png\nProcessed 259/1025: tapu-bulu.png\nProcessed 260/1025: totodile.png\nProcessed 261/1025: tauros.png\nProcessed 262/1025: silcoon.png\nProcessed 263/1025: cottonee.png\nProcessed 264/1025: araquanid.png\nProcessed 265/1025: gallade.png\nProcessed 266/1025: tropius.png\nProcessed 267/1025: bouffalant.png\nProcessed 268/1025: wattrel.png\nProcessed 269/1025: frigibax.png\nProcessed 270/1025: woobat.png\nProcessed 271/1025: archen.png\nProcessed 272/1025: ho-oh.png\nProcessed 273/1025: scorbunny.png\nProcessed 274/1025: iron-valiant.png\nProcessed 275/1025: noibat.png\nProcessed 276/1025: wartortle.png\nProcessed 277/1025: chien-pao.png\nProcessed 278/1025: eevee.png\nProcessed 279/1025: mewtwo.png\nProcessed 280/1025: buneary.png\nProcessed 281/1025: sableye.png\nProcessed 282/1025: bidoof.png\nProcessed 283/1025: rookidee.png\nProcessed 284/1025: scovillain.png\nProcessed 285/1025: braixen.png\nProcessed 286/1025: raging-bolt.png\nProcessed 287/1025: elgyem.png\nProcessed 288/1025: gloom.png\nProcessed 289/1025: electabuzz.png\nProcessed 290/1025: piloswine.png\nProcessed 291/1025: miltank.png\nProcessed 292/1025: guzzlord.png\nProcessed 293/1025: cinccino.png\nProcessed 294/1025: greavard.png\nProcessed 295/1025: hattrem.png\nProcessed 296/1025: hatenna.png\nProcessed 297/1025: anorith.png\nProcessed 298/1025: wurmple.png\nProcessed 299/1025: shelgon.png\nProcessed 300/1025: toxapex.png\nProcessed 301/1025: swalot.png\nProcessed 302/1025: lurantis.png\nProcessed 303/1025: tyrunt.png\nProcessed 304/1025: pidgey.png\nProcessed 305/1025: quagsire.png\nProcessed 306/1025: gurdurr.png\nProcessed 307/1025: amoonguss.png\nProcessed 308/1025: archaludon.png\nProcessed 309/1025: mankey.png\nProcessed 310/1025: illumise.png\nProcessed 311/1025: cacturne.png\nProcessed 312/1025: tinkatink.png\nProcessed 313/1025: igglybuff.png\nProcessed 314/1025: maschiff.png\nProcessed 315/1025: qwilfish.png\nProcessed 316/1025: goldeen.png\nProcessed 317/1025: abra.png\nProcessed 318/1025: tinkatuff.png\nProcessed 319/1025: nidoran-f.png\nProcessed 320/1025: purrloin.png\nProcessed 321/1025: gulpin.png\nProcessed 322/1025: munchlax.png\nProcessed 323/1025: crobat.png\nProcessed 324/1025: volcanion.png\nProcessed 325/1025: lillipup.png\nProcessed 326/1025: xerneas.png\nProcessed 327/1025: lechonk.png\nProcessed 328/1025: lapras.png\nProcessed 329/1025: gyarados.png\nProcessed 330/1025: walrein.png\nProcessed 331/1025: sneasel.png\nProcessed 332/1025: lickilicky.png\nProcessed 333/1025: skuntank.png\nProcessed 334/1025: murkrow.png\nProcessed 335/1025: bibarel.png\nProcessed 336/1025: tentacool.png\nProcessed 337/1025: watchog.png\nProcessed 338/1025: oshawott.png\nProcessed 339/1025: pidgeotto.png\nProcessed 340/1025: espurr.png\nProcessed 341/1025: houndoom.png\nProcessed 342/1025: nidorino.png\nProcessed 343/1025: hariyama.png\nProcessed 344/1025: milcery.png\nProcessed 345/1025: talonflame.png\nProcessed 346/1025: dustox.png\nProcessed 347/1025: mimikyu.png\nProcessed 348/1025: kommo-o.png\nProcessed 349/1025: weepinbell.png\nProcessed 350/1025: seel.png\nProcessed 351/1025: houndstone.png\nProcessed 352/1025: pyukumuku.png\nProcessed 353/1025: naclstack.png\nProcessed 354/1025: annihilape.png\nProcessed 355/1025: pineco.png\nProcessed 356/1025: cosmog.png\nProcessed 357/1025: kadabra.png\nProcessed 358/1025: swablu.png\nProcessed 359/1025: skitty.png\nProcessed 360/1025: regidrago.png\nProcessed 361/1025: masquerain.png\nProcessed 362/1025: minun.png\nProcessed 363/1025: heracross.png\nProcessed 364/1025: cursola.png\nProcessed 365/1025: trapinch.png\nProcessed 366/1025: girafarig.png\nProcessed 367/1025: tyrantrum.png\nProcessed 368/1025: graveler.png\nProcessed 369/1025: corvisquire.png\nProcessed 370/1025: poochyena.png\nProcessed 371/1025: electivire.png\nProcessed 372/1025: shiinotic.png\nProcessed 373/1025: melmetal.png\nProcessed 374/1025: ledian.png\nProcessed 375/1025: cherrim.png\nProcessed 376/1025: popplio.png\nProcessed 377/1025: sandy-shocks.png\nProcessed 378/1025: polteageist.png\nProcessed 379/1025: primeape.png\nProcessed 380/1025: glastrier.png\nProcessed 381/1025: vanillish.png\nProcessed 382/1025: aegislash.png\nProcessed 383/1025: probopass.png\nProcessed 384/1025: skiploom.png\nProcessed 385/1025: chesnaught.png\nProcessed 386/1025: kyurem.png\nProcessed 387/1025: drakloak.png\nProcessed 388/1025: carnivine.png\nProcessed 389/1025: torracat.png\nProcessed 390/1025: gardevoir.png\nProcessed 391/1025: pinsir.png\nProcessed 392/1025: snorunt.png\nProcessed 393/1025: samurott.png\nProcessed 394/1025: necrozma.png\nProcessed 395/1025: darmanitan.png\nProcessed 396/1025: charcadet.png\nProcessed 397/1025: smeargle.png\nProcessed 398/1025: cetoddle.png\nProcessed 399/1025: rellor.png\nProcessed 400/1025: inteleon.png\nProcessed 401/1025: magby.png\nProcessed 402/1025: ursaring.png\nProcessed 403/1025: crabrawler.png\nProcessed 404/1025: haxorus.png\nProcessed 405/1025: geodude.png\nProcessed 406/1025: foongus.png\nProcessed 407/1025: hippowdon.png\nProcessed 408/1025: poltchageist.png\nProcessed 409/1025: toedscool.png\nProcessed 410/1025: ursaluna.png\nProcessed 411/1025: delibird.png\nProcessed 412/1025: oranguru.png\nProcessed 413/1025: drapion.png\nProcessed 414/1025: koraidon.png\nProcessed 415/1025: smoochum.png\nProcessed 416/1025: scyther.png\nProcessed 417/1025: stunky.png\nProcessed 418/1025: swadloon.png\nProcessed 419/1025: klefki.png\nProcessed 420/1025: vivillon.png\nProcessed 421/1025: wormadam.png\nProcessed 422/1025: honedge.png\nProcessed 423/1025: entei.png\nProcessed 424/1025: servine.png\nProcessed 425/1025: zubat.png\nProcessed 426/1025: umbreon.png\nProcessed 427/1025: reuniclus.png\nProcessed 428/1025: froslass.png\nProcessed 429/1025: pheromosa.png\nProcessed 430/1025: brute-bonnet.png\nProcessed 431/1025: magcargo.png\nProcessed 432/1025: boldore.png\nProcessed 433/1025: genesect.png\nProcessed 434/1025: larvesta.png\nProcessed 435/1025: meowscarada.png\nProcessed 436/1025: golett.png\nProcessed 437/1025: kartana.png\nProcessed 438/1025: voltorb.png\nProcessed 439/1025: regirock.png\nProcessed 440/1025: cherubi.png\nProcessed 441/1025: barboach.png\nProcessed 442/1025: wooper.png\nProcessed 443/1025: escavalier.png\nProcessed 444/1025: vileplume.png\nProcessed 445/1025: mismagius.png\nProcessed 446/1025: blastoise.png\nProcessed 447/1025: litten.png\nProcessed 448/1025: sealeo.png\nProcessed 449/1025: dreepy.png\nProcessed 450/1025: snivy.png\nProcessed 451/1025: keldeo.png\nProcessed 452/1025: snorlax.png\nProcessed 453/1025: zamazenta.png\nProcessed 454/1025: lopunny.png\nProcessed 455/1025: persian.png\nProcessed 456/1025: beartic.png\nProcessed 457/1025: gothorita.png\nProcessed 458/1025: giratina.png\nProcessed 459/1025: rattata.png\nProcessed 460/1025: starly.png\nProcessed 461/1025: bewear.png\nProcessed 462/1025: nymble.png\nProcessed 463/1025: houndour.png\nProcessed 464/1025: eelektrik.png\nProcessed 465/1025: appletun.png\nProcessed 466/1025: quilava.png\nProcessed 467/1025: whimsicott.png\nProcessed 468/1025: magikarp.png\nProcessed 469/1025: skeledirge.png\nProcessed 470/1025: drowzee.png\nProcessed 471/1025: squawkabilly.png\nProcessed 472/1025: claydol.png\nProcessed 473/1025: clamperl.png\nProcessed 474/1025: seedot.png\nProcessed 475/1025: dottler.png\nProcessed 476/1025: garchomp.png\nProcessed 477/1025: skiddo.png\nProcessed 478/1025: hypno.png\nProcessed 479/1025: weedle.png\nProcessed 480/1025: garganacl.png\nProcessed 481/1025: greedent.png\nProcessed 482/1025: stakataka.png\nProcessed 483/1025: ambipom.png\nProcessed 484/1025: spoink.png\nProcessed 485/1025: pikipek.png\nProcessed 486/1025: frillish.png\nProcessed 487/1025: cutiefly.png\nProcessed 488/1025: honchkrow.png\nProcessed 489/1025: tympole.png\nProcessed 490/1025: ting-lu.png\nProcessed 491/1025: meganium.png\nProcessed 492/1025: incineroar.png\nProcessed 493/1025: landorus.png\nProcessed 494/1025: lileep.png\nProcessed 495/1025: pachirisu.png\nProcessed 496/1025: iron-moth.png\nProcessed 497/1025: tyranitar.png\nProcessed 498/1025: magnezone.png\nProcessed 499/1025: beldum.png\nProcessed 500/1025: glameow.png\nProcessed 501/1025: dondozo.png\nProcessed 502/1025: swanna.png\nProcessed 503/1025: primarina.png\nProcessed 504/1025: nuzleaf.png\nProcessed 505/1025: zangoose.png\nProcessed 506/1025: gogoat.png\nProcessed 507/1025: jolteon.png\nProcessed 508/1025: moltres.png\nProcessed 509/1025: dragonite.png\nProcessed 510/1025: golurk.png\nProcessed 511/1025: metapod.png\nProcessed 512/1025: golisopod.png\nProcessed 513/1025: marowak.png\nProcessed 514/1025: gigalith.png\nProcessed 515/1025: emboar.png\nProcessed 516/1025: exeggutor.png\nProcessed 517/1025: magneton.png\nProcessed 518/1025: mesprit.png\nProcessed 519/1025: dipplin.png\nProcessed 520/1025: groudon.png\nProcessed 521/1025: wobbuffet.png\nProcessed 522/1025: delphox.png\nProcessed 523/1025: exploud.png\nProcessed 524/1025: dracozolt.png\nProcessed 525/1025: mantyke.png\nProcessed 526/1025: malamar.png\nProcessed 527/1025: simisage.png\nProcessed 528/1025: espeon.png\nProcessed 529/1025: paras.png\nProcessed 530/1025: noctowl.png\nProcessed 531/1025: hitmonlee.png\nProcessed 532/1025: slaking.png\nProcessed 533/1025: solrock.png\nProcessed 534/1025: pelipper.png\nProcessed 535/1025: komala.png\nProcessed 536/1025: poliwhirl.png\nProcessed 537/1025: palpitoad.png\nProcessed 538/1025: eelektross.png\nProcessed 539/1025: patrat.png\nProcessed 540/1025: spheal.png\nProcessed 541/1025: seadra.png\nProcessed 542/1025: lotad.png\nProcessed 543/1025: flutter-mane.png\nProcessed 544/1025: ralts.png\nProcessed 545/1025: registeel.png\nProcessed 546/1025: slurpuff.png\nProcessed 547/1025: arcanine.png\nProcessed 548/1025: fomantis.png\nProcessed 549/1025: smoliv.png\nProcessed 550/1025: eiscue.png\nProcessed 551/1025: tangrowth.png\nProcessed 552/1025: aerodactyl.png\nProcessed 553/1025: pincurchin.png\nProcessed 554/1025: bronzor.png\nProcessed 555/1025: sylveon.png\nProcessed 556/1025: munkidori.png\nProcessed 557/1025: kyogre.png\nProcessed 558/1025: sharpedo.png\nProcessed 559/1025: cufant.png\nProcessed 560/1025: flabebe.png\nProcessed 561/1025: quilladin.png\nProcessed 562/1025: feebas.png\nProcessed 563/1025: metagross.png\nProcessed 564/1025: maractus.png\nProcessed 565/1025: muk.png\nProcessed 566/1025: empoleon.png\nProcessed 567/1025: chimchar.png\nProcessed 568/1025: sandaconda.png\nProcessed 569/1025: staryu.png\nProcessed 570/1025: scream-tail.png\nProcessed 571/1025: pancham.png\nProcessed 572/1025: dwebble.png\nProcessed 573/1025: bruxish.png\nProcessed 574/1025: joltik.png\nProcessed 575/1025: armaldo.png\nProcessed 576/1025: grimmsnarl.png\nProcessed 577/1025: phione.png\nProcessed 578/1025: frosmoth.png\nProcessed 579/1025: rhyperior.png\nProcessed 580/1025: pansear.png\nProcessed 581/1025: alcremie.png\nProcessed 582/1025: happiny.png\nProcessed 583/1025: spearow.png\nProcessed 584/1025: helioptile.png\nProcessed 585/1025: kingambit.png\nProcessed 586/1025: fearow.png\nProcessed 587/1025: wooloo.png\nProcessed 588/1025: prinplup.png\nProcessed 589/1025: diancie.png\nProcessed 590/1025: rabsca.png\nProcessed 591/1025: poipole.png\nProcessed 592/1025: ogerpon.png\nProcessed 593/1025: klink.png\nProcessed 594/1025: farfetchd.png\nProcessed 595/1025: gorebyss.png\nProcessed 596/1025: iron-bundle.png\nProcessed 597/1025: noivern.png\nProcessed 598/1025: herdier.png\nProcessed 599/1025: tynamo.png\nProcessed 600/1025: whiscash.png\nProcessed 601/1025: nidoran-m.png\nProcessed 602/1025: diglett.png\nProcessed 603/1025: wyrdeer.png\nProcessed 604/1025: krokorok.png\nProcessed 605/1025: scizor.png\nProcessed 606/1025: nihilego.png\nProcessed 607/1025: skarmory.png\nProcessed 608/1025: remoraid.png\nProcessed 609/1025: shaymin.png\nProcessed 610/1025: hitmontop.png\nProcessed 611/1025: snom.png\nProcessed 612/1025: cosmoem.png\nProcessed 613/1025: miraidon.png\nProcessed 614/1025: slowbro.png\nProcessed 615/1025: numel.png\nProcessed 616/1025: glalie.png\nProcessed 617/1025: vaporeon.png\nProcessed 618/1025: porygon-z.png\nProcessed 619/1025: lunatone.png\nProcessed 620/1025: machoke.png\nProcessed 621/1025: mareanie.png\nProcessed 622/1025: bastiodon.png\nProcessed 623/1025: mandibuzz.png\nProcessed 624/1025: arceus.png\nProcessed 625/1025: throh.png\nProcessed 626/1025: abomasnow.png\nProcessed 627/1025: bronzong.png\nProcessed 628/1025: flittle.png\nProcessed 629/1025: greninja.png\nProcessed 630/1025: hoppip.png\nProcessed 631/1025: kecleon.png\nProcessed 632/1025: crustle.png\nProcessed 633/1025: shuppet.png\nProcessed 634/1025: stonjourner.png\nProcessed 635/1025: gossifleur.png\nProcessed 636/1025: ducklett.png\nProcessed 637/1025: plusle.png\nProcessed 638/1025: poliwag.png\nProcessed 639/1025: treecko.png\nProcessed 640/1025: torchic.png\nProcessed 641/1025: kabutops.png\nProcessed 642/1025: dunsparce.png\nProcessed 643/1025: flygon.png\nProcessed 644/1025: morpeko.png\nProcessed 645/1025: spectrier.png\nProcessed 646/1025: orbeetle.png\nProcessed 647/1025: dugtrio.png\nProcessed 648/1025: marill.png\nProcessed 649/1025: magearna.png\nProcessed 650/1025: toxtricity.png\nProcessed 651/1025: fezandipiti.png\nProcessed 652/1025: silvally.png\nProcessed 653/1025: cacnea.png\nProcessed 654/1025: slugma.png\nProcessed 655/1025: oddish.png\nProcessed 656/1025: jirachi.png\nProcessed 657/1025: veluza.png\nProcessed 658/1025: ribombee.png\nProcessed 659/1025: glimmora.png\nProcessed 660/1025: litwick.png\nProcessed 661/1025: pidgeot.png\nProcessed 662/1025: sandslash.png\nProcessed 663/1025: drilbur.png\nProcessed 664/1025: cranidos.png\nProcessed 665/1025: chansey.png\nProcessed 666/1025: blacephalon.png\nProcessed 667/1025: camerupt.png\nProcessed 668/1025: arctozolt.png\nProcessed 669/1025: dudunsparce.png\nProcessed 670/1025: pichu.png\nProcessed 671/1025: duskull.png\nProcessed 672/1025: palafin.png\nProcessed 673/1025: kangaskhan.png\nProcessed 674/1025: chingling.png\nProcessed 675/1025: whirlipede.png\nProcessed 676/1025: silicobra.png\nProcessed 677/1025: ferroseed.png\nProcessed 678/1025: toucannon.png\nProcessed 679/1025: barbaracle.png\nProcessed 680/1025: salamence.png\nProcessed 681/1025: zekrom.png\nProcessed 682/1025: haunter.png\nProcessed 683/1025: glaceon.png\nProcessed 684/1025: golduck.png\nProcessed 685/1025: milotic.png\nProcessed 686/1025: politoed.png\nProcessed 687/1025: indeedee.png\nProcessed 688/1025: absol.png\nProcessed 689/1025: mienshao.png\nProcessed 690/1025: sprigatito.png\nProcessed 691/1025: aron.png\nProcessed 692/1025: tepig.png\nProcessed 693/1025: klang.png\nProcessed 694/1025: zapdos.png\nProcessed 695/1025: clauncher.png\nProcessed 696/1025: riolu.png\nProcessed 697/1025: skwovet.png\nProcessed 698/1025: raichu.png\nProcessed 699/1025: varoom.png\nProcessed 700/1025: hitmonchan.png\nProcessed 701/1025: slakoth.png\nProcessed 702/1025: ponyta.png\nProcessed 703/1025: togepi.png\nProcessed 704/1025: snover.png\nProcessed 705/1025: dubwool.png\nProcessed 706/1025: solosis.png\nProcessed 707/1025: fraxure.png\nProcessed 708/1025: caterpie.png\nProcessed 709/1025: iron-hands.png\nProcessed 710/1025: gimmighoul.png\nProcessed 711/1025: psyduck.png\nProcessed 712/1025: teddiursa.png\nProcessed 713/1025: tapu-lele.png\nProcessed 714/1025: piplup.png\nProcessed 715/1025: flamigo.png\nProcessed 716/1025: okidogi.png\nProcessed 717/1025: nickit.png\nProcessed 718/1025: darumaka.png\nProcessed 719/1025: finizen.png\nProcessed 720/1025: weavile.png\nProcessed 721/1025: mawile.png\nProcessed 722/1025: sewaddle.png\nProcessed 723/1025: unfezant.png\nProcessed 724/1025: buzzwole.png\nProcessed 725/1025: rockruff.png\nProcessed 726/1025: manaphy.png\nProcessed 727/1025: bunnelby.png\nProcessed 728/1025: seismitoad.png\nProcessed 729/1025: simipour.png\nProcessed 730/1025: vanilluxe.png\nProcessed 731/1025: cubchoo.png\nProcessed 732/1025: mantine.png\nProcessed 733/1025: maushold.png\nProcessed 734/1025: basculin.png\nProcessed 735/1025: dartrix.png\nProcessed 736/1025: weezing.png\nProcessed 737/1025: corsola.png\nProcessed 738/1025: roserade.png\nProcessed 739/1025: carbink.png\nProcessed 740/1025: heliolisk.png\nProcessed 741/1025: omanyte.png\nProcessed 742/1025: rufflet.png\nProcessed 743/1025: zygarde.png\nProcessed 744/1025: mothim.png\nProcessed 745/1025: klinklang.png\nProcessed 746/1025: sliggoo.png\nProcessed 747/1025: raticate.png\nProcessed 748/1025: grookey.png\nProcessed 749/1025: carkol.png\nProcessed 750/1025: dhelmise.png\nProcessed 751/1025: vibrava.png\nProcessed 752/1025: wiglett.png\nProcessed 753/1025: shiftry.png\nProcessed 754/1025: swampert.png\nProcessed 755/1025: excadrill.png\nProcessed 756/1025: brambleghast.png\nProcessed 757/1025: morelull.png\nProcessed 758/1025: fuecoco.png\nProcessed 759/1025: chatot.png\nProcessed 760/1025: klawf.png\nProcessed 761/1025: jigglypuff.png\nProcessed 762/1025: enamorus.png\nProcessed 763/1025: loudred.png\nProcessed 764/1025: forretress.png\nProcessed 765/1025: togetic.png\nProcessed 766/1025: feraligatr.png\nProcessed 767/1025: bisharp.png\nProcessed 768/1025: lycanroc.png\nProcessed 769/1025: iron-treads.png\nProcessed 770/1025: mabosstiff.png\nProcessed 771/1025: vikavolt.png\nProcessed 772/1025: victreebel.png\nProcessed 773/1025: decidueye.png\nProcessed 774/1025: cinderace.png\nProcessed 775/1025: spinarak.png\nProcessed 776/1025: onix.png\nProcessed 777/1025: wugtrio.png\nProcessed 778/1025: aurorus.png\nProcessed 779/1025: venonat.png\nProcessed 780/1025: thundurus.png\nProcessed 781/1025: wimpod.png\nProcessed 782/1025: nidorina.png\nProcessed 783/1025: tranquill.png\nProcessed 784/1025: hydrapple.png\nProcessed 785/1025: farigiraf.png\nProcessed 786/1025: buizel.png\nProcessed 787/1025: mightyena.png\nProcessed 788/1025: vulpix.png\nProcessed 789/1025: clawitzer.png\nProcessed 790/1025: revavroom.png\nProcessed 791/1025: lilligant.png\nProcessed 792/1025: togedemaru.png\nProcessed 793/1025: minccino.png\nProcessed 794/1025: roaring-moon.png\nProcessed 795/1025: charjabug.png\nProcessed 796/1025: doduo.png\nProcessed 797/1025: ferrothorn.png\nProcessed 798/1025: misdreavus.png\nProcessed 799/1025: kingdra.png\nProcessed 800/1025: shuckle.png\nProcessed 801/1025: rayquaza.png\nProcessed 802/1025: palossand.png\nProcessed 803/1025: spidops.png\nProcessed 804/1025: mudbray.png\nProcessed 805/1025: kricketune.png\nProcessed 806/1025: skrelp.png\nProcessed 807/1025: luxray.png\nProcessed 808/1025: ledyba.png\nProcessed 809/1025: baltoy.png\nProcessed 810/1025: basculegion.png\nProcessed 811/1025: trumbeak.png\nProcessed 812/1025: dewpider.png\nProcessed 813/1025: applin.png\nProcessed 814/1025: sandygast.png\nProcessed 815/1025: yanma.png\nProcessed 816/1025: kubfu.png\nProcessed 817/1025: lugia.png\nProcessed 818/1025: turtonator.png\nProcessed 819/1025: hawlucha.png\nProcessed 820/1025: leavanny.png\nProcessed 821/1025: vespiquen.png\nProcessed 822/1025: solgaleo.png\nProcessed 823/1025: squirtle.png\nProcessed 824/1025: slowking.png\nProcessed 825/1025: exeggcute.png\nProcessed 826/1025: emolga.png\nProcessed 827/1025: carvanha.png\nProcessed 828/1025: zacian.png\nProcessed 829/1025: grafaiai.png\nProcessed 830/1025: clefairy.png\nProcessed 831/1025: bayleef.png\nProcessed 832/1025: drifloon.png\nProcessed 833/1025: stufful.png\nProcessed 834/1025: sceptile.png\nProcessed 835/1025: azurill.png\nProcessed 836/1025: chespin.png\nProcessed 837/1025: jangmo-o.png\nProcessed 838/1025: panpour.png\nProcessed 839/1025: meloetta.png\nProcessed 840/1025: glimmet.png\nProcessed 841/1025: sinistcha.png\nProcessed 842/1025: staravia.png\nProcessed 843/1025: linoone.png\nProcessed 844/1025: tinkaton.png\nProcessed 845/1025: iron-crown.png\nProcessed 846/1025: spiritomb.png\nProcessed 847/1025: zoroark.png\nProcessed 848/1025: chikorita.png\nProcessed 849/1025: vanillite.png\nProcessed 850/1025: scatterbug.png\nProcessed 851/1025: spinda.png\nProcessed 852/1025: beheeyem.png\nProcessed 853/1025: grimer.png\nProcessed 854/1025: pawmot.png\nProcessed 855/1025: leafeon.png\nProcessed 856/1025: cyclizar.png\nProcessed 857/1025: deerling.png\nProcessed 858/1025: celebi.png\nProcessed 859/1025: breloom.png\nProcessed 860/1025: turtwig.png\nProcessed 861/1025: zweilous.png\nProcessed 862/1025: burmy.png\nProcessed 863/1025: dolliv.png\nProcessed 864/1025: copperajah.png\nProcessed 865/1025: swinub.png\nProcessed 866/1025: axew.png\nProcessed 867/1025: dragapult.png\nProcessed 868/1025: pangoro.png\nProcessed 869/1025: darkrai.png\nProcessed 870/1025: krabby.png\nProcessed 871/1025: dewgong.png\nProcessed 872/1025: eldegoss.png\nProcessed 873/1025: sirfetchd.png\nProcessed 874/1025: volcarona.png\nProcessed 875/1025: vigoroth.png\nProcessed 876/1025: bombirdier.png\nProcessed 877/1025: falinks.png\nProcessed 878/1025: ninetales.png\nProcessed 879/1025: pawniard.png\nProcessed 880/1025: rolycoly.png\nProcessed 881/1025: blitzle.png\nProcessed 882/1025: porygon2.png\nProcessed 883/1025: croagunk.png\nProcessed 884/1025: xatu.png\nProcessed 885/1025: lairon.png\nProcessed 886/1025: tentacruel.png\nProcessed 887/1025: toedscruel.png\nProcessed 888/1025: krookodile.png\nProcessed 889/1025: gothitelle.png\nProcessed 890/1025: regice.png\nProcessed 891/1025: goodra.png\nProcessed 892/1025: thwackey.png\nProcessed 893/1025: meowth.png\nProcessed 894/1025: grovyle.png\nProcessed 895/1025: volbeat.png\nProcessed 896/1025: unown.png\nProcessed 897/1025: terrakion.png\nProcessed 898/1025: togekiss.png\nProcessed 899/1025: kricketot.png\nProcessed 900/1025: zeraora.png\nProcessed 901/1025: bellsprout.png\nProcessed 902/1025: luvdisc.png\nProcessed 903/1025: garbodor.png\nProcessed 904/1025: horsea.png\nProcessed 905/1025: nincada.png\nProcessed 906/1025: dragalge.png\nProcessed 907/1025: electrode.png\nProcessed 908/1025: salazzle.png\nProcessed 909/1025: fennekin.png\nProcessed 910/1025: metang.png\nProcessed 911/1025: donphan.png\nProcessed 912/1025: jynx.png\nProcessed 913/1025: yamask.png\nProcessed 914/1025: druddigon.png\nProcessed 915/1025: kabuto.png\nProcessed 916/1025: magmar.png\nProcessed 917/1025: swoobat.png\nProcessed 918/1025: braviary.png\nProcessed 919/1025: iron-thorns.png\nProcessed 920/1025: charmander.png\nProcessed 921/1025: baxcalibur.png\nProcessed 922/1025: rowlet.png\nProcessed 923/1025: lombre.png\nProcessed 924/1025: durant.png\nProcessed 925/1025: golem.png\nProcessed 926/1025: sunkern.png\nProcessed 927/1025: chewtle.png\nProcessed 928/1025: dewott.png\nProcessed 929/1025: croconaw.png\nProcessed 930/1025: clobbopus.png\nProcessed 931/1025: cobalion.png\nProcessed 932/1025: pansage.png\nProcessed 933/1025: deino.png\nProcessed 934/1025: spewpa.png\nProcessed 935/1025: floette.png\nProcessed 936/1025: combee.png\nProcessed 937/1025: flapple.png\nProcessed 938/1025: iron-boulder.png\nProcessed 939/1025: cofagrigus.png\nProcessed 940/1025: drednaw.png\nProcessed 941/1025: furfrou.png\nProcessed 942/1025: gouging-fire.png\nProcessed 943/1025: furret.png\nProcessed 944/1025: clefable.png\nProcessed 945/1025: wailmer.png\nProcessed 946/1025: grumpig.png\nProcessed 947/1025: charmeleon.png\nProcessed 948/1025: phantump.png\nProcessed 949/1025: venomoth.png\nProcessed 950/1025: natu.png\nProcessed 951/1025: boltund.png\nProcessed 952/1025: simisear.png\nProcessed 953/1025: blissey.png\nProcessed 954/1025: spritzee.png\nProcessed 955/1025: virizion.png\nProcessed 956/1025: armarouge.png\nProcessed 957/1025: scolipede.png\nProcessed 958/1025: rillaboom.png\nProcessed 959/1025: lokix.png\nProcessed 960/1025: fidough.png\nProcessed 961/1025: mamoswine.png\nProcessed 962/1025: minior.png\nProcessed 963/1025: mr-rime.png\nProcessed 964/1025: kirlia.png\nProcessed 965/1025: capsakid.png\nProcessed 966/1025: omastar.png\nProcessed 967/1025: drizzile.png\nProcessed 968/1025: bramblin.png\nProcessed 969/1025: hoopa.png\nProcessed 970/1025: steelix.png\nProcessed 971/1025: rotom.png\nProcessed 972/1025: hoothoot.png\nProcessed 973/1025: karrablast.png\nProcessed 974/1025: torkoal.png\nProcessed 975/1025: cramorant.png\nProcessed 976/1025: victini.png\nProcessed 977/1025: cloyster.png\nProcessed 978/1025: espathra.png\nProcessed 979/1025: relicanth.png\nProcessed 980/1025: wynaut.png\nProcessed 981/1025: raboot.png\nProcessed 982/1025: zorua.png\nProcessed 983/1025: blipbug.png\nProcessed 984/1025: timburr.png\nProcessed 985/1025: drifblim.png\nProcessed 986/1025: altaria.png\nProcessed 987/1025: cradily.png\nProcessed 988/1025: mudkip.png\nProcessed 989/1025: toxel.png\nProcessed 990/1025: lunala.png\nProcessed 991/1025: mudsdale.png\nProcessed 992/1025: shellder.png\nProcessed 993/1025: jumpluff.png\nProcessed 994/1025: bounsweet.png\nProcessed 995/1025: trubbish.png\nProcessed 996/1025: beautifly.png\nProcessed 997/1025: bagon.png\nProcessed 998/1025: oinkologne.png\nProcessed 999/1025: celesteela.png\nProcessed 1000/1025: centiskorch.png\nProcessed 1001/1025: marshtomp.png\nProcessed 1002/1025: uxie.png\nProcessed 1003/1025: suicune.png\nProcessed 1004/1025: pumpkaboo.png\nProcessed 1005/1025: sudowoodo.png\nProcessed 1006/1025: bergmite.png\nProcessed 1007/1025: lanturn.png\nProcessed 1008/1025: kingler.png\nProcessed 1009/1025: shroomish.png\nProcessed 1010/1025: gligar.png\nProcessed 1011/1025: roselia.png\nProcessed 1012/1025: bonsly.png\nProcessed 1013/1025: tatsugiri.png\nProcessed 1014/1025: type-null.png\nProcessed 1015/1025: dachsbun.png\nProcessed 1016/1025: combusken.png\nProcessed 1017/1025: arctibax.png\nProcessed 1018/1025: slowpoke.png\nProcessed 1019/1025: nacli.png\nProcessed 1020/1025: floatzel.png\nProcessed 1021/1025: latios.png\nProcessed 1022/1025: mew.png\nProcessed 1023/1025: ekans.png\nProcessed 1024/1025: cetitan.png\nProcessed 1025/1025: hakamo-o.png\nProcessing complete! Successfully processed 1025/1025 sprites.\n\n\nWe only care about efficient representations of this data for this tutorial. .npy file and .txt file of names it is!\n\n\n\ndatafy_pokemon_sprites\n\n datafy_pokemon_sprites (ds_name:str)\n\nConvert Pokemon sprites to numpy arrays and save filenames\n\ndownload_pokemon_sprites()\nprocess_pokemon_sprites(\"processed_adaptive\", binarize_method='adaptive') # Best\npoke_pixels_path, poke_names_path = datafy_pokemon_sprites(\"processed_adaptive\")\n\nLet’s load the data and take a look at it.\n\n\n\nload_bipolar_pokemon_sprites\n\n load_bipolar_pokemon_sprites (ds_name:str='processed_adaptive',\n                               binarize_method:str='adaptive')\n\nLoad Pokemon sprites from numpy arrays\nThis is what the pokemon data looks like after all the processing.\n\n\nFetching sprite page from: https://pokemondb.net/sprites\nFound 1025 Pokemon sprites to download\nUsing 0.1s delay between downloads\nSkipping 1/1025: bulbasaur.png (already exists)\nSkipping 2/1025: ivysaur.png (already exists)\nSkipping 3/1025: venusaur.png (already exists)\nSkipping 4/1025: charmander.png (already exists)\nSkipping 5/1025: charmeleon.png (already exists)\nSkipping 6/1025: charizard.png (already exists)\nSkipping 7/1025: squirtle.png (already exists)\nSkipping 8/1025: wartortle.png (already exists)\nSkipping 9/1025: blastoise.png (already exists)\nSkipping 10/1025: caterpie.png (already exists)\nSkipping 11/1025: metapod.png (already exists)\nSkipping 12/1025: butterfree.png (already exists)\nSkipping 13/1025: weedle.png (already exists)\nSkipping 14/1025: kakuna.png (already exists)\nSkipping 15/1025: beedrill.png (already exists)\nSkipping 16/1025: pidgey.png (already exists)\nSkipping 17/1025: pidgeotto.png (already exists)\nSkipping 18/1025: pidgeot.png (already exists)\nSkipping 19/1025: rattata.png (already exists)\nSkipping 20/1025: raticate.png (already exists)\nSkipping 21/1025: spearow.png (already exists)\nSkipping 22/1025: fearow.png (already exists)\nSkipping 23/1025: ekans.png (already exists)\nSkipping 24/1025: arbok.png (already exists)\nSkipping 25/1025: pikachu.png (already exists)\nSkipping 26/1025: raichu.png (already exists)\nSkipping 27/1025: sandshrew.png (already exists)\nSkipping 28/1025: sandslash.png (already exists)\nSkipping 29/1025: nidoran-f.png (already exists)\nSkipping 30/1025: nidorina.png (already exists)\nSkipping 31/1025: nidoqueen.png (already exists)\nSkipping 32/1025: nidoran-m.png (already exists)\nSkipping 33/1025: nidorino.png (already exists)\nSkipping 34/1025: nidoking.png (already exists)\nSkipping 35/1025: clefairy.png (already exists)\nSkipping 36/1025: clefable.png (already exists)\nSkipping 37/1025: vulpix.png (already exists)\nSkipping 38/1025: ninetales.png (already exists)\nSkipping 39/1025: jigglypuff.png (already exists)\nSkipping 40/1025: wigglytuff.png (already exists)\nSkipping 41/1025: zubat.png (already exists)\nSkipping 42/1025: golbat.png (already exists)\nSkipping 43/1025: oddish.png (already exists)\nSkipping 44/1025: gloom.png (already exists)\nSkipping 45/1025: vileplume.png (already exists)\nSkipping 46/1025: paras.png (already exists)\nSkipping 47/1025: parasect.png (already exists)\nSkipping 48/1025: venonat.png (already exists)\nSkipping 49/1025: venomoth.png (already exists)\nSkipping 50/1025: diglett.png (already exists)\nSkipping 51/1025: dugtrio.png (already exists)\nSkipping 52/1025: meowth.png (already exists)\nSkipping 53/1025: persian.png (already exists)\nSkipping 54/1025: psyduck.png (already exists)\nSkipping 55/1025: golduck.png (already exists)\nSkipping 56/1025: mankey.png (already exists)\nSkipping 57/1025: primeape.png (already exists)\nSkipping 58/1025: growlithe.png (already exists)\nSkipping 59/1025: arcanine.png (already exists)\nSkipping 60/1025: poliwag.png (already exists)\nSkipping 61/1025: poliwhirl.png (already exists)\nSkipping 62/1025: poliwrath.png (already exists)\nSkipping 63/1025: abra.png (already exists)\nSkipping 64/1025: kadabra.png (already exists)\nSkipping 65/1025: alakazam.png (already exists)\nSkipping 66/1025: machop.png (already exists)\nSkipping 67/1025: machoke.png (already exists)\nSkipping 68/1025: machamp.png (already exists)\nSkipping 69/1025: bellsprout.png (already exists)\nSkipping 70/1025: weepinbell.png (already exists)\nSkipping 71/1025: victreebel.png (already exists)\nSkipping 72/1025: tentacool.png (already exists)\nSkipping 73/1025: tentacruel.png (already exists)\nSkipping 74/1025: geodude.png (already exists)\nSkipping 75/1025: graveler.png (already exists)\nSkipping 76/1025: golem.png (already exists)\nSkipping 77/1025: ponyta.png (already exists)\nSkipping 78/1025: rapidash.png (already exists)\nSkipping 79/1025: slowpoke.png (already exists)\nSkipping 80/1025: slowbro.png (already exists)\nSkipping 81/1025: magnemite.png (already exists)\nSkipping 82/1025: magneton.png (already exists)\nSkipping 83/1025: farfetchd.png (already exists)\nSkipping 84/1025: doduo.png (already exists)\nSkipping 85/1025: dodrio.png (already exists)\nSkipping 86/1025: seel.png (already exists)\nSkipping 87/1025: dewgong.png (already exists)\nSkipping 88/1025: grimer.png (already exists)\nSkipping 89/1025: muk.png (already exists)\nSkipping 90/1025: shellder.png (already exists)\nSkipping 91/1025: cloyster.png (already exists)\nSkipping 92/1025: gastly.png (already exists)\nSkipping 93/1025: haunter.png (already exists)\nSkipping 94/1025: gengar.png (already exists)\nSkipping 95/1025: onix.png (already exists)\nSkipping 96/1025: drowzee.png (already exists)\nSkipping 97/1025: hypno.png (already exists)\nSkipping 98/1025: krabby.png (already exists)\nSkipping 99/1025: kingler.png (already exists)\nSkipping 100/1025: voltorb.png (already exists)\nSkipping 101/1025: electrode.png (already exists)\nSkipping 102/1025: exeggcute.png (already exists)\nSkipping 103/1025: exeggutor.png (already exists)\nSkipping 104/1025: cubone.png (already exists)\nSkipping 105/1025: marowak.png (already exists)\nSkipping 106/1025: hitmonlee.png (already exists)\nSkipping 107/1025: hitmonchan.png (already exists)\nSkipping 108/1025: lickitung.png (already exists)\nSkipping 109/1025: koffing.png (already exists)\nSkipping 110/1025: weezing.png (already exists)\nSkipping 111/1025: rhyhorn.png (already exists)\nSkipping 112/1025: rhydon.png (already exists)\nSkipping 113/1025: chansey.png (already exists)\nSkipping 114/1025: tangela.png (already exists)\nSkipping 115/1025: kangaskhan.png (already exists)\nSkipping 116/1025: horsea.png (already exists)\nSkipping 117/1025: seadra.png (already exists)\nSkipping 118/1025: goldeen.png (already exists)\nSkipping 119/1025: seaking.png (already exists)\nSkipping 120/1025: staryu.png (already exists)\nSkipping 121/1025: starmie.png (already exists)\nSkipping 122/1025: mr-mime.png (already exists)\nSkipping 123/1025: scyther.png (already exists)\nSkipping 124/1025: jynx.png (already exists)\nSkipping 125/1025: electabuzz.png (already exists)\nSkipping 126/1025: magmar.png (already exists)\nSkipping 127/1025: pinsir.png (already exists)\nSkipping 128/1025: tauros.png (already exists)\nSkipping 129/1025: magikarp.png (already exists)\nSkipping 130/1025: gyarados.png (already exists)\nSkipping 131/1025: lapras.png (already exists)\nSkipping 132/1025: ditto.png (already exists)\nSkipping 133/1025: eevee.png (already exists)\nSkipping 134/1025: vaporeon.png (already exists)\nSkipping 135/1025: jolteon.png (already exists)\nSkipping 136/1025: flareon.png (already exists)\nSkipping 137/1025: porygon.png (already exists)\nSkipping 138/1025: omanyte.png (already exists)\nSkipping 139/1025: omastar.png (already exists)\nSkipping 140/1025: kabuto.png (already exists)\nSkipping 141/1025: kabutops.png (already exists)\nSkipping 142/1025: aerodactyl.png (already exists)\nSkipping 143/1025: snorlax.png (already exists)\nSkipping 144/1025: articuno.png (already exists)\nSkipping 145/1025: zapdos.png (already exists)\nSkipping 146/1025: moltres.png (already exists)\nSkipping 147/1025: dratini.png (already exists)\nSkipping 148/1025: dragonair.png (already exists)\nSkipping 149/1025: dragonite.png (already exists)\nSkipping 150/1025: mewtwo.png (already exists)\nSkipping 151/1025: mew.png (already exists)\nSkipping 152/1025: chikorita.png (already exists)\nSkipping 153/1025: bayleef.png (already exists)\nSkipping 154/1025: meganium.png (already exists)\nSkipping 155/1025: cyndaquil.png (already exists)\nSkipping 156/1025: quilava.png (already exists)\nSkipping 157/1025: typhlosion.png (already exists)\nSkipping 158/1025: totodile.png (already exists)\nSkipping 159/1025: croconaw.png (already exists)\nSkipping 160/1025: feraligatr.png (already exists)\nSkipping 161/1025: sentret.png (already exists)\nSkipping 162/1025: furret.png (already exists)\nSkipping 163/1025: hoothoot.png (already exists)\nSkipping 164/1025: noctowl.png (already exists)\nSkipping 165/1025: ledyba.png (already exists)\nSkipping 166/1025: ledian.png (already exists)\nSkipping 167/1025: spinarak.png (already exists)\nSkipping 168/1025: ariados.png (already exists)\nSkipping 169/1025: crobat.png (already exists)\nSkipping 170/1025: chinchou.png (already exists)\nSkipping 171/1025: lanturn.png (already exists)\nSkipping 172/1025: pichu.png (already exists)\nSkipping 173/1025: cleffa.png (already exists)\nSkipping 174/1025: igglybuff.png (already exists)\nSkipping 175/1025: togepi.png (already exists)\nSkipping 176/1025: togetic.png (already exists)\nSkipping 177/1025: natu.png (already exists)\nSkipping 178/1025: xatu.png (already exists)\nSkipping 179/1025: mareep.png (already exists)\nSkipping 180/1025: flaaffy.png (already exists)\nSkipping 181/1025: ampharos.png (already exists)\nSkipping 182/1025: bellossom.png (already exists)\nSkipping 183/1025: marill.png (already exists)\nSkipping 184/1025: azumarill.png (already exists)\nSkipping 185/1025: sudowoodo.png (already exists)\nSkipping 186/1025: politoed.png (already exists)\nSkipping 187/1025: hoppip.png (already exists)\nSkipping 188/1025: skiploom.png (already exists)\nSkipping 189/1025: jumpluff.png (already exists)\nSkipping 190/1025: aipom.png (already exists)\nSkipping 191/1025: sunkern.png (already exists)\nSkipping 192/1025: sunflora.png (already exists)\nSkipping 193/1025: yanma.png (already exists)\nSkipping 194/1025: wooper.png (already exists)\nSkipping 195/1025: quagsire.png (already exists)\nSkipping 196/1025: espeon.png (already exists)\nSkipping 197/1025: umbreon.png (already exists)\nSkipping 198/1025: murkrow.png (already exists)\nSkipping 199/1025: slowking.png (already exists)\nSkipping 200/1025: misdreavus.png (already exists)\nSkipping 201/1025: unown.png (already exists)\nSkipping 202/1025: wobbuffet.png (already exists)\nSkipping 203/1025: girafarig.png (already exists)\nSkipping 204/1025: pineco.png (already exists)\nSkipping 205/1025: forretress.png (already exists)\nSkipping 206/1025: dunsparce.png (already exists)\nSkipping 207/1025: gligar.png (already exists)\nSkipping 208/1025: steelix.png (already exists)\nSkipping 209/1025: snubbull.png (already exists)\nSkipping 210/1025: granbull.png (already exists)\nSkipping 211/1025: qwilfish.png (already exists)\nSkipping 212/1025: scizor.png (already exists)\nSkipping 213/1025: shuckle.png (already exists)\nSkipping 214/1025: heracross.png (already exists)\nSkipping 215/1025: sneasel.png (already exists)\nSkipping 216/1025: teddiursa.png (already exists)\nSkipping 217/1025: ursaring.png (already exists)\nSkipping 218/1025: slugma.png (already exists)\nSkipping 219/1025: magcargo.png (already exists)\nSkipping 220/1025: swinub.png (already exists)\nSkipping 221/1025: piloswine.png (already exists)\nSkipping 222/1025: corsola.png (already exists)\nSkipping 223/1025: remoraid.png (already exists)\nSkipping 224/1025: octillery.png (already exists)\nSkipping 225/1025: delibird.png (already exists)\nSkipping 226/1025: mantine.png (already exists)\nSkipping 227/1025: skarmory.png (already exists)\nSkipping 228/1025: houndour.png (already exists)\nSkipping 229/1025: houndoom.png (already exists)\nSkipping 230/1025: kingdra.png (already exists)\nSkipping 231/1025: phanpy.png (already exists)\nSkipping 232/1025: donphan.png (already exists)\nSkipping 233/1025: porygon2.png (already exists)\nSkipping 234/1025: stantler.png (already exists)\nSkipping 235/1025: smeargle.png (already exists)\nSkipping 236/1025: tyrogue.png (already exists)\nSkipping 237/1025: hitmontop.png (already exists)\nSkipping 238/1025: smoochum.png (already exists)\nSkipping 239/1025: elekid.png (already exists)\nSkipping 240/1025: magby.png (already exists)\nSkipping 241/1025: miltank.png (already exists)\nSkipping 242/1025: blissey.png (already exists)\nSkipping 243/1025: raikou.png (already exists)\nSkipping 244/1025: entei.png (already exists)\nSkipping 245/1025: suicune.png (already exists)\nSkipping 246/1025: larvitar.png (already exists)\nSkipping 247/1025: pupitar.png (already exists)\nSkipping 248/1025: tyranitar.png (already exists)\nSkipping 249/1025: lugia.png (already exists)\nSkipping 250/1025: ho-oh.png (already exists)\nSkipping 251/1025: celebi.png (already exists)\nSkipping 252/1025: treecko.png (already exists)\nSkipping 253/1025: grovyle.png (already exists)\nSkipping 254/1025: sceptile.png (already exists)\nSkipping 255/1025: torchic.png (already exists)\nSkipping 256/1025: combusken.png (already exists)\nSkipping 257/1025: blaziken.png (already exists)\nSkipping 258/1025: mudkip.png (already exists)\nSkipping 259/1025: marshtomp.png (already exists)\nSkipping 260/1025: swampert.png (already exists)\nSkipping 261/1025: poochyena.png (already exists)\nSkipping 262/1025: mightyena.png (already exists)\nSkipping 263/1025: zigzagoon.png (already exists)\nSkipping 264/1025: linoone.png (already exists)\nSkipping 265/1025: wurmple.png (already exists)\nSkipping 266/1025: silcoon.png (already exists)\nSkipping 267/1025: beautifly.png (already exists)\nSkipping 268/1025: cascoon.png (already exists)\nSkipping 269/1025: dustox.png (already exists)\nSkipping 270/1025: lotad.png (already exists)\nSkipping 271/1025: lombre.png (already exists)\nSkipping 272/1025: ludicolo.png (already exists)\nSkipping 273/1025: seedot.png (already exists)\nSkipping 274/1025: nuzleaf.png (already exists)\nSkipping 275/1025: shiftry.png (already exists)\nSkipping 276/1025: taillow.png (already exists)\nSkipping 277/1025: swellow.png (already exists)\nSkipping 278/1025: wingull.png (already exists)\nSkipping 279/1025: pelipper.png (already exists)\nSkipping 280/1025: ralts.png (already exists)\nSkipping 281/1025: kirlia.png (already exists)\nSkipping 282/1025: gardevoir.png (already exists)\nSkipping 283/1025: surskit.png (already exists)\nSkipping 284/1025: masquerain.png (already exists)\nSkipping 285/1025: shroomish.png (already exists)\nSkipping 286/1025: breloom.png (already exists)\nSkipping 287/1025: slakoth.png (already exists)\nSkipping 288/1025: vigoroth.png (already exists)\nSkipping 289/1025: slaking.png (already exists)\nSkipping 290/1025: nincada.png (already exists)\nSkipping 291/1025: ninjask.png (already exists)\nSkipping 292/1025: shedinja.png (already exists)\nSkipping 293/1025: whismur.png (already exists)\nSkipping 294/1025: loudred.png (already exists)\nSkipping 295/1025: exploud.png (already exists)\nSkipping 296/1025: makuhita.png (already exists)\nSkipping 297/1025: hariyama.png (already exists)\nSkipping 298/1025: azurill.png (already exists)\nSkipping 299/1025: nosepass.png (already exists)\nSkipping 300/1025: skitty.png (already exists)\nSkipping 301/1025: delcatty.png (already exists)\nSkipping 302/1025: sableye.png (already exists)\nSkipping 303/1025: mawile.png (already exists)\nSkipping 304/1025: aron.png (already exists)\nSkipping 305/1025: lairon.png (already exists)\nSkipping 306/1025: aggron.png (already exists)\nSkipping 307/1025: meditite.png (already exists)\nSkipping 308/1025: medicham.png (already exists)\nSkipping 309/1025: electrike.png (already exists)\nSkipping 310/1025: manectric.png (already exists)\nSkipping 311/1025: plusle.png (already exists)\nSkipping 312/1025: minun.png (already exists)\nSkipping 313/1025: volbeat.png (already exists)\nSkipping 314/1025: illumise.png (already exists)\nSkipping 315/1025: roselia.png (already exists)\nSkipping 316/1025: gulpin.png (already exists)\nSkipping 317/1025: swalot.png (already exists)\nSkipping 318/1025: carvanha.png (already exists)\nSkipping 319/1025: sharpedo.png (already exists)\nSkipping 320/1025: wailmer.png (already exists)\nSkipping 321/1025: wailord.png (already exists)\nSkipping 322/1025: numel.png (already exists)\nSkipping 323/1025: camerupt.png (already exists)\nSkipping 324/1025: torkoal.png (already exists)\nSkipping 325/1025: spoink.png (already exists)\nSkipping 326/1025: grumpig.png (already exists)\nSkipping 327/1025: spinda.png (already exists)\nSkipping 328/1025: trapinch.png (already exists)\nSkipping 329/1025: vibrava.png (already exists)\nSkipping 330/1025: flygon.png (already exists)\nSkipping 331/1025: cacnea.png (already exists)\nSkipping 332/1025: cacturne.png (already exists)\nSkipping 333/1025: swablu.png (already exists)\nSkipping 334/1025: altaria.png (already exists)\nSkipping 335/1025: zangoose.png (already exists)\nSkipping 336/1025: seviper.png (already exists)\nSkipping 337/1025: lunatone.png (already exists)\nSkipping 338/1025: solrock.png (already exists)\nSkipping 339/1025: barboach.png (already exists)\nSkipping 340/1025: whiscash.png (already exists)\nSkipping 341/1025: corphish.png (already exists)\nSkipping 342/1025: crawdaunt.png (already exists)\nSkipping 343/1025: baltoy.png (already exists)\nSkipping 344/1025: claydol.png (already exists)\nSkipping 345/1025: lileep.png (already exists)\nSkipping 346/1025: cradily.png (already exists)\nSkipping 347/1025: anorith.png (already exists)\nSkipping 348/1025: armaldo.png (already exists)\nSkipping 349/1025: feebas.png (already exists)\nSkipping 350/1025: milotic.png (already exists)\nSkipping 351/1025: castform.png (already exists)\nSkipping 352/1025: kecleon.png (already exists)\nSkipping 353/1025: shuppet.png (already exists)\nSkipping 354/1025: banette.png (already exists)\nSkipping 355/1025: duskull.png (already exists)\nSkipping 356/1025: dusclops.png (already exists)\nSkipping 357/1025: tropius.png (already exists)\nSkipping 358/1025: chimecho.png (already exists)\nSkipping 359/1025: absol.png (already exists)\nSkipping 360/1025: wynaut.png (already exists)\nSkipping 361/1025: snorunt.png (already exists)\nSkipping 362/1025: glalie.png (already exists)\nSkipping 363/1025: spheal.png (already exists)\nSkipping 364/1025: sealeo.png (already exists)\nSkipping 365/1025: walrein.png (already exists)\nSkipping 366/1025: clamperl.png (already exists)\nSkipping 367/1025: huntail.png (already exists)\nSkipping 368/1025: gorebyss.png (already exists)\nSkipping 369/1025: relicanth.png (already exists)\nSkipping 370/1025: luvdisc.png (already exists)\nSkipping 371/1025: bagon.png (already exists)\nSkipping 372/1025: shelgon.png (already exists)\nSkipping 373/1025: salamence.png (already exists)\nSkipping 374/1025: beldum.png (already exists)\nSkipping 375/1025: metang.png (already exists)\nSkipping 376/1025: metagross.png (already exists)\nSkipping 377/1025: regirock.png (already exists)\nSkipping 378/1025: regice.png (already exists)\nSkipping 379/1025: registeel.png (already exists)\nSkipping 380/1025: latias.png (already exists)\nSkipping 381/1025: latios.png (already exists)\nSkipping 382/1025: kyogre.png (already exists)\nSkipping 383/1025: groudon.png (already exists)\nSkipping 384/1025: rayquaza.png (already exists)\nSkipping 385/1025: jirachi.png (already exists)\nSkipping 386/1025: deoxys.png (already exists)\nSkipping 387/1025: turtwig.png (already exists)\nSkipping 388/1025: grotle.png (already exists)\nSkipping 389/1025: torterra.png (already exists)\nSkipping 390/1025: chimchar.png (already exists)\nSkipping 391/1025: monferno.png (already exists)\nSkipping 392/1025: infernape.png (already exists)\nSkipping 393/1025: piplup.png (already exists)\nSkipping 394/1025: prinplup.png (already exists)\nSkipping 395/1025: empoleon.png (already exists)\nSkipping 396/1025: starly.png (already exists)\nSkipping 397/1025: staravia.png (already exists)\nSkipping 398/1025: staraptor.png (already exists)\nSkipping 399/1025: bidoof.png (already exists)\nSkipping 400/1025: bibarel.png (already exists)\nSkipping 401/1025: kricketot.png (already exists)\nSkipping 402/1025: kricketune.png (already exists)\nSkipping 403/1025: shinx.png (already exists)\nSkipping 404/1025: luxio.png (already exists)\nSkipping 405/1025: luxray.png (already exists)\nSkipping 406/1025: budew.png (already exists)\nSkipping 407/1025: roserade.png (already exists)\nSkipping 408/1025: cranidos.png (already exists)\nSkipping 409/1025: rampardos.png (already exists)\nSkipping 410/1025: shieldon.png (already exists)\nSkipping 411/1025: bastiodon.png (already exists)\nSkipping 412/1025: burmy.png (already exists)\nSkipping 413/1025: wormadam.png (already exists)\nSkipping 414/1025: mothim.png (already exists)\nSkipping 415/1025: combee.png (already exists)\nSkipping 416/1025: vespiquen.png (already exists)\nSkipping 417/1025: pachirisu.png (already exists)\nSkipping 418/1025: buizel.png (already exists)\nSkipping 419/1025: floatzel.png (already exists)\nSkipping 420/1025: cherubi.png (already exists)\nSkipping 421/1025: cherrim.png (already exists)\nSkipping 422/1025: shellos.png (already exists)\nSkipping 423/1025: gastrodon.png (already exists)\nSkipping 424/1025: ambipom.png (already exists)\nSkipping 425/1025: drifloon.png (already exists)\nSkipping 426/1025: drifblim.png (already exists)\nSkipping 427/1025: buneary.png (already exists)\nSkipping 428/1025: lopunny.png (already exists)\nSkipping 429/1025: mismagius.png (already exists)\nSkipping 430/1025: honchkrow.png (already exists)\nSkipping 431/1025: glameow.png (already exists)\nSkipping 432/1025: purugly.png (already exists)\nSkipping 433/1025: chingling.png (already exists)\nSkipping 434/1025: stunky.png (already exists)\nSkipping 435/1025: skuntank.png (already exists)\nSkipping 436/1025: bronzor.png (already exists)\nSkipping 437/1025: bronzong.png (already exists)\nSkipping 438/1025: bonsly.png (already exists)\nSkipping 439/1025: mime-jr.png (already exists)\nSkipping 440/1025: happiny.png (already exists)\nSkipping 441/1025: chatot.png (already exists)\nSkipping 442/1025: spiritomb.png (already exists)\nSkipping 443/1025: gible.png (already exists)\nSkipping 444/1025: gabite.png (already exists)\nSkipping 445/1025: garchomp.png (already exists)\nSkipping 446/1025: munchlax.png (already exists)\nSkipping 447/1025: riolu.png (already exists)\nSkipping 448/1025: lucario.png (already exists)\nSkipping 449/1025: hippopotas.png (already exists)\nSkipping 450/1025: hippowdon.png (already exists)\nSkipping 451/1025: skorupi.png (already exists)\nSkipping 452/1025: drapion.png (already exists)\nSkipping 453/1025: croagunk.png (already exists)\nSkipping 454/1025: toxicroak.png (already exists)\nSkipping 455/1025: carnivine.png (already exists)\nSkipping 456/1025: finneon.png (already exists)\nSkipping 457/1025: lumineon.png (already exists)\nSkipping 458/1025: mantyke.png (already exists)\nSkipping 459/1025: snover.png (already exists)\nSkipping 460/1025: abomasnow.png (already exists)\nSkipping 461/1025: weavile.png (already exists)\nSkipping 462/1025: magnezone.png (already exists)\nSkipping 463/1025: lickilicky.png (already exists)\nSkipping 464/1025: rhyperior.png (already exists)\nSkipping 465/1025: tangrowth.png (already exists)\nSkipping 466/1025: electivire.png (already exists)\nSkipping 467/1025: magmortar.png (already exists)\nSkipping 468/1025: togekiss.png (already exists)\nSkipping 469/1025: yanmega.png (already exists)\nSkipping 470/1025: leafeon.png (already exists)\nSkipping 471/1025: glaceon.png (already exists)\nSkipping 472/1025: gliscor.png (already exists)\nSkipping 473/1025: mamoswine.png (already exists)\nSkipping 474/1025: porygon-z.png (already exists)\nSkipping 475/1025: gallade.png (already exists)\nSkipping 476/1025: probopass.png (already exists)\nSkipping 477/1025: dusknoir.png (already exists)\nSkipping 478/1025: froslass.png (already exists)\nSkipping 479/1025: rotom.png (already exists)\nSkipping 480/1025: uxie.png (already exists)\nSkipping 481/1025: mesprit.png (already exists)\nSkipping 482/1025: azelf.png (already exists)\nSkipping 483/1025: dialga.png (already exists)\nSkipping 484/1025: palkia.png (already exists)\nSkipping 485/1025: heatran.png (already exists)\nSkipping 486/1025: regigigas.png (already exists)\nSkipping 487/1025: giratina.png (already exists)\nSkipping 488/1025: cresselia.png (already exists)\nSkipping 489/1025: phione.png (already exists)\nSkipping 490/1025: manaphy.png (already exists)\nSkipping 491/1025: darkrai.png (already exists)\nSkipping 492/1025: shaymin.png (already exists)\nSkipping 493/1025: arceus.png (already exists)\nSkipping 494/1025: victini.png (already exists)\nSkipping 495/1025: snivy.png (already exists)\nSkipping 496/1025: servine.png (already exists)\nSkipping 497/1025: serperior.png (already exists)\nSkipping 498/1025: tepig.png (already exists)\nSkipping 499/1025: pignite.png (already exists)\nSkipping 500/1025: emboar.png (already exists)\nSkipping 501/1025: oshawott.png (already exists)\nSkipping 502/1025: dewott.png (already exists)\nSkipping 503/1025: samurott.png (already exists)\nSkipping 504/1025: patrat.png (already exists)\nSkipping 505/1025: watchog.png (already exists)\nSkipping 506/1025: lillipup.png (already exists)\nSkipping 507/1025: herdier.png (already exists)\nSkipping 508/1025: stoutland.png (already exists)\nSkipping 509/1025: purrloin.png (already exists)\nSkipping 510/1025: liepard.png (already exists)\nSkipping 511/1025: pansage.png (already exists)\nSkipping 512/1025: simisage.png (already exists)\nSkipping 513/1025: pansear.png (already exists)\nSkipping 514/1025: simisear.png (already exists)\nSkipping 515/1025: panpour.png (already exists)\nSkipping 516/1025: simipour.png (already exists)\nSkipping 517/1025: munna.png (already exists)\nSkipping 518/1025: musharna.png (already exists)\nSkipping 519/1025: pidove.png (already exists)\nSkipping 520/1025: tranquill.png (already exists)\nSkipping 521/1025: unfezant.png (already exists)\nSkipping 522/1025: blitzle.png (already exists)\nSkipping 523/1025: zebstrika.png (already exists)\nSkipping 524/1025: roggenrola.png (already exists)\nSkipping 525/1025: boldore.png (already exists)\nSkipping 526/1025: gigalith.png (already exists)\nSkipping 527/1025: woobat.png (already exists)\nSkipping 528/1025: swoobat.png (already exists)\nSkipping 529/1025: drilbur.png (already exists)\nSkipping 530/1025: excadrill.png (already exists)\nSkipping 531/1025: audino.png (already exists)\nSkipping 532/1025: timburr.png (already exists)\nSkipping 533/1025: gurdurr.png (already exists)\nSkipping 534/1025: conkeldurr.png (already exists)\nSkipping 535/1025: tympole.png (already exists)\nSkipping 536/1025: palpitoad.png (already exists)\nSkipping 537/1025: seismitoad.png (already exists)\nSkipping 538/1025: throh.png (already exists)\nSkipping 539/1025: sawk.png (already exists)\nSkipping 540/1025: sewaddle.png (already exists)\nSkipping 541/1025: swadloon.png (already exists)\nSkipping 542/1025: leavanny.png (already exists)\nSkipping 543/1025: venipede.png (already exists)\nSkipping 544/1025: whirlipede.png (already exists)\nSkipping 545/1025: scolipede.png (already exists)\nSkipping 546/1025: cottonee.png (already exists)\nSkipping 547/1025: whimsicott.png (already exists)\nSkipping 548/1025: petilil.png (already exists)\nSkipping 549/1025: lilligant.png (already exists)\nSkipping 550/1025: basculin.png (already exists)\nSkipping 551/1025: sandile.png (already exists)\nSkipping 552/1025: krokorok.png (already exists)\nSkipping 553/1025: krookodile.png (already exists)\nSkipping 554/1025: darumaka.png (already exists)\nSkipping 555/1025: darmanitan.png (already exists)\nSkipping 556/1025: maractus.png (already exists)\nSkipping 557/1025: dwebble.png (already exists)\nSkipping 558/1025: crustle.png (already exists)\nSkipping 559/1025: scraggy.png (already exists)\nSkipping 560/1025: scrafty.png (already exists)\nSkipping 561/1025: sigilyph.png (already exists)\nSkipping 562/1025: yamask.png (already exists)\nSkipping 563/1025: cofagrigus.png (already exists)\nSkipping 564/1025: tirtouga.png (already exists)\nSkipping 565/1025: carracosta.png (already exists)\nSkipping 566/1025: archen.png (already exists)\nSkipping 567/1025: archeops.png (already exists)\nSkipping 568/1025: trubbish.png (already exists)\nSkipping 569/1025: garbodor.png (already exists)\nSkipping 570/1025: zorua.png (already exists)\nSkipping 571/1025: zoroark.png (already exists)\nSkipping 572/1025: minccino.png (already exists)\nSkipping 573/1025: cinccino.png (already exists)\nSkipping 574/1025: gothita.png (already exists)\nSkipping 575/1025: gothorita.png (already exists)\nSkipping 576/1025: gothitelle.png (already exists)\nSkipping 577/1025: solosis.png (already exists)\nSkipping 578/1025: duosion.png (already exists)\nSkipping 579/1025: reuniclus.png (already exists)\nSkipping 580/1025: ducklett.png (already exists)\nSkipping 581/1025: swanna.png (already exists)\nSkipping 582/1025: vanillite.png (already exists)\nSkipping 583/1025: vanillish.png (already exists)\nSkipping 584/1025: vanilluxe.png (already exists)\nSkipping 585/1025: deerling.png (already exists)\nSkipping 586/1025: sawsbuck.png (already exists)\nSkipping 587/1025: emolga.png (already exists)\nSkipping 588/1025: karrablast.png (already exists)\nSkipping 589/1025: escavalier.png (already exists)\nSkipping 590/1025: foongus.png (already exists)\nSkipping 591/1025: amoonguss.png (already exists)\nSkipping 592/1025: frillish.png (already exists)\nSkipping 593/1025: jellicent.png (already exists)\nSkipping 594/1025: alomomola.png (already exists)\nSkipping 595/1025: joltik.png (already exists)\nSkipping 596/1025: galvantula.png (already exists)\nSkipping 597/1025: ferroseed.png (already exists)\nSkipping 598/1025: ferrothorn.png (already exists)\nSkipping 599/1025: klink.png (already exists)\nSkipping 600/1025: klang.png (already exists)\nSkipping 601/1025: klinklang.png (already exists)\nSkipping 602/1025: tynamo.png (already exists)\nSkipping 603/1025: eelektrik.png (already exists)\nSkipping 604/1025: eelektross.png (already exists)\nSkipping 605/1025: elgyem.png (already exists)\nSkipping 606/1025: beheeyem.png (already exists)\nSkipping 607/1025: litwick.png (already exists)\nSkipping 608/1025: lampent.png (already exists)\nSkipping 609/1025: chandelure.png (already exists)\nSkipping 610/1025: axew.png (already exists)\nSkipping 611/1025: fraxure.png (already exists)\nSkipping 612/1025: haxorus.png (already exists)\nSkipping 613/1025: cubchoo.png (already exists)\nSkipping 614/1025: beartic.png (already exists)\nSkipping 615/1025: cryogonal.png (already exists)\nSkipping 616/1025: shelmet.png (already exists)\nSkipping 617/1025: accelgor.png (already exists)\nSkipping 618/1025: stunfisk.png (already exists)\nSkipping 619/1025: mienfoo.png (already exists)\nSkipping 620/1025: mienshao.png (already exists)\nSkipping 621/1025: druddigon.png (already exists)\nSkipping 622/1025: golett.png (already exists)\nSkipping 623/1025: golurk.png (already exists)\nSkipping 624/1025: pawniard.png (already exists)\nSkipping 625/1025: bisharp.png (already exists)\nSkipping 626/1025: bouffalant.png (already exists)\nSkipping 627/1025: rufflet.png (already exists)\nSkipping 628/1025: braviary.png (already exists)\nSkipping 629/1025: vullaby.png (already exists)\nSkipping 630/1025: mandibuzz.png (already exists)\nSkipping 631/1025: heatmor.png (already exists)\nSkipping 632/1025: durant.png (already exists)\nSkipping 633/1025: deino.png (already exists)\nSkipping 634/1025: zweilous.png (already exists)\nSkipping 635/1025: hydreigon.png (already exists)\nSkipping 636/1025: larvesta.png (already exists)\nSkipping 637/1025: volcarona.png (already exists)\nSkipping 638/1025: cobalion.png (already exists)\nSkipping 639/1025: terrakion.png (already exists)\nSkipping 640/1025: virizion.png (already exists)\nSkipping 641/1025: tornadus.png (already exists)\nSkipping 642/1025: thundurus.png (already exists)\nSkipping 643/1025: reshiram.png (already exists)\nSkipping 644/1025: zekrom.png (already exists)\nSkipping 645/1025: landorus.png (already exists)\nSkipping 646/1025: kyurem.png (already exists)\nSkipping 647/1025: keldeo.png (already exists)\nSkipping 648/1025: meloetta.png (already exists)\nSkipping 649/1025: genesect.png (already exists)\nSkipping 650/1025: chespin.png (already exists)\nSkipping 651/1025: quilladin.png (already exists)\nSkipping 652/1025: chesnaught.png (already exists)\nSkipping 653/1025: fennekin.png (already exists)\nSkipping 654/1025: braixen.png (already exists)\nSkipping 655/1025: delphox.png (already exists)\nSkipping 656/1025: froakie.png (already exists)\nSkipping 657/1025: frogadier.png (already exists)\nSkipping 658/1025: greninja.png (already exists)\nSkipping 659/1025: bunnelby.png (already exists)\nSkipping 660/1025: diggersby.png (already exists)\nSkipping 661/1025: fletchling.png (already exists)\nSkipping 662/1025: fletchinder.png (already exists)\nSkipping 663/1025: talonflame.png (already exists)\nSkipping 664/1025: scatterbug.png (already exists)\nSkipping 665/1025: spewpa.png (already exists)\nSkipping 666/1025: vivillon.png (already exists)\nSkipping 667/1025: litleo.png (already exists)\nSkipping 668/1025: pyroar.png (already exists)\nSkipping 669/1025: flabebe.png (already exists)\nSkipping 670/1025: floette.png (already exists)\nSkipping 671/1025: florges.png (already exists)\nSkipping 672/1025: skiddo.png (already exists)\nSkipping 673/1025: gogoat.png (already exists)\nSkipping 674/1025: pancham.png (already exists)\nSkipping 675/1025: pangoro.png (already exists)\nSkipping 676/1025: furfrou.png (already exists)\nSkipping 677/1025: espurr.png (already exists)\nSkipping 678/1025: meowstic.png (already exists)\nSkipping 679/1025: honedge.png (already exists)\nSkipping 680/1025: doublade.png (already exists)\nSkipping 681/1025: aegislash.png (already exists)\nSkipping 682/1025: spritzee.png (already exists)\nSkipping 683/1025: aromatisse.png (already exists)\nSkipping 684/1025: swirlix.png (already exists)\nSkipping 685/1025: slurpuff.png (already exists)\nSkipping 686/1025: inkay.png (already exists)\nSkipping 687/1025: malamar.png (already exists)\nSkipping 688/1025: binacle.png (already exists)\nSkipping 689/1025: barbaracle.png (already exists)\nSkipping 690/1025: skrelp.png (already exists)\nSkipping 691/1025: dragalge.png (already exists)\nSkipping 692/1025: clauncher.png (already exists)\nSkipping 693/1025: clawitzer.png (already exists)\nSkipping 694/1025: helioptile.png (already exists)\nSkipping 695/1025: heliolisk.png (already exists)\nSkipping 696/1025: tyrunt.png (already exists)\nSkipping 697/1025: tyrantrum.png (already exists)\nSkipping 698/1025: amaura.png (already exists)\nSkipping 699/1025: aurorus.png (already exists)\nSkipping 700/1025: sylveon.png (already exists)\nSkipping 701/1025: hawlucha.png (already exists)\nSkipping 702/1025: dedenne.png (already exists)\nSkipping 703/1025: carbink.png (already exists)\nSkipping 704/1025: goomy.png (already exists)\nSkipping 705/1025: sliggoo.png (already exists)\nSkipping 706/1025: goodra.png (already exists)\nSkipping 707/1025: klefki.png (already exists)\nSkipping 708/1025: phantump.png (already exists)\nSkipping 709/1025: trevenant.png (already exists)\nSkipping 710/1025: pumpkaboo.png (already exists)\nSkipping 711/1025: gourgeist.png (already exists)\nSkipping 712/1025: bergmite.png (already exists)\nSkipping 713/1025: avalugg.png (already exists)\nSkipping 714/1025: noibat.png (already exists)\nSkipping 715/1025: noivern.png (already exists)\nSkipping 716/1025: xerneas.png (already exists)\nSkipping 717/1025: yveltal.png (already exists)\nSkipping 718/1025: zygarde.png (already exists)\nSkipping 719/1025: diancie.png (already exists)\nSkipping 720/1025: hoopa.png (already exists)\nSkipping 721/1025: volcanion.png (already exists)\nSkipping 722/1025: rowlet.png (already exists)\nSkipping 723/1025: dartrix.png (already exists)\nSkipping 724/1025: decidueye.png (already exists)\nSkipping 725/1025: litten.png (already exists)\nSkipping 726/1025: torracat.png (already exists)\nSkipping 727/1025: incineroar.png (already exists)\nSkipping 728/1025: popplio.png (already exists)\nSkipping 729/1025: brionne.png (already exists)\nSkipping 730/1025: primarina.png (already exists)\nSkipping 731/1025: pikipek.png (already exists)\nSkipping 732/1025: trumbeak.png (already exists)\nSkipping 733/1025: toucannon.png (already exists)\nSkipping 734/1025: yungoos.png (already exists)\nSkipping 735/1025: gumshoos.png (already exists)\nSkipping 736/1025: grubbin.png (already exists)\nSkipping 737/1025: charjabug.png (already exists)\nSkipping 738/1025: vikavolt.png (already exists)\nSkipping 739/1025: crabrawler.png (already exists)\nSkipping 740/1025: crabominable.png (already exists)\nSkipping 741/1025: oricorio.png (already exists)\nSkipping 742/1025: cutiefly.png (already exists)\nSkipping 743/1025: ribombee.png (already exists)\nSkipping 744/1025: rockruff.png (already exists)\nSkipping 745/1025: lycanroc.png (already exists)\nSkipping 746/1025: wishiwashi.png (already exists)\nSkipping 747/1025: mareanie.png (already exists)\nSkipping 748/1025: toxapex.png (already exists)\nSkipping 749/1025: mudbray.png (already exists)\nSkipping 750/1025: mudsdale.png (already exists)\nSkipping 751/1025: dewpider.png (already exists)\nSkipping 752/1025: araquanid.png (already exists)\nSkipping 753/1025: fomantis.png (already exists)\nSkipping 754/1025: lurantis.png (already exists)\nSkipping 755/1025: morelull.png (already exists)\nSkipping 756/1025: shiinotic.png (already exists)\nSkipping 757/1025: salandit.png (already exists)\nSkipping 758/1025: salazzle.png (already exists)\nSkipping 759/1025: stufful.png (already exists)\nSkipping 760/1025: bewear.png (already exists)\nSkipping 761/1025: bounsweet.png (already exists)\nSkipping 762/1025: steenee.png (already exists)\nSkipping 763/1025: tsareena.png (already exists)\nSkipping 764/1025: comfey.png (already exists)\nSkipping 765/1025: oranguru.png (already exists)\nSkipping 766/1025: passimian.png (already exists)\nSkipping 767/1025: wimpod.png (already exists)\nSkipping 768/1025: golisopod.png (already exists)\nSkipping 769/1025: sandygast.png (already exists)\nSkipping 770/1025: palossand.png (already exists)\nSkipping 771/1025: pyukumuku.png (already exists)\nSkipping 772/1025: type-null.png (already exists)\nSkipping 773/1025: silvally.png (already exists)\nSkipping 774/1025: minior.png (already exists)\nSkipping 775/1025: komala.png (already exists)\nSkipping 776/1025: turtonator.png (already exists)\nSkipping 777/1025: togedemaru.png (already exists)\nSkipping 778/1025: mimikyu.png (already exists)\nSkipping 779/1025: bruxish.png (already exists)\nSkipping 780/1025: drampa.png (already exists)\nSkipping 781/1025: dhelmise.png (already exists)\nSkipping 782/1025: jangmo-o.png (already exists)\nSkipping 783/1025: hakamo-o.png (already exists)\nSkipping 784/1025: kommo-o.png (already exists)\nSkipping 785/1025: tapu-koko.png (already exists)\nSkipping 786/1025: tapu-lele.png (already exists)\nSkipping 787/1025: tapu-bulu.png (already exists)\nSkipping 788/1025: tapu-fini.png (already exists)\nSkipping 789/1025: cosmog.png (already exists)\nSkipping 790/1025: cosmoem.png (already exists)\nSkipping 791/1025: solgaleo.png (already exists)\nSkipping 792/1025: lunala.png (already exists)\nSkipping 793/1025: nihilego.png (already exists)\nSkipping 794/1025: buzzwole.png (already exists)\nSkipping 795/1025: pheromosa.png (already exists)\nSkipping 796/1025: xurkitree.png (already exists)\nSkipping 797/1025: celesteela.png (already exists)\nSkipping 798/1025: kartana.png (already exists)\nSkipping 799/1025: guzzlord.png (already exists)\nSkipping 800/1025: necrozma.png (already exists)\nSkipping 801/1025: magearna.png (already exists)\nSkipping 802/1025: marshadow.png (already exists)\nSkipping 803/1025: poipole.png (already exists)\nSkipping 804/1025: naganadel.png (already exists)\nSkipping 805/1025: stakataka.png (already exists)\nSkipping 806/1025: blacephalon.png (already exists)\nSkipping 807/1025: zeraora.png (already exists)\nSkipping 808/1025: meltan.png (already exists)\nSkipping 809/1025: melmetal.png (already exists)\nSkipping 810/1025: grookey.png (already exists)\nSkipping 811/1025: thwackey.png (already exists)\nSkipping 812/1025: rillaboom.png (already exists)\nSkipping 813/1025: scorbunny.png (already exists)\nSkipping 814/1025: raboot.png (already exists)\nSkipping 815/1025: cinderace.png (already exists)\nSkipping 816/1025: sobble.png (already exists)\nSkipping 817/1025: drizzile.png (already exists)\nSkipping 818/1025: inteleon.png (already exists)\nSkipping 819/1025: skwovet.png (already exists)\nSkipping 820/1025: greedent.png (already exists)\nSkipping 821/1025: rookidee.png (already exists)\nSkipping 822/1025: corvisquire.png (already exists)\nSkipping 823/1025: corviknight.png (already exists)\nSkipping 824/1025: blipbug.png (already exists)\nSkipping 825/1025: dottler.png (already exists)\nSkipping 826/1025: orbeetle.png (already exists)\nSkipping 827/1025: nickit.png (already exists)\nSkipping 828/1025: thievul.png (already exists)\nSkipping 829/1025: gossifleur.png (already exists)\nSkipping 830/1025: eldegoss.png (already exists)\nSkipping 831/1025: wooloo.png (already exists)\nSkipping 832/1025: dubwool.png (already exists)\nSkipping 833/1025: chewtle.png (already exists)\nSkipping 834/1025: drednaw.png (already exists)\nSkipping 835/1025: yamper.png (already exists)\nSkipping 836/1025: boltund.png (already exists)\nSkipping 837/1025: rolycoly.png (already exists)\nSkipping 838/1025: carkol.png (already exists)\nSkipping 839/1025: coalossal.png (already exists)\nSkipping 840/1025: applin.png (already exists)\nSkipping 841/1025: flapple.png (already exists)\nSkipping 842/1025: appletun.png (already exists)\nSkipping 843/1025: silicobra.png (already exists)\nSkipping 844/1025: sandaconda.png (already exists)\nSkipping 845/1025: cramorant.png (already exists)\nSkipping 846/1025: arrokuda.png (already exists)\nSkipping 847/1025: barraskewda.png (already exists)\nSkipping 848/1025: toxel.png (already exists)\nSkipping 849/1025: toxtricity.png (already exists)\nSkipping 850/1025: sizzlipede.png (already exists)\nSkipping 851/1025: centiskorch.png (already exists)\nSkipping 852/1025: clobbopus.png (already exists)\nSkipping 853/1025: grapploct.png (already exists)\nSkipping 854/1025: sinistea.png (already exists)\nSkipping 855/1025: polteageist.png (already exists)\nSkipping 856/1025: hatenna.png (already exists)\nSkipping 857/1025: hattrem.png (already exists)\nSkipping 858/1025: hatterene.png (already exists)\nSkipping 859/1025: impidimp.png (already exists)\nSkipping 860/1025: morgrem.png (already exists)\nSkipping 861/1025: grimmsnarl.png (already exists)\nSkipping 862/1025: obstagoon.png (already exists)\nSkipping 863/1025: perrserker.png (already exists)\nSkipping 864/1025: cursola.png (already exists)\nSkipping 865/1025: sirfetchd.png (already exists)\nSkipping 866/1025: mr-rime.png (already exists)\nSkipping 867/1025: runerigus.png (already exists)\nSkipping 868/1025: milcery.png (already exists)\nSkipping 869/1025: alcremie.png (already exists)\nSkipping 870/1025: falinks.png (already exists)\nSkipping 871/1025: pincurchin.png (already exists)\nSkipping 872/1025: snom.png (already exists)\nSkipping 873/1025: frosmoth.png (already exists)\nSkipping 874/1025: stonjourner.png (already exists)\nSkipping 875/1025: eiscue.png (already exists)\nSkipping 876/1025: indeedee.png (already exists)\nSkipping 877/1025: morpeko.png (already exists)\nSkipping 878/1025: cufant.png (already exists)\nSkipping 879/1025: copperajah.png (already exists)\nSkipping 880/1025: dracozolt.png (already exists)\nSkipping 881/1025: arctozolt.png (already exists)\nSkipping 882/1025: dracovish.png (already exists)\nSkipping 883/1025: arctovish.png (already exists)\nSkipping 884/1025: duraludon.png (already exists)\nSkipping 885/1025: dreepy.png (already exists)\nSkipping 886/1025: drakloak.png (already exists)\nSkipping 887/1025: dragapult.png (already exists)\nSkipping 888/1025: zacian.png (already exists)\nSkipping 889/1025: zamazenta.png (already exists)\nSkipping 890/1025: eternatus.png (already exists)\nSkipping 891/1025: kubfu.png (already exists)\nSkipping 892/1025: urshifu.png (already exists)\nSkipping 893/1025: zarude.png (already exists)\nSkipping 894/1025: regieleki.png (already exists)\nSkipping 895/1025: regidrago.png (already exists)\nSkipping 896/1025: glastrier.png (already exists)\nSkipping 897/1025: spectrier.png (already exists)\nSkipping 898/1025: calyrex.png (already exists)\nSkipping 899/1025: wyrdeer.png (already exists)\nSkipping 900/1025: kleavor.png (already exists)\nSkipping 901/1025: ursaluna.png (already exists)\nSkipping 902/1025: basculegion.png (already exists)\nSkipping 903/1025: sneasler.png (already exists)\nSkipping 904/1025: overqwil.png (already exists)\nSkipping 905/1025: enamorus.png (already exists)\nSkipping 906/1025: sprigatito.png (already exists)\nSkipping 907/1025: floragato.png (already exists)\nSkipping 908/1025: meowscarada.png (already exists)\nSkipping 909/1025: fuecoco.png (already exists)\nSkipping 910/1025: crocalor.png (already exists)\nSkipping 911/1025: skeledirge.png (already exists)\nSkipping 912/1025: quaxly.png (already exists)\nSkipping 913/1025: quaxwell.png (already exists)\nSkipping 914/1025: quaquaval.png (already exists)\nSkipping 915/1025: lechonk.png (already exists)\nSkipping 916/1025: oinkologne.png (already exists)\nSkipping 917/1025: tarountula.png (already exists)\nSkipping 918/1025: spidops.png (already exists)\nSkipping 919/1025: nymble.png (already exists)\nSkipping 920/1025: lokix.png (already exists)\nSkipping 921/1025: pawmi.png (already exists)\nSkipping 922/1025: pawmo.png (already exists)\nSkipping 923/1025: pawmot.png (already exists)\nSkipping 924/1025: tandemaus.png (already exists)\nSkipping 925/1025: maushold.png (already exists)\nSkipping 926/1025: fidough.png (already exists)\nSkipping 927/1025: dachsbun.png (already exists)\nSkipping 928/1025: smoliv.png (already exists)\nSkipping 929/1025: dolliv.png (already exists)\nSkipping 930/1025: arboliva.png (already exists)\nSkipping 931/1025: squawkabilly.png (already exists)\nSkipping 932/1025: nacli.png (already exists)\nSkipping 933/1025: naclstack.png (already exists)\nSkipping 934/1025: garganacl.png (already exists)\nSkipping 935/1025: charcadet.png (already exists)\nSkipping 936/1025: armarouge.png (already exists)\nSkipping 937/1025: ceruledge.png (already exists)\nSkipping 938/1025: tadbulb.png (already exists)\nSkipping 939/1025: bellibolt.png (already exists)\nSkipping 940/1025: wattrel.png (already exists)\nSkipping 941/1025: kilowattrel.png (already exists)\nSkipping 942/1025: maschiff.png (already exists)\nSkipping 943/1025: mabosstiff.png (already exists)\nSkipping 944/1025: shroodle.png (already exists)\nSkipping 945/1025: grafaiai.png (already exists)\nSkipping 946/1025: bramblin.png (already exists)\nSkipping 947/1025: brambleghast.png (already exists)\nSkipping 948/1025: toedscool.png (already exists)\nSkipping 949/1025: toedscruel.png (already exists)\nSkipping 950/1025: klawf.png (already exists)\nSkipping 951/1025: capsakid.png (already exists)\nSkipping 952/1025: scovillain.png (already exists)\nSkipping 953/1025: rellor.png (already exists)\nSkipping 954/1025: rabsca.png (already exists)\nSkipping 955/1025: flittle.png (already exists)\nSkipping 956/1025: espathra.png (already exists)\nSkipping 957/1025: tinkatink.png (already exists)\nSkipping 958/1025: tinkatuff.png (already exists)\nSkipping 959/1025: tinkaton.png (already exists)\nSkipping 960/1025: wiglett.png (already exists)\nSkipping 961/1025: wugtrio.png (already exists)\nSkipping 962/1025: bombirdier.png (already exists)\nSkipping 963/1025: finizen.png (already exists)\nSkipping 964/1025: palafin.png (already exists)\nSkipping 965/1025: varoom.png (already exists)\nSkipping 966/1025: revavroom.png (already exists)\nSkipping 967/1025: cyclizar.png (already exists)\nSkipping 968/1025: orthworm.png (already exists)\nSkipping 969/1025: glimmet.png (already exists)\nSkipping 970/1025: glimmora.png (already exists)\nSkipping 971/1025: greavard.png (already exists)\nSkipping 972/1025: houndstone.png (already exists)\nSkipping 973/1025: flamigo.png (already exists)\nSkipping 974/1025: cetoddle.png (already exists)\nSkipping 975/1025: cetitan.png (already exists)\nSkipping 976/1025: veluza.png (already exists)\nSkipping 977/1025: dondozo.png (already exists)\nSkipping 978/1025: tatsugiri.png (already exists)\nSkipping 979/1025: annihilape.png (already exists)\nSkipping 980/1025: clodsire.png (already exists)\nSkipping 981/1025: farigiraf.png (already exists)\nSkipping 982/1025: dudunsparce.png (already exists)\nSkipping 983/1025: kingambit.png (already exists)\nSkipping 984/1025: great-tusk.png (already exists)\nSkipping 985/1025: scream-tail.png (already exists)\nSkipping 986/1025: brute-bonnet.png (already exists)\nSkipping 987/1025: flutter-mane.png (already exists)\nSkipping 988/1025: slither-wing.png (already exists)\nSkipping 989/1025: sandy-shocks.png (already exists)\nSkipping 990/1025: iron-treads.png (already exists)\nSkipping 991/1025: iron-bundle.png (already exists)\nSkipping 992/1025: iron-hands.png (already exists)\nSkipping 993/1025: iron-jugulis.png (already exists)\nSkipping 994/1025: iron-moth.png (already exists)\nSkipping 995/1025: iron-thorns.png (already exists)\nSkipping 996/1025: frigibax.png (already exists)\nSkipping 997/1025: arctibax.png (already exists)\nSkipping 998/1025: baxcalibur.png (already exists)\nSkipping 999/1025: gimmighoul.png (already exists)\nSkipping 1000/1025: gholdengo.png (already exists)\nSkipping 1001/1025: wo-chien.png (already exists)\nSkipping 1002/1025: chien-pao.png (already exists)\nSkipping 1003/1025: ting-lu.png (already exists)\nSkipping 1004/1025: chi-yu.png (already exists)\nSkipping 1005/1025: roaring-moon.png (already exists)\nSkipping 1006/1025: iron-valiant.png (already exists)\nSkipping 1007/1025: koraidon.png (already exists)\nSkipping 1008/1025: miraidon.png (already exists)\nSkipping 1009/1025: walking-wake.png (already exists)\nSkipping 1010/1025: iron-leaves.png (already exists)\nSkipping 1011/1025: dipplin.png (already exists)\nSkipping 1012/1025: poltchageist.png (already exists)\nSkipping 1013/1025: sinistcha.png (already exists)\nSkipping 1014/1025: okidogi.png (already exists)\nSkipping 1015/1025: munkidori.png (already exists)\nSkipping 1016/1025: fezandipiti.png (already exists)\nSkipping 1017/1025: ogerpon.png (already exists)\nSkipping 1018/1025: archaludon.png (already exists)\nSkipping 1019/1025: hydrapple.png (already exists)\nSkipping 1020/1025: gouging-fire.png (already exists)\nSkipping 1021/1025: raging-bolt.png (already exists)\nSkipping 1022/1025: iron-boulder.png (already exists)\nSkipping 1023/1025: iron-crown.png (already exists)\nSkipping 1024/1025: terapagos.png (already exists)\nSkipping 1025/1025: pecharunt.png (already exists)\nDownload complete! Downloaded: 0, Skipped: 1025, Total found: 1025\nProcessing 1025 Pokemon sprites...\nCanvas size: 48x48\nSprite will occupy 95.0% of canvas (45 pixels max)\nBinarization using 'adaptive' method\nProcessed 1/1025: parasect.png\nProcessed 2/1025: sobble.png\nProcessed 3/1025: lumineon.png\nProcessed 4/1025: raikou.png\nProcessed 5/1025: runerigus.png\nProcessed 6/1025: dedenne.png\nProcessed 7/1025: pyroar.png\nProcessed 8/1025: pawmi.png\nProcessed 9/1025: articuno.png\nProcessed 10/1025: meowstic.png\nProcessed 11/1025: magmortar.png\nProcessed 12/1025: bulbasaur.png\nProcessed 13/1025: banette.png\nProcessed 14/1025: staraptor.png\nProcessed 15/1025: pidove.png\nProcessed 16/1025: morgrem.png\nProcessed 17/1025: comfey.png\nProcessed 18/1025: taillow.png\nProcessed 19/1025: charizard.png\nProcessed 20/1025: infernape.png\nProcessed 21/1025: sandshrew.png\nProcessed 22/1025: marshadow.png\nProcessed 23/1025: alakazam.png\nProcessed 24/1025: lickitung.png\nProcessed 25/1025: starmie.png\nProcessed 26/1025: wishiwashi.png\nProcessed 27/1025: yanmega.png\nProcessed 28/1025: yveltal.png\nProcessed 29/1025: zigzagoon.png\nProcessed 30/1025: petilil.png\nProcessed 31/1025: torterra.png\nProcessed 32/1025: purugly.png\nProcessed 33/1025: tandemaus.png\nProcessed 34/1025: cresselia.png\nProcessed 35/1025: regigigas.png\nProcessed 36/1025: palkia.png\nProcessed 37/1025: impidimp.png\nProcessed 38/1025: froakie.png\nProcessed 39/1025: kilowattrel.png\nProcessed 40/1025: arrokuda.png\nProcessed 41/1025: munna.png\nProcessed 42/1025: bellossom.png\nProcessed 43/1025: gabite.png\nProcessed 44/1025: tapu-koko.png\nProcessed 45/1025: mareep.png\nProcessed 46/1025: shieldon.png\nProcessed 47/1025: sneasler.png\nProcessed 48/1025: stoutland.png\nProcessed 49/1025: corviknight.png\nProcessed 50/1025: walking-wake.png\nProcessed 51/1025: accelgor.png\nProcessed 52/1025: makuhita.png\nProcessed 53/1025: seviper.png\nProcessed 54/1025: tirtouga.png\nProcessed 55/1025: lampent.png\nProcessed 56/1025: chi-yu.png\nProcessed 57/1025: ceruledge.png\nProcessed 58/1025: nidoking.png\nProcessed 59/1025: huntail.png\nProcessed 60/1025: nosepass.png\nProcessed 61/1025: carracosta.png\nProcessed 62/1025: nidoqueen.png\nProcessed 63/1025: inkay.png\nProcessed 64/1025: archeops.png\nProcessed 65/1025: avalugg.png\nProcessed 66/1025: grubbin.png\nProcessed 67/1025: sentret.png\nProcessed 68/1025: golbat.png\nProcessed 69/1025: skorupi.png\nProcessed 70/1025: conkeldurr.png\nProcessed 71/1025: jellicent.png\nProcessed 72/1025: yungoos.png\nProcessed 73/1025: meltan.png\nProcessed 74/1025: arctovish.png\nProcessed 75/1025: barraskewda.png\nProcessed 76/1025: luxio.png\nProcessed 77/1025: duraludon.png\nProcessed 78/1025: binacle.png\nProcessed 79/1025: galvantula.png\nProcessed 80/1025: crawdaunt.png\nProcessed 81/1025: hippopotas.png\nProcessed 82/1025: swirlix.png\nProcessed 83/1025: finneon.png\nProcessed 84/1025: kleavor.png\nProcessed 85/1025: medicham.png\nProcessed 86/1025: yamper.png\nProcessed 87/1025: poliwrath.png\nProcessed 88/1025: deoxys.png\nProcessed 89/1025: magnemite.png\nProcessed 90/1025: florges.png\nProcessed 91/1025: serperior.png\nProcessed 92/1025: coalossal.png\nProcessed 93/1025: shinx.png\nProcessed 94/1025: quaquaval.png\nProcessed 95/1025: budew.png\nProcessed 96/1025: shellos.png\nProcessed 97/1025: gothita.png\nProcessed 98/1025: overqwil.png\nProcessed 99/1025: porygon.png\nProcessed 100/1025: alomomola.png\nProcessed 101/1025: urshifu.png\nProcessed 102/1025: electrike.png\nProcessed 103/1025: dratini.png\nProcessed 104/1025: mienfoo.png\nProcessed 105/1025: reshiram.png\nProcessed 106/1025: sawsbuck.png\nProcessed 107/1025: blaziken.png\nProcessed 108/1025: flareon.png\nProcessed 109/1025: gliscor.png\nProcessed 110/1025: heatran.png\nProcessed 111/1025: gholdengo.png\nProcessed 112/1025: thievul.png\nProcessed 113/1025: beedrill.png\nProcessed 114/1025: butterfree.png\nProcessed 115/1025: tadbulb.png\nProcessed 116/1025: fletchinder.png\nProcessed 117/1025: tsareena.png\nProcessed 118/1025: ludicolo.png\nProcessed 119/1025: phanpy.png\nProcessed 120/1025: toxicroak.png\nProcessed 121/1025: cyndaquil.png\nProcessed 122/1025: swellow.png\nProcessed 123/1025: machop.png\nProcessed 124/1025: eternatus.png\nProcessed 125/1025: fletchling.png\nProcessed 126/1025: naganadel.png\nProcessed 127/1025: ariados.png\nProcessed 128/1025: mr-mime.png\nProcessed 129/1025: passimian.png\nProcessed 130/1025: frogadier.png\nProcessed 131/1025: roggenrola.png\nProcessed 132/1025: shroodle.png\nProcessed 133/1025: shedinja.png\nProcessed 134/1025: duosion.png\nProcessed 135/1025: scraggy.png\nProcessed 136/1025: gumshoos.png\nProcessed 137/1025: latias.png\nProcessed 138/1025: shelmet.png\nProcessed 139/1025: sawk.png\nProcessed 140/1025: pawmo.png\nProcessed 141/1025: venusaur.png\nProcessed 142/1025: azelf.png\nProcessed 143/1025: sinistea.png\nProcessed 144/1025: tornadus.png\nProcessed 145/1025: quaxwell.png\nProcessed 146/1025: obstagoon.png\nProcessed 147/1025: musharna.png\nProcessed 148/1025: drampa.png\nProcessed 149/1025: liepard.png\nProcessed 150/1025: aipom.png\nProcessed 151/1025: seaking.png\nProcessed 152/1025: arboliva.png\nProcessed 153/1025: xurkitree.png\nProcessed 154/1025: meditite.png\nProcessed 155/1025: heatmor.png\nProcessed 156/1025: chandelure.png\nProcessed 157/1025: ivysaur.png\nProcessed 158/1025: calyrex.png\nProcessed 159/1025: gible.png\nProcessed 160/1025: corphish.png\nProcessed 161/1025: octillery.png\nProcessed 162/1025: cubone.png\nProcessed 163/1025: crocalor.png\nProcessed 164/1025: dusclops.png\nProcessed 165/1025: koffing.png\nProcessed 166/1025: wo-chien.png\nProcessed 167/1025: gengar.png\nProcessed 168/1025: larvitar.png\nProcessed 169/1025: rampardos.png\nProcessed 170/1025: tyrogue.png\nProcessed 171/1025: wigglytuff.png\nProcessed 172/1025: oricorio.png\nProcessed 173/1025: castform.png\nProcessed 174/1025: quaxly.png\nProcessed 175/1025: dusknoir.png\nProcessed 176/1025: chinchou.png\nProcessed 177/1025: litleo.png\nProcessed 178/1025: floragato.png\nProcessed 179/1025: manectric.png\nProcessed 180/1025: grapploct.png\nProcessed 181/1025: ditto.png\nProcessed 182/1025: scrafty.png\nProcessed 183/1025: great-tusk.png\nProcessed 184/1025: kakuna.png\nProcessed 185/1025: gastrodon.png\nProcessed 186/1025: wingull.png\nProcessed 187/1025: flaaffy.png\nProcessed 188/1025: sunflora.png\nProcessed 189/1025: iron-jugulis.png\nProcessed 190/1025: gastly.png\nProcessed 191/1025: tarountula.png\nProcessed 192/1025: delcatty.png\nProcessed 193/1025: granbull.png\nProcessed 194/1025: iron-leaves.png\nProcessed 195/1025: whismur.png\nProcessed 196/1025: gourgeist.png\nProcessed 197/1025: cleffa.png\nProcessed 198/1025: growlithe.png\nProcessed 199/1025: sigilyph.png\nProcessed 200/1025: cascoon.png\nProcessed 201/1025: pecharunt.png\nProcessed 202/1025: rapidash.png\nProcessed 203/1025: hydreigon.png\nProcessed 204/1025: cryogonal.png\nProcessed 205/1025: stunfisk.png\nProcessed 206/1025: stantler.png\nProcessed 207/1025: dracovish.png\nProcessed 208/1025: tapu-fini.png\nProcessed 209/1025: steenee.png\nProcessed 210/1025: slither-wing.png\nProcessed 211/1025: snubbull.png\nProcessed 212/1025: crabominable.png\nProcessed 213/1025: surskit.png\nProcessed 214/1025: ampharos.png\nProcessed 215/1025: monferno.png\nProcessed 216/1025: terapagos.png\nProcessed 217/1025: rhydon.png\nProcessed 218/1025: orthworm.png\nProcessed 219/1025: clodsire.png\nProcessed 220/1025: arbok.png\nProcessed 221/1025: amaura.png\nProcessed 222/1025: lucario.png\nProcessed 223/1025: hatterene.png\nProcessed 224/1025: pupitar.png\nProcessed 225/1025: elekid.png\nProcessed 226/1025: goomy.png\nProcessed 227/1025: vullaby.png\nProcessed 228/1025: venipede.png\nProcessed 229/1025: doublade.png\nProcessed 230/1025: bellibolt.png\nProcessed 231/1025: regieleki.png\nProcessed 232/1025: tangela.png\nProcessed 233/1025: pignite.png\nProcessed 234/1025: sandile.png\nProcessed 235/1025: brionne.png\nProcessed 236/1025: machamp.png\nProcessed 237/1025: wailord.png\nProcessed 238/1025: rhyhorn.png\nProcessed 239/1025: ninjask.png\nProcessed 240/1025: salandit.png\nProcessed 241/1025: grotle.png\nProcessed 242/1025: dragonair.png\nProcessed 243/1025: mime-jr.png\nProcessed 244/1025: zebstrika.png\nProcessed 245/1025: zarude.png\nProcessed 246/1025: pikachu.png\nProcessed 247/1025: typhlosion.png\nProcessed 248/1025: sizzlipede.png\nProcessed 249/1025: aromatisse.png\nProcessed 250/1025: chimecho.png\nProcessed 251/1025: diggersby.png\nProcessed 252/1025: trevenant.png\nProcessed 253/1025: audino.png\nProcessed 254/1025: dodrio.png\nProcessed 255/1025: azumarill.png\nProcessed 256/1025: perrserker.png\nProcessed 257/1025: aggron.png\nProcessed 258/1025: dialga.png\nProcessed 259/1025: tapu-bulu.png\nProcessed 260/1025: totodile.png\nProcessed 261/1025: tauros.png\nProcessed 262/1025: silcoon.png\nProcessed 263/1025: cottonee.png\nProcessed 264/1025: araquanid.png\nProcessed 265/1025: gallade.png\nProcessed 266/1025: tropius.png\nProcessed 267/1025: bouffalant.png\nProcessed 268/1025: wattrel.png\nProcessed 269/1025: frigibax.png\nProcessed 270/1025: woobat.png\nProcessed 271/1025: archen.png\nProcessed 272/1025: ho-oh.png\nProcessed 273/1025: scorbunny.png\nProcessed 274/1025: iron-valiant.png\nProcessed 275/1025: noibat.png\nProcessed 276/1025: wartortle.png\nProcessed 277/1025: chien-pao.png\nProcessed 278/1025: eevee.png\nProcessed 279/1025: mewtwo.png\nProcessed 280/1025: buneary.png\nProcessed 281/1025: sableye.png\nProcessed 282/1025: bidoof.png\nProcessed 283/1025: rookidee.png\nProcessed 284/1025: scovillain.png\nProcessed 285/1025: braixen.png\nProcessed 286/1025: raging-bolt.png\nProcessed 287/1025: elgyem.png\nProcessed 288/1025: gloom.png\nProcessed 289/1025: electabuzz.png\nProcessed 290/1025: piloswine.png\nProcessed 291/1025: miltank.png\nProcessed 292/1025: guzzlord.png\nProcessed 293/1025: cinccino.png\nProcessed 294/1025: greavard.png\nProcessed 295/1025: hattrem.png\nProcessed 296/1025: hatenna.png\nProcessed 297/1025: anorith.png\nProcessed 298/1025: wurmple.png\nProcessed 299/1025: shelgon.png\nProcessed 300/1025: toxapex.png\nProcessed 301/1025: swalot.png\nProcessed 302/1025: lurantis.png\nProcessed 303/1025: tyrunt.png\nProcessed 304/1025: pidgey.png\nProcessed 305/1025: quagsire.png\nProcessed 306/1025: gurdurr.png\nProcessed 307/1025: amoonguss.png\nProcessed 308/1025: archaludon.png\nProcessed 309/1025: mankey.png\nProcessed 310/1025: illumise.png\nProcessed 311/1025: cacturne.png\nProcessed 312/1025: tinkatink.png\nProcessed 313/1025: igglybuff.png\nProcessed 314/1025: maschiff.png\nProcessed 315/1025: qwilfish.png\nProcessed 316/1025: goldeen.png\nProcessed 317/1025: abra.png\nProcessed 318/1025: tinkatuff.png\nProcessed 319/1025: nidoran-f.png\nProcessed 320/1025: purrloin.png\nProcessed 321/1025: gulpin.png\nProcessed 322/1025: munchlax.png\nProcessed 323/1025: crobat.png\nProcessed 324/1025: volcanion.png\nProcessed 325/1025: lillipup.png\nProcessed 326/1025: xerneas.png\nProcessed 327/1025: lechonk.png\nProcessed 328/1025: lapras.png\nProcessed 329/1025: gyarados.png\nProcessed 330/1025: walrein.png\nProcessed 331/1025: sneasel.png\nProcessed 332/1025: lickilicky.png\nProcessed 333/1025: skuntank.png\nProcessed 334/1025: murkrow.png\nProcessed 335/1025: bibarel.png\nProcessed 336/1025: tentacool.png\nProcessed 337/1025: watchog.png\nProcessed 338/1025: oshawott.png\nProcessed 339/1025: pidgeotto.png\nProcessed 340/1025: espurr.png\nProcessed 341/1025: houndoom.png\nProcessed 342/1025: nidorino.png\nProcessed 343/1025: hariyama.png\nProcessed 344/1025: milcery.png\nProcessed 345/1025: talonflame.png\nProcessed 346/1025: dustox.png\nProcessed 347/1025: mimikyu.png\nProcessed 348/1025: kommo-o.png\nProcessed 349/1025: weepinbell.png\nProcessed 350/1025: seel.png\nProcessed 351/1025: houndstone.png\nProcessed 352/1025: pyukumuku.png\nProcessed 353/1025: naclstack.png\nProcessed 354/1025: annihilape.png\nProcessed 355/1025: pineco.png\nProcessed 356/1025: cosmog.png\nProcessed 357/1025: kadabra.png\nProcessed 358/1025: swablu.png\nProcessed 359/1025: skitty.png\nProcessed 360/1025: regidrago.png\nProcessed 361/1025: masquerain.png\nProcessed 362/1025: minun.png\nProcessed 363/1025: heracross.png\nProcessed 364/1025: cursola.png\nProcessed 365/1025: trapinch.png\nProcessed 366/1025: girafarig.png\nProcessed 367/1025: tyrantrum.png\nProcessed 368/1025: graveler.png\nProcessed 369/1025: corvisquire.png\nProcessed 370/1025: poochyena.png\nProcessed 371/1025: electivire.png\nProcessed 372/1025: shiinotic.png\nProcessed 373/1025: melmetal.png\nProcessed 374/1025: ledian.png\nProcessed 375/1025: cherrim.png\nProcessed 376/1025: popplio.png\nProcessed 377/1025: sandy-shocks.png\nProcessed 378/1025: polteageist.png\nProcessed 379/1025: primeape.png\nProcessed 380/1025: glastrier.png\nProcessed 381/1025: vanillish.png\nProcessed 382/1025: aegislash.png\nProcessed 383/1025: probopass.png\nProcessed 384/1025: skiploom.png\nProcessed 385/1025: chesnaught.png\nProcessed 386/1025: kyurem.png\nProcessed 387/1025: drakloak.png\nProcessed 388/1025: carnivine.png\nProcessed 389/1025: torracat.png\nProcessed 390/1025: gardevoir.png\nProcessed 391/1025: pinsir.png\nProcessed 392/1025: snorunt.png\nProcessed 393/1025: samurott.png\nProcessed 394/1025: necrozma.png\nProcessed 395/1025: darmanitan.png\nProcessed 396/1025: charcadet.png\nProcessed 397/1025: smeargle.png\nProcessed 398/1025: cetoddle.png\nProcessed 399/1025: rellor.png\nProcessed 400/1025: inteleon.png\nProcessed 401/1025: magby.png\nProcessed 402/1025: ursaring.png\nProcessed 403/1025: crabrawler.png\nProcessed 404/1025: haxorus.png\nProcessed 405/1025: geodude.png\nProcessed 406/1025: foongus.png\nProcessed 407/1025: hippowdon.png\nProcessed 408/1025: poltchageist.png\nProcessed 409/1025: toedscool.png\nProcessed 410/1025: ursaluna.png\nProcessed 411/1025: delibird.png\nProcessed 412/1025: oranguru.png\nProcessed 413/1025: drapion.png\nProcessed 414/1025: koraidon.png\nProcessed 415/1025: smoochum.png\nProcessed 416/1025: scyther.png\nProcessed 417/1025: stunky.png\nProcessed 418/1025: swadloon.png\nProcessed 419/1025: klefki.png\nProcessed 420/1025: vivillon.png\nProcessed 421/1025: wormadam.png\nProcessed 422/1025: honedge.png\nProcessed 423/1025: entei.png\nProcessed 424/1025: servine.png\nProcessed 425/1025: zubat.png\nProcessed 426/1025: umbreon.png\nProcessed 427/1025: reuniclus.png\nProcessed 428/1025: froslass.png\nProcessed 429/1025: pheromosa.png\nProcessed 430/1025: brute-bonnet.png\nProcessed 431/1025: magcargo.png\nProcessed 432/1025: boldore.png\nProcessed 433/1025: genesect.png\nProcessed 434/1025: larvesta.png\nProcessed 435/1025: meowscarada.png\nProcessed 436/1025: golett.png\nProcessed 437/1025: kartana.png\nProcessed 438/1025: voltorb.png\nProcessed 439/1025: regirock.png\nProcessed 440/1025: cherubi.png\nProcessed 441/1025: barboach.png\nProcessed 442/1025: wooper.png\nProcessed 443/1025: escavalier.png\nProcessed 444/1025: vileplume.png\nProcessed 445/1025: mismagius.png\nProcessed 446/1025: blastoise.png\nProcessed 447/1025: litten.png\nProcessed 448/1025: sealeo.png\nProcessed 449/1025: dreepy.png\nProcessed 450/1025: snivy.png\nProcessed 451/1025: keldeo.png\nProcessed 452/1025: snorlax.png\nProcessed 453/1025: zamazenta.png\nProcessed 454/1025: lopunny.png\nProcessed 455/1025: persian.png\nProcessed 456/1025: beartic.png\nProcessed 457/1025: gothorita.png\nProcessed 458/1025: giratina.png\nProcessed 459/1025: rattata.png\nProcessed 460/1025: starly.png\nProcessed 461/1025: bewear.png\nProcessed 462/1025: nymble.png\nProcessed 463/1025: houndour.png\nProcessed 464/1025: eelektrik.png\nProcessed 465/1025: appletun.png\nProcessed 466/1025: quilava.png\nProcessed 467/1025: whimsicott.png\nProcessed 468/1025: magikarp.png\nProcessed 469/1025: skeledirge.png\nProcessed 470/1025: drowzee.png\nProcessed 471/1025: squawkabilly.png\nProcessed 472/1025: claydol.png\nProcessed 473/1025: clamperl.png\nProcessed 474/1025: seedot.png\nProcessed 475/1025: dottler.png\nProcessed 476/1025: garchomp.png\nProcessed 477/1025: skiddo.png\nProcessed 478/1025: hypno.png\nProcessed 479/1025: weedle.png\nProcessed 480/1025: garganacl.png\nProcessed 481/1025: greedent.png\nProcessed 482/1025: stakataka.png\nProcessed 483/1025: ambipom.png\nProcessed 484/1025: spoink.png\nProcessed 485/1025: pikipek.png\nProcessed 486/1025: frillish.png\nProcessed 487/1025: cutiefly.png\nProcessed 488/1025: honchkrow.png\nProcessed 489/1025: tympole.png\nProcessed 490/1025: ting-lu.png\nProcessed 491/1025: meganium.png\nProcessed 492/1025: incineroar.png\nProcessed 493/1025: landorus.png\nProcessed 494/1025: lileep.png\nProcessed 495/1025: pachirisu.png\nProcessed 496/1025: iron-moth.png\nProcessed 497/1025: tyranitar.png\nProcessed 498/1025: magnezone.png\nProcessed 499/1025: beldum.png\nProcessed 500/1025: glameow.png\nProcessed 501/1025: dondozo.png\nProcessed 502/1025: swanna.png\nProcessed 503/1025: primarina.png\nProcessed 504/1025: nuzleaf.png\nProcessed 505/1025: zangoose.png\nProcessed 506/1025: gogoat.png\nProcessed 507/1025: jolteon.png\nProcessed 508/1025: moltres.png\nProcessed 509/1025: dragonite.png\nProcessed 510/1025: golurk.png\nProcessed 511/1025: metapod.png\nProcessed 512/1025: golisopod.png\nProcessed 513/1025: marowak.png\nProcessed 514/1025: gigalith.png\nProcessed 515/1025: emboar.png\nProcessed 516/1025: exeggutor.png\nProcessed 517/1025: magneton.png\nProcessed 518/1025: mesprit.png\nProcessed 519/1025: dipplin.png\nProcessed 520/1025: groudon.png\nProcessed 521/1025: wobbuffet.png\nProcessed 522/1025: delphox.png\nProcessed 523/1025: exploud.png\nProcessed 524/1025: dracozolt.png\nProcessed 525/1025: mantyke.png\nProcessed 526/1025: malamar.png\nProcessed 527/1025: simisage.png\nProcessed 528/1025: espeon.png\nProcessed 529/1025: paras.png\nProcessed 530/1025: noctowl.png\nProcessed 531/1025: hitmonlee.png\nProcessed 532/1025: slaking.png\nProcessed 533/1025: solrock.png\nProcessed 534/1025: pelipper.png\nProcessed 535/1025: komala.png\nProcessed 536/1025: poliwhirl.png\nProcessed 537/1025: palpitoad.png\nProcessed 538/1025: eelektross.png\nProcessed 539/1025: patrat.png\nProcessed 540/1025: spheal.png\nProcessed 541/1025: seadra.png\nProcessed 542/1025: lotad.png\nProcessed 543/1025: flutter-mane.png\nProcessed 544/1025: ralts.png\nProcessed 545/1025: registeel.png\nProcessed 546/1025: slurpuff.png\nProcessed 547/1025: arcanine.png\nProcessed 548/1025: fomantis.png\nProcessed 549/1025: smoliv.png\nProcessed 550/1025: eiscue.png\nProcessed 551/1025: tangrowth.png\nProcessed 552/1025: aerodactyl.png\nProcessed 553/1025: pincurchin.png\nProcessed 554/1025: bronzor.png\nProcessed 555/1025: sylveon.png\nProcessed 556/1025: munkidori.png\nProcessed 557/1025: kyogre.png\nProcessed 558/1025: sharpedo.png\nProcessed 559/1025: cufant.png\nProcessed 560/1025: flabebe.png\nProcessed 561/1025: quilladin.png\nProcessed 562/1025: feebas.png\nProcessed 563/1025: metagross.png\nProcessed 564/1025: maractus.png\nProcessed 565/1025: muk.png\nProcessed 566/1025: empoleon.png\nProcessed 567/1025: chimchar.png\nProcessed 568/1025: sandaconda.png\nProcessed 569/1025: staryu.png\nProcessed 570/1025: scream-tail.png\nProcessed 571/1025: pancham.png\nProcessed 572/1025: dwebble.png\nProcessed 573/1025: bruxish.png\nProcessed 574/1025: joltik.png\nProcessed 575/1025: armaldo.png\nProcessed 576/1025: grimmsnarl.png\nProcessed 577/1025: phione.png\nProcessed 578/1025: frosmoth.png\nProcessed 579/1025: rhyperior.png\nProcessed 580/1025: pansear.png\nProcessed 581/1025: alcremie.png\nProcessed 582/1025: happiny.png\nProcessed 583/1025: spearow.png\nProcessed 584/1025: helioptile.png\nProcessed 585/1025: kingambit.png\nProcessed 586/1025: fearow.png\nProcessed 587/1025: wooloo.png\nProcessed 588/1025: prinplup.png\nProcessed 589/1025: diancie.png\nProcessed 590/1025: rabsca.png\nProcessed 591/1025: poipole.png\nProcessed 592/1025: ogerpon.png\nProcessed 593/1025: klink.png\nProcessed 594/1025: farfetchd.png\nProcessed 595/1025: gorebyss.png\nProcessed 596/1025: iron-bundle.png\nProcessed 597/1025: noivern.png\nProcessed 598/1025: herdier.png\nProcessed 599/1025: tynamo.png\nProcessed 600/1025: whiscash.png\nProcessed 601/1025: nidoran-m.png\nProcessed 602/1025: diglett.png\nProcessed 603/1025: wyrdeer.png\nProcessed 604/1025: krokorok.png\nProcessed 605/1025: scizor.png\nProcessed 606/1025: nihilego.png\nProcessed 607/1025: skarmory.png\nProcessed 608/1025: remoraid.png\nProcessed 609/1025: shaymin.png\nProcessed 610/1025: hitmontop.png\nProcessed 611/1025: snom.png\nProcessed 612/1025: cosmoem.png\nProcessed 613/1025: miraidon.png\nProcessed 614/1025: slowbro.png\nProcessed 615/1025: numel.png\nProcessed 616/1025: glalie.png\nProcessed 617/1025: vaporeon.png\nProcessed 618/1025: porygon-z.png\nProcessed 619/1025: lunatone.png\nProcessed 620/1025: machoke.png\nProcessed 621/1025: mareanie.png\nProcessed 622/1025: bastiodon.png\nProcessed 623/1025: mandibuzz.png\nProcessed 624/1025: arceus.png\nProcessed 625/1025: throh.png\nProcessed 626/1025: abomasnow.png\nProcessed 627/1025: bronzong.png\nProcessed 628/1025: flittle.png\nProcessed 629/1025: greninja.png\nProcessed 630/1025: hoppip.png\nProcessed 631/1025: kecleon.png\nProcessed 632/1025: crustle.png\nProcessed 633/1025: shuppet.png\nProcessed 634/1025: stonjourner.png\nProcessed 635/1025: gossifleur.png\nProcessed 636/1025: ducklett.png\nProcessed 637/1025: plusle.png\nProcessed 638/1025: poliwag.png\nProcessed 639/1025: treecko.png\nProcessed 640/1025: torchic.png\nProcessed 641/1025: kabutops.png\nProcessed 642/1025: dunsparce.png\nProcessed 643/1025: flygon.png\nProcessed 644/1025: morpeko.png\nProcessed 645/1025: spectrier.png\nProcessed 646/1025: orbeetle.png\nProcessed 647/1025: dugtrio.png\nProcessed 648/1025: marill.png\nProcessed 649/1025: magearna.png\nProcessed 650/1025: toxtricity.png\nProcessed 651/1025: fezandipiti.png\nProcessed 652/1025: silvally.png\nProcessed 653/1025: cacnea.png\nProcessed 654/1025: slugma.png\nProcessed 655/1025: oddish.png\nProcessed 656/1025: jirachi.png\nProcessed 657/1025: veluza.png\nProcessed 658/1025: ribombee.png\nProcessed 659/1025: glimmora.png\nProcessed 660/1025: litwick.png\nProcessed 661/1025: pidgeot.png\nProcessed 662/1025: sandslash.png\nProcessed 663/1025: drilbur.png\nProcessed 664/1025: cranidos.png\nProcessed 665/1025: chansey.png\nProcessed 666/1025: blacephalon.png\nProcessed 667/1025: camerupt.png\nProcessed 668/1025: arctozolt.png\nProcessed 669/1025: dudunsparce.png\nProcessed 670/1025: pichu.png\nProcessed 671/1025: duskull.png\nProcessed 672/1025: palafin.png\nProcessed 673/1025: kangaskhan.png\nProcessed 674/1025: chingling.png\nProcessed 675/1025: whirlipede.png\nProcessed 676/1025: silicobra.png\nProcessed 677/1025: ferroseed.png\nProcessed 678/1025: toucannon.png\nProcessed 679/1025: barbaracle.png\nProcessed 680/1025: salamence.png\nProcessed 681/1025: zekrom.png\nProcessed 682/1025: haunter.png\nProcessed 683/1025: glaceon.png\nProcessed 684/1025: golduck.png\nProcessed 685/1025: milotic.png\nProcessed 686/1025: politoed.png\nProcessed 687/1025: indeedee.png\nProcessed 688/1025: absol.png\nProcessed 689/1025: mienshao.png\nProcessed 690/1025: sprigatito.png\nProcessed 691/1025: aron.png\nProcessed 692/1025: tepig.png\nProcessed 693/1025: klang.png\nProcessed 694/1025: zapdos.png\nProcessed 695/1025: clauncher.png\nProcessed 696/1025: riolu.png\nProcessed 697/1025: skwovet.png\nProcessed 698/1025: raichu.png\nProcessed 699/1025: varoom.png\nProcessed 700/1025: hitmonchan.png\nProcessed 701/1025: slakoth.png\nProcessed 702/1025: ponyta.png\nProcessed 703/1025: togepi.png\nProcessed 704/1025: snover.png\nProcessed 705/1025: dubwool.png\nProcessed 706/1025: solosis.png\nProcessed 707/1025: fraxure.png\nProcessed 708/1025: caterpie.png\nProcessed 709/1025: iron-hands.png\nProcessed 710/1025: gimmighoul.png\nProcessed 711/1025: psyduck.png\nProcessed 712/1025: teddiursa.png\nProcessed 713/1025: tapu-lele.png\nProcessed 714/1025: piplup.png\nProcessed 715/1025: flamigo.png\nProcessed 716/1025: okidogi.png\nProcessed 717/1025: nickit.png\nProcessed 718/1025: darumaka.png\nProcessed 719/1025: finizen.png\nProcessed 720/1025: weavile.png\nProcessed 721/1025: mawile.png\nProcessed 722/1025: sewaddle.png\nProcessed 723/1025: unfezant.png\nProcessed 724/1025: buzzwole.png\nProcessed 725/1025: rockruff.png\nProcessed 726/1025: manaphy.png\nProcessed 727/1025: bunnelby.png\nProcessed 728/1025: seismitoad.png\nProcessed 729/1025: simipour.png\nProcessed 730/1025: vanilluxe.png\nProcessed 731/1025: cubchoo.png\nProcessed 732/1025: mantine.png\nProcessed 733/1025: maushold.png\nProcessed 734/1025: basculin.png\nProcessed 735/1025: dartrix.png\nProcessed 736/1025: weezing.png\nProcessed 737/1025: corsola.png\nProcessed 738/1025: roserade.png\nProcessed 739/1025: carbink.png\nProcessed 740/1025: heliolisk.png\nProcessed 741/1025: omanyte.png\nProcessed 742/1025: rufflet.png\nProcessed 743/1025: zygarde.png\nProcessed 744/1025: mothim.png\nProcessed 745/1025: klinklang.png\nProcessed 746/1025: sliggoo.png\nProcessed 747/1025: raticate.png\nProcessed 748/1025: grookey.png\nProcessed 749/1025: carkol.png\nProcessed 750/1025: dhelmise.png\nProcessed 751/1025: vibrava.png\nProcessed 752/1025: wiglett.png\nProcessed 753/1025: shiftry.png\nProcessed 754/1025: swampert.png\nProcessed 755/1025: excadrill.png\nProcessed 756/1025: brambleghast.png\nProcessed 757/1025: morelull.png\nProcessed 758/1025: fuecoco.png\nProcessed 759/1025: chatot.png\nProcessed 760/1025: klawf.png\nProcessed 761/1025: jigglypuff.png\nProcessed 762/1025: enamorus.png\nProcessed 763/1025: loudred.png\nProcessed 764/1025: forretress.png\nProcessed 765/1025: togetic.png\nProcessed 766/1025: feraligatr.png\nProcessed 767/1025: bisharp.png\nProcessed 768/1025: lycanroc.png\nProcessed 769/1025: iron-treads.png\nProcessed 770/1025: mabosstiff.png\nProcessed 771/1025: vikavolt.png\nProcessed 772/1025: victreebel.png\nProcessed 773/1025: decidueye.png\nProcessed 774/1025: cinderace.png\nProcessed 775/1025: spinarak.png\nProcessed 776/1025: onix.png\nProcessed 777/1025: wugtrio.png\nProcessed 778/1025: aurorus.png\nProcessed 779/1025: venonat.png\nProcessed 780/1025: thundurus.png\nProcessed 781/1025: wimpod.png\nProcessed 782/1025: nidorina.png\nProcessed 783/1025: tranquill.png\nProcessed 784/1025: hydrapple.png\nProcessed 785/1025: farigiraf.png\nProcessed 786/1025: buizel.png\nProcessed 787/1025: mightyena.png\nProcessed 788/1025: vulpix.png\nProcessed 789/1025: clawitzer.png\nProcessed 790/1025: revavroom.png\nProcessed 791/1025: lilligant.png\nProcessed 792/1025: togedemaru.png\nProcessed 793/1025: minccino.png\nProcessed 794/1025: roaring-moon.png\nProcessed 795/1025: charjabug.png\nProcessed 796/1025: doduo.png\nProcessed 797/1025: ferrothorn.png\nProcessed 798/1025: misdreavus.png\nProcessed 799/1025: kingdra.png\nProcessed 800/1025: shuckle.png\nProcessed 801/1025: rayquaza.png\nProcessed 802/1025: palossand.png\nProcessed 803/1025: spidops.png\nProcessed 804/1025: mudbray.png\nProcessed 805/1025: kricketune.png\nProcessed 806/1025: skrelp.png\nProcessed 807/1025: luxray.png\nProcessed 808/1025: ledyba.png\nProcessed 809/1025: baltoy.png\nProcessed 810/1025: basculegion.png\nProcessed 811/1025: trumbeak.png\nProcessed 812/1025: dewpider.png\nProcessed 813/1025: applin.png\nProcessed 814/1025: sandygast.png\nProcessed 815/1025: yanma.png\nProcessed 816/1025: kubfu.png\nProcessed 817/1025: lugia.png\nProcessed 818/1025: turtonator.png\nProcessed 819/1025: hawlucha.png\nProcessed 820/1025: leavanny.png\nProcessed 821/1025: vespiquen.png\nProcessed 822/1025: solgaleo.png\nProcessed 823/1025: squirtle.png\nProcessed 824/1025: slowking.png\nProcessed 825/1025: exeggcute.png\nProcessed 826/1025: emolga.png\nProcessed 827/1025: carvanha.png\nProcessed 828/1025: zacian.png\nProcessed 829/1025: grafaiai.png\nProcessed 830/1025: clefairy.png\nProcessed 831/1025: bayleef.png\nProcessed 832/1025: drifloon.png\nProcessed 833/1025: stufful.png\nProcessed 834/1025: sceptile.png\nProcessed 835/1025: azurill.png\nProcessed 836/1025: chespin.png\nProcessed 837/1025: jangmo-o.png\nProcessed 838/1025: panpour.png\nProcessed 839/1025: meloetta.png\nProcessed 840/1025: glimmet.png\nProcessed 841/1025: sinistcha.png\nProcessed 842/1025: staravia.png\nProcessed 843/1025: linoone.png\nProcessed 844/1025: tinkaton.png\nProcessed 845/1025: iron-crown.png\nProcessed 846/1025: spiritomb.png\nProcessed 847/1025: zoroark.png\nProcessed 848/1025: chikorita.png\nProcessed 849/1025: vanillite.png\nProcessed 850/1025: scatterbug.png\nProcessed 851/1025: spinda.png\nProcessed 852/1025: beheeyem.png\nProcessed 853/1025: grimer.png\nProcessed 854/1025: pawmot.png\nProcessed 855/1025: leafeon.png\nProcessed 856/1025: cyclizar.png\nProcessed 857/1025: deerling.png\nProcessed 858/1025: celebi.png\nProcessed 859/1025: breloom.png\nProcessed 860/1025: turtwig.png\nProcessed 861/1025: zweilous.png\nProcessed 862/1025: burmy.png\nProcessed 863/1025: dolliv.png\nProcessed 864/1025: copperajah.png\nProcessed 865/1025: swinub.png\nProcessed 866/1025: axew.png\nProcessed 867/1025: dragapult.png\nProcessed 868/1025: pangoro.png\nProcessed 869/1025: darkrai.png\nProcessed 870/1025: krabby.png\nProcessed 871/1025: dewgong.png\nProcessed 872/1025: eldegoss.png\nProcessed 873/1025: sirfetchd.png\nProcessed 874/1025: volcarona.png\nProcessed 875/1025: vigoroth.png\nProcessed 876/1025: bombirdier.png\nProcessed 877/1025: falinks.png\nProcessed 878/1025: ninetales.png\nProcessed 879/1025: pawniard.png\nProcessed 880/1025: rolycoly.png\nProcessed 881/1025: blitzle.png\nProcessed 882/1025: porygon2.png\nProcessed 883/1025: croagunk.png\nProcessed 884/1025: xatu.png\nProcessed 885/1025: lairon.png\nProcessed 886/1025: tentacruel.png\nProcessed 887/1025: toedscruel.png\nProcessed 888/1025: krookodile.png\nProcessed 889/1025: gothitelle.png\nProcessed 890/1025: regice.png\nProcessed 891/1025: goodra.png\nProcessed 892/1025: thwackey.png\nProcessed 893/1025: meowth.png\nProcessed 894/1025: grovyle.png\nProcessed 895/1025: volbeat.png\nProcessed 896/1025: unown.png\nProcessed 897/1025: terrakion.png\nProcessed 898/1025: togekiss.png\nProcessed 899/1025: kricketot.png\nProcessed 900/1025: zeraora.png\nProcessed 901/1025: bellsprout.png\nProcessed 902/1025: luvdisc.png\nProcessed 903/1025: garbodor.png\nProcessed 904/1025: horsea.png\nProcessed 905/1025: nincada.png\nProcessed 906/1025: dragalge.png\nProcessed 907/1025: electrode.png\nProcessed 908/1025: salazzle.png\nProcessed 909/1025: fennekin.png\nProcessed 910/1025: metang.png\nProcessed 911/1025: donphan.png\nProcessed 912/1025: jynx.png\nProcessed 913/1025: yamask.png\nProcessed 914/1025: druddigon.png\nProcessed 915/1025: kabuto.png\nProcessed 916/1025: magmar.png\nProcessed 917/1025: swoobat.png\nProcessed 918/1025: braviary.png\nProcessed 919/1025: iron-thorns.png\nProcessed 920/1025: charmander.png\nProcessed 921/1025: baxcalibur.png\nProcessed 922/1025: rowlet.png\nProcessed 923/1025: lombre.png\nProcessed 924/1025: durant.png\nProcessed 925/1025: golem.png\nProcessed 926/1025: sunkern.png\nProcessed 927/1025: chewtle.png\nProcessed 928/1025: dewott.png\nProcessed 929/1025: croconaw.png\nProcessed 930/1025: clobbopus.png\nProcessed 931/1025: cobalion.png\nProcessed 932/1025: pansage.png\nProcessed 933/1025: deino.png\nProcessed 934/1025: spewpa.png\nProcessed 935/1025: floette.png\nProcessed 936/1025: combee.png\nProcessed 937/1025: flapple.png\nProcessed 938/1025: iron-boulder.png\nProcessed 939/1025: cofagrigus.png\nProcessed 940/1025: drednaw.png\nProcessed 941/1025: furfrou.png\nProcessed 942/1025: gouging-fire.png\nProcessed 943/1025: furret.png\nProcessed 944/1025: clefable.png\nProcessed 945/1025: wailmer.png\nProcessed 946/1025: grumpig.png\nProcessed 947/1025: charmeleon.png\nProcessed 948/1025: phantump.png\nProcessed 949/1025: venomoth.png\nProcessed 950/1025: natu.png\nProcessed 951/1025: boltund.png\nProcessed 952/1025: simisear.png\nProcessed 953/1025: blissey.png\nProcessed 954/1025: spritzee.png\nProcessed 955/1025: virizion.png\nProcessed 956/1025: armarouge.png\nProcessed 957/1025: scolipede.png\nProcessed 958/1025: rillaboom.png\nProcessed 959/1025: lokix.png\nProcessed 960/1025: fidough.png\nProcessed 961/1025: mamoswine.png\nProcessed 962/1025: minior.png\nProcessed 963/1025: mr-rime.png\nProcessed 964/1025: kirlia.png\nProcessed 965/1025: capsakid.png\nProcessed 966/1025: omastar.png\nProcessed 967/1025: drizzile.png\nProcessed 968/1025: bramblin.png\nProcessed 969/1025: hoopa.png\nProcessed 970/1025: steelix.png\nProcessed 971/1025: rotom.png\nProcessed 972/1025: hoothoot.png\nProcessed 973/1025: karrablast.png\nProcessed 974/1025: torkoal.png\nProcessed 975/1025: cramorant.png\nProcessed 976/1025: victini.png\nProcessed 977/1025: cloyster.png\nProcessed 978/1025: espathra.png\nProcessed 979/1025: relicanth.png\nProcessed 980/1025: wynaut.png\nProcessed 981/1025: raboot.png\nProcessed 982/1025: zorua.png\nProcessed 983/1025: blipbug.png\nProcessed 984/1025: timburr.png\nProcessed 985/1025: drifblim.png\nProcessed 986/1025: altaria.png\nProcessed 987/1025: cradily.png\nProcessed 988/1025: mudkip.png\nProcessed 989/1025: toxel.png\nProcessed 990/1025: lunala.png\nProcessed 991/1025: mudsdale.png\nProcessed 992/1025: shellder.png\nProcessed 993/1025: jumpluff.png\nProcessed 994/1025: bounsweet.png\nProcessed 995/1025: trubbish.png\nProcessed 996/1025: beautifly.png\nProcessed 997/1025: bagon.png\nProcessed 998/1025: oinkologne.png\nProcessed 999/1025: celesteela.png\nProcessed 1000/1025: centiskorch.png\nProcessed 1001/1025: marshtomp.png\nProcessed 1002/1025: uxie.png\nProcessed 1003/1025: suicune.png\nProcessed 1004/1025: pumpkaboo.png\nProcessed 1005/1025: sudowoodo.png\nProcessed 1006/1025: bergmite.png\nProcessed 1007/1025: lanturn.png\nProcessed 1008/1025: kingler.png\nProcessed 1009/1025: shroomish.png\nProcessed 1010/1025: gligar.png\nProcessed 1011/1025: roselia.png\nProcessed 1012/1025: bonsly.png\nProcessed 1013/1025: tatsugiri.png\nProcessed 1014/1025: type-null.png\nProcessed 1015/1025: dachsbun.png\nProcessed 1016/1025: combusken.png\nProcessed 1017/1025: arctibax.png\nProcessed 1018/1025: slowpoke.png\nProcessed 1019/1025: nacli.png\nProcessed 1020/1025: floatzel.png\nProcessed 1021/1025: latios.png\nProcessed 1022/1025: mew.png\nProcessed 1023/1025: ekans.png\nProcessed 1024/1025: cetitan.png\nProcessed 1025/1025: hakamo-o.png\nProcessing complete! Successfully processed 1025/1025 sprites.",
    "crumbs": [
      "lib",
      "Pokemon Sprites"
    ]
  },
  {
    "objectID": "lib/data_utils.html#downloading-data-from-github-repo",
    "href": "lib/data_utils.html#downloading-data-from-github-repo",
    "title": "Pokemon Sprites",
    "section": "Downloading data from github repo",
    "text": "Downloading data from github repo\nAll data used in this tutorial are small, so we upload and download them from the github itself.\n\n# poke_pixels, poke_names = get_pokemon_data()\npoke_pixels, poke_names = load_bipolar_pokemon_sprites()\nnp.save(pf.DATA / \"pokesprites_pixels.npy\", poke_pixels)\nwith open(pf.DATA / \"pokesprites_names.txt\", 'w') as f: f.write('\\n'.join(poke_names))\n\nWe can download the github repo to our cache directory.\n\n\nget_et_checkpoint\n\n get_et_checkpoint ()\n\n\n\n\nget_et_imgs\n\n get_et_imgs ()\n\n\n\n\nget_pokemon_data\n\n get_pokemon_data ()\n\n\n\n\ndownload_remote_data\n\n download_remote_data ()\n\n\n\n\ndownload_github_folder\n\n download_github_folder (repo:str='bhoov/amtutorial', branch:str='main',\n                         folder_path:str='data', cache_subdir:str='')\n\nDownload GitHub folder in one API call using Trees API.",
    "crumbs": [
      "lib",
      "Pokemon Sprites"
    ]
  }
]