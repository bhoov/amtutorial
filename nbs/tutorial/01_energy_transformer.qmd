# Energy Transformer
> Rederiving the Transformer as an Associative Memory.

<style>
    .red { color:rgb(247, 109, 104); }
    .blue { color:rgb(64, 130, 200); }
    .green { color:rgb(89, 203, 78); }
    .yellow { color:rgb(252, 211, 28); }
</style>

## Transformers look like Dynamical Systems
> Squint, and the Transformer looks like a dynamical system. 

At its core, the transformer is a stack of $L$ transformer blocks that takes a length $N$ sequence of input tokens $\{\mathbf{x}^{(0)}_1, \ldots, \mathbf{x}^{(0)}_N\}$ and outputs a length $N$ sequence of output tokens $\{\mathbf{x}^{(L)}_1, \ldots, \mathbf{x}^{(L)}_N\}$. Each token $\mathbf{x}^{(l)}_i \in \mathbb{R}^D$ is a vector of dimension $D$. 

When blocks are stacked, the residual connections form a "residual highway" that consists entirely of normalizations and additions from `Attention` and `MLP` operations.

![A vanilla Transformer Block consisting of 4 main operations: [**(multi-headed) attention**]{.red}, [**MLP**]{.blue}, [**(pre-)layernorms**]{.green}, and [**residual connections**]{.yellow}. The Transformer is a stack of these blocks, which we show depicted as a "residual highway" design. The residual highway showcases how each block "perturbs" its input, and the mathematical operation looks like a dynamical system. If the system can be described such that the operation of each block is a gradient descent, the system becomes an energy-based model.](./figs/standard-transformer.png){#fig-standard-transformer}

**Associative Memory** (AM) requires a global energy function, where each computation minimizes the total energy of the system. Our goal is to derive an energy function whose gradient looks as much like the Transformer block as possible.

## The Energy of the Transformer Block

We will now build a kind of associative memory called the "Energy Transformer" [@hoover2024energy] that turns the familiar transformer operation into an energy minimization. Energy Transformer (ET) defines a single energy on an $\mathbf{x} \in \mathbb{R}^{N \times D}$ collection of tokens, where we can think of each token $\mathbf{x}_B$ as a "particle" that knows some information about itself and needs to figure out what it should become. Some particles (unmasked tokens) already know their identity, while others (masked tokens) only know their position and must discover their identity by interacting with their neighbors.

Minimizing the energy of the Energy Transformer (ET) is a recurrent process. The entire transformer consists of a single Transformer block, and each "layer" of the transformer becomes a gradient descent step down the energy. This gradient descent step looks remarkably like a standard transformer block, complete with attention, MLP-like operations, layer normalizations, and residual connections.

The global energy combines two intuitive ideas: (1) **attention energy** that encourages masked tokens to align with relevant unmasked tokens, and (2) **memory energy** that ensures all tokens look like realistic patterns the model has learned. The gradient of each of these energies look like a self-attention and MLP, respectively, with some shared weight constraints.

This is one of those situations where the code ends up being significantly simpler than the equations. We write the equations for completeness, but feel free to skip to [@sec-ET-implementation] for succinct code.

### Math of Attention Energy

We describe the energy of a multi-headed attention with $H$ heads, where the $h$-th head of attention is parameterized by $\mathbf{W}_h^Q, \mathbf{W}_h^K \in \mathbb{R}^{D \times Y}$, where $Y$ is the "head dimension". The input to the attention is the normalized token vectors $\hat{\mathbf{x}} \in \mathbb{R}^{N \times D}$. In the math that follows, we index the heads by $h=1\ldots H$, the head dimension by $\alpha=1\ldots Y$, tokens by $A,B,C=1 \ldots N$, and each token vector by $i,j=1\ldots D$.

:::{.callout-note}
## Einstein notation
We find it convenient to use Einstein notation for the math, since it maps 1:1 to the einops operations we'll use in the code. If you aren't familiar with the notation, check out [this awesome tutorial](https://einops.rocks/1-einops-basics/). But fair warning, the equations at first look pretty complicated with all the indices.

One tip for reading equations with lots of indices: *you don't need to remember the shape or order of tensors*, just remember the meaning of the indices. The number of subscripts is the number of dimensions of the tensor, and the meaning of each dimension is captured in the index name. For example, let $B=1\ldots N$ index the token position in a sequence, and let $i=1\ldots D$ index into each token vector. $x_{Bi}$ is an element of a 2-dimensional tensor capturing the sequence length $N$ and token dimension $D$. Transposes don't have meaning since things are named, so $x_{Bi} = x_{iB}$. So long as you know the index semantics, you can read always read the equation. Everything is just scalar multiplication and addition.
:::

The familiar queries and keys are computed as normal linear transformations:

$$ 
   \begin{split}
        K_{h \alpha B} &= \sum\limits_j W^K_{h \alpha j}\; \hat{x}_{Bj}, \qquad \mathbf{K} \in \mathbb{R}^{H \times Y \times N} \\
        Q_{h \alpha C} &= \sum\limits_j W^Q_{h \alpha j}\; \hat{x}_{Cj}, \qquad \mathbf{Q} \in \mathbb{R}^{H \times Y \times N}
    \end{split}
$$

Our familiar "raw attention scores" (pre-softmax) are still the dot-product correlations between each query and key:

$$
A_{hBC} = \sum_{\alpha} K_{h\alpha B} Q_{h\alpha C} 
$$

Now for the different part: we describe the energy of the attention as the negative log-sum-exp of the attention scores. We will use the $\beta$ as an inverse-temperature hyperparameter to scale the attention scores.

$$
E^\text{ATT} = -\frac{1}{\beta} \sum_{h=1}^H \sum_{C=1}^N \log \left( \sum_{B \neq C} \exp(\beta A_{hBC}) \right)
$${#eq-attention-energy}

As we saw in [a previous notebook](./00_dense_storage.ipynb), the negative log-sum-exp is an exponential variation of the Dense Associative Memory. The cool thing is that the gradient of the negative log-sum-exp is the softmax, which is what we'd like to see in the attention update rule.

:::{.callout-note}
## Where are our values?
You may recall that traditional attention also has a value matrix. When we take the gradient of @eq-attention-energy, we lose the flexibility to include an independently parameterized values: the values **must** be a function of the queries and the keys.
:::

### Memory Energy

In traditional transformers, the MLP (without biases) can be written as a two-layer feedforward network with a ReLU on the hidden activations. The MLP is parameterized by two weight matrices $\mathbf{V}, \mathbf{W} \in \mathbb{R}^{M \times D}$ where $M$ is the size of the hidden layer ($M=4D$ is often viewed as the default expansion factor atop token dimension $D$). Let's again use Einstein notation, where $\mu=1\ldots M$ indexes the hidden units, $i,j=1\ldots D$ index the token dimensions, and $B=1\ldots N$ indexes each token. 

$$
\text{MLP}(\hat{\mathbf{x}})_{Bi} = \sum_\mu W_{\mu i} \; \text{ReLU}\left(\sum_j V_{\mu j} \hat{\mathbf{x}}_{Bj}\right)
$${#eq-mlp-update}

If we assume weight sharing between $\mathbf{V} = \mathbf{W} = \boldsymbol{\xi}$, this is a gradient descent step down the energy of a Hopfield Network 

$$
E^{\text{HN}}(\hat{\mathbf{x}}) = - \sum_{B, \mu} F\left(\sum_j \xi_{\mu j} \hat{\mathbf{x}}_{Bj}\right)
$$

with rectified quadratic energy $F(\cdot) := \frac12 \text{ReLU}(\cdot)^2$. If we say $f(\cdot) := F'(\cdot) = \text{ReLU}(\cdot)$, the negative gradient of the energy is

$$
-\frac{\partial E^{\text{HN}}(\mathbf{\hat{x}})}{\partial \hat{x}_{Bi}} 
= \sum_\mu \xi_{\mu i} \; f\left(\sum_j \xi_{\mu j} \hat{\mathbf{x}}_{Bj}\right),
$$

which is identical to the MLP operation in @eq-mlp-update with a weight sharing constraint.

:::{.callout-note}
It is perfectly reasonable to consider other convex functions $F$ for use in the energy. Polynomials of higher degree $n$ or exponential functions are both valid and will yield [Dense Associative Memory](./00_dense_storage.ipynb). However, because traditional Transformers use a ReLU activation, we use a rectified quadratic energy.
:::

### Code implementation {#sec-ET-implementation}

Let's implement the attention energy in code. We will use [`jax`](https://github.com/jax-ml/jax) and [`equinox`](https://github.com/patrick-kidger/equinox) for our code.

```{python}
#| hide
import jax, jax.numpy as jnp, jax.random as jr, jax.tree_util as jtu, jax.lax as lax
import equinox as eqx
from dataclasses import dataclass
from typing import *
import matplotlib.pyplot as plt
import numpy as np
from fastcore.basics import *
import matplotlib.pyplot as plt
from jaxtyping import Float, Array
```

```{python}
class EnergyAttention(eqx.Module):
  """Energy of multi-headed attention.
  
  Has only two learnable parameters, Wk and Wq"""
  Wq: Float[Array, "H D Y"] # Query projection
  Wk: Float[Array, "H D Y"] # Key projection
  beta: Optional[float] = None # inverse temperature

  def energy(
    self, 
    xhat:Float[Array, "N D"], # Layer normalized tokens
    ) -> float:
    "Attention energy's update rule is computed via autograd of this energy."
    beta = self.beta or 1/jnp.sqrt(self.Wk.shape[-1])
    K = jnp.einsum("kd,hdy->khy", xhat, self.Wk)
    Q = jnp.einsum("qd,hdy->qhy", xhat, self.Wq)
    A = jax.nn.logsumexp(beta * jnp.einsum("khy,qhy->hqk", Q, K), -1)
    return -1/beta * A.sum()

class HopfieldNetwork(eqx.Module):
  "A simple Hopfield Net with ReLU activation replaces the MLP"
  Xi: Float[Array, "M D"]

  def energy(self, xhat:Float[Array, "N D"]):
    """Return the Hopfield Network's energy"""
    hid = jnp.einsum("nd,md->nm", xhat, self.Xi)
    E = -0.5 * (hid.clip(0) ** 2).sum()
    return E

def layer_norm(x, axis=-1, eps=1e-5):
    "Simple layer normalization to check convergence"
    xmean = x.mean(axis, keepdims=True)
    xmeaned = x-xmean
    return xmeaned / jnp.sqrt((xmeaned**2).mean(axis, keepdims=True) + eps)

class EnergyLayerNorm(eqx.Module):
  """Define our primary activation function (modified LayerNorm) as a lagrangian with energy"""
  gamma: Float[Array, ""]  # Scaling scalar
  delta: Float[Array, "D"] # Bias per token
  use_bias: bool = False
  eps: float = 1e-5
    
  def lagrangian(self, x):
    """Integral of the standard LayerNorm"""
    D = x.shape[-1]
    xmeaned = x - x.mean(-1, keepdims=True)
    t1 = D * self.gamma * jnp.sqrt((1 / D * xmeaned**2).sum() + self.eps)
    if not self.use_bias: return t1
    t2 = (self.delta * x).sum()
    return t1 + t2

  def __call__(self, x):
    """LayerNorm. The derivative of the Lagrangian"""
    xmeaned = x - x.mean(-1, keepdims=True)
    v = self.gamma * (xmeaned) / jnp.sqrt((xmeaned**2).mean(-1, keepdims=True)+ self.eps)
    if self.use_bias: return v + self.delta
    return v
    
  def energy(self, x):
    """Compute the energy of this Lagrangian through the Legendre Transform"""
    return (self(x) * x).sum() - self.lagrangian(x)
```

That's it! We rely on autograd to do the energy minimization. Let's check that the energy of each module monotonically decreases and is bounded from below. 

```{python}
N, H, D, Y = 100, 12, 128, 42
key1, key2, key3 = jr.split(jr.PRNGKey(11), 3)
# eattn = EnergyAttention(Wq=jnp.sqrt(1/Y)*jr.normal(key1, (H, D, Y)), Wk=jnp.sqrt(1/Y)*jr.normal(key2, (H, D, Y)))
# eattn = EnergyAttention(Wq=jr.normal(key1, (H, D, Y)), Wk=jr.normal(key2, (H, D, Y)))
eattn = EnergyAttention(Wq=jr.normal(key1, (H, D, Y))/Y, Wk=jr.normal(key2, (H, D, Y))/Y)
lnorm = EnergyLayerNorm(gamma=1., delta=jnp.zeros(D))

def energy_recall(Efn, x_init, nsteps, step_size):
  "Simple gradient descent to recall a memory"
  @jax.jit
  def gd_step(xhat, i):
      energy, grad = jax.value_and_grad(Efn)(layer_norm(xhat))
      xhat_next = xhat - step_size * grad
      return xhat_next, energy

  xhat_init = lnorm(x_init)
  final_xhat, energy_history = jax.lax.scan(
      gd_step,
      xhat_init,
      jnp.arange(nsteps)
  )
  return final_xhat, energy_history

x_init = lnorm(jr.normal(key3, (N, D))) # Layer normalized tokens
final_attn_xhat, attn_energy_history = energy_recall(eattn.energy, x_init, nsteps=3000, step_size=0.5)
final_hn_xhat, hn_energy_history = energy_recall(hn.energy, x_init, nsteps=3000, step_size=0.5)
```

```{python}
#| echo: false
#| label: fig-energy-descent-combined
#| fig-cap: Energy descent for both attention and Hopfield network components. The attention energy descent is much faster than the Hopfield network energy descent.
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Plot attention energy descent
ax1.plot(attn_energy_history, linewidth=2, color='blue')
ax1.set_xlabel('Gradient Descent Steps')
ax1.set_ylabel('Energy')
ax1.set_title('Energy Attention Descent')
ax1.grid(True, alpha=0.3)

# Plot Hopfield network energy descent  
ax2.plot(hn_energy_history, linewidth=2, color='red')
ax2.set_xlabel('Gradient Descent Steps')
ax2.set_ylabel('Energy')
ax2.set_title('Hopfield Network Energy Descent')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

This energy is also bounded from below.

```{python}
#| hide
N, D = 100, 128
M = 4 * D
key1, key2 = jr.split(jr.PRNGKey(12))

hn = HopfieldNetwork(Xi=jr.normal(key1, (M, D)))
xhat_init = layer_norm(jr.normal(key3, (N, D))) # Layer normalized tokens

final_xhat, energy_history = energy_recall(hn.energy, xhat_init, nsteps=3000, step_size=0.5)
```

```{python}
#| echo: false
# Plot the energy descent
plt.figure(figsize=(7, 5))
plt.plot(energy_history, linewidth=2, color='blue')
plt.xlabel('Gradient Descent Steps')
plt.ylabel('Energy')
plt.title('Hopfield Network Energy Descent')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### Total energy

The total energy of the ET block is the sum of the attention energy and the memory energy. 

$$
E^\text{ET}(\hat{\mathbf{x}}) = E^\text{ATT}(\hat{\mathbf{x}}) + E^\text{HN}(\hat{\mathbf{x}})
$$

The negative gradient $-\nabla E_\theta(\mathbf{x})$ of the energy has all [four components]{#four-components} of the standard transformer block, albeit with some weight sharing and the `MLP+Attn` operations happening parallel rather than sequentially.

```{python}
class EnergyTransformer(eqx.Module):
  "Sum the memory and attention energies"
  attn: EnergyAttention
  hn: HopfieldNetwork
  config: ETConfig = eqx.field(static=True)
  
  def __init__(self, key:jr.PRNGKey, config:ETConfig):
    attn_key, hn_key = jr.split(key)
    self.attn = EnergyAttention(attn_key, config)
    self.hn = HopfieldNetwork(hn_key, config)
    self.config = config

  def energy(self, g:jnp.ndarray):
    """Return the energy of the whole Transformer"""
    return self.attn.energy(g) + self.hn.energy(g)
```


```{python}

@dataclass
class ETConfig():
  D: int = 768 # Token dimension of ET
  Y: int = 64 # Token dimension of each query and key
  n_heads: int = 12 # Number of heads
  scale_mems: float = 4. # Scale the number of memories by this factor relative to token dimension D

class EnergyLayerNorm(eqx.Module):
  """Define our primary activation function (modified LayerNorm) as a lagrangian with energy"""
  gamma: jax.Array  # Scaling scalar
  delta: jax.Array  # Bias
  use_bias: bool
  eps: float
  
  def __init__(self, dim: int, use_bias:bool = True, eps:float = 1e-5):
    self.use_bias = use_bias
    self.gamma = jnp.ones(())
    self.delta = jnp.zeros(dim)
    self.eps = eps
    
  def lagrangian(self, x):
    """The integral of the standard LayerNorm, with the following twist: `gamma` is a scalar, not a vector of shape `dim` as in the original layernorm """
    D = x.shape[-1]
    xmeaned = x - x.mean(-1, keepdims=True)
    t1 = D * self.gamma * jnp.sqrt((1 / D * xmeaned**2).sum() + self.eps)
    if not self.use_bias: 
      return t1
    t2 = (self.delta * x).sum()
    return t1 + t2

  def g(self, x):
    """The manual derivative of the lagrangian. 
    
    You could compute this with autograd, but it is more efficient and clear to implement it directly
    """
    xmeaned = x - x.mean(-1, keepdims=True)
    v = self.gamma * (xmeaned) / jnp.sqrt((xmeaned**2).mean(-1, keepdims=True)+ self.eps)
    if self.use_bias:
        return v + self.delta
    return v

  def __call__(self, x):
    """An alias for the activation function `g`"""
    return self.g(x)
    
  def energy(self, x):
    """Compute the energy of this Lagrangian through the Legendre Transform"""
    return (self.g(x) * x).sum() - self.lagrangian(x)
    
class EnergyAttention(eqx.Module):
  """Our novel attention with energy

  Has only two learnable parameters, Wk and Wq
  """
  Wq: jax.Array
  Wk: jax.Array
  config: ETConfig = eqx.field(static=True)

  def __init__(self, key:jr.PRNGKey, config:ETConfig):
    kkey, qkey = jr.split(key)
    self.Wk = jr.normal(kkey, (config.n_heads, config.Y, config.D))
    self.Wq = jr.normal(qkey, (config.n_heads, config.Y, config.D))
    self.config = config

  def energy(self, g:jnp.ndarray):
    """Return the energy of the block. The update rule is autograd through this function"""
    beta = 1/jnp.sqrt(self.config.Y)
    K = jnp.einsum("kd,hzd->khz", g, self.Wk) # nKeys,nHeads,Y
    Q = jnp.einsum("qd,hzd->qhz", g, self.Wq) # nQueries,nHeads,Y
    A = jax.nn.logsumexp(beta * jnp.einsum("qhz,khz->hqk", Q, K), -1) # nHeads,nQueries,nKeys
    return -1/beta * A.sum()

class HopfieldNetwork(eqx.Module):
  """ A simple Hopfield Network (we use ReLU as the activation function) replaces the MLP in traditional Transformers """
  Xi: jax.Array

  def __init__(self, key:jr.PRNGKey, config:ETConfig):
    nmems = int(config.scale_mems * config.D)
    self.Xi = jr.normal(key, (config.D, nmems))

  def energy(self, g:jnp.ndarray):
    """Return the Hopfield Network's energy"""
    hid = jnp.einsum("nd,dm->nm", g, self.Xi) # nTokens, nMems
    E = -0.5 * (jax.nn.relu(hid) ** 2).sum()
    return E

class EnergyTransformer(eqx.Module):
  """A simple wrapper class that sums the energies of the Hopfield Network and the Attention"""
  attn: EnergyAttention
  hn: HopfieldNetwork
  config: ETConfig = eqx.field(static=True)
  
  def __init__(self, key:jr.PRNGKey, config:ETConfig):
    attn_key, hn_key = jr.split(key)
    self.attn = EnergyAttention(attn_key, config)
    self.hn = HopfieldNetwork(hn_key, config)
    self.config = config

  def energy(self, g:jnp.ndarray):
    """Return the energy of the whole Transformer"""
    return self.attn.energy(g) + self.hn.energy(g)
  
```

## Using a pretrained Energy Transformer

## Training an Energy Transformer

We train ET on a simple dataset, should work on CPU.




The Transformer is a very flexible computing paradigm that can be used for the two major approaches of modern language modeling: **masked token prediction** (e.g., BERT and diffusion-style transformers) where you predict the fraction of input tokens that are MASKed using information from the unmasked tokens, and **autoregressive language modeling** (e.g., GPT-style models), where each token in the input sequence is transformed into the next prediction token.
