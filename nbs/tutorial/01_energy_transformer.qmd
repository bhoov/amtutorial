# Energy Transformer
> Rederiving the Transformer as an Associative Memory.

<style>
    .red { color:rgb(247, 109, 104); }
    .blue { color:rgb(64, 130, 200); }
    .green { color:rgb(89, 203, 78); }
    .yellow { color:rgb(252, 211, 28); }
</style>

## Transformers look like Dynamic Systems
> Squint, and the Transformer looks like a dynamic system. 

The transformer is the engine behind the best neural networks we have today. At its core, the transformer is a stack of $L$ transformer blocks that takes a length $N$ sequence of input tokens $\{\mathbf{x}^{(0)}_1, \ldots, \mathbf{x}^{(0)}_N\}$ and outputs a length $N$ sequence of output tokens $\{\mathbf{x}^{(L)}_1, \ldots, \mathbf{x}^{(L)}_N\}$. Each token $\mathbf{x}^{(l)}_i \in \mathbb{R}^D$ is a vector of dimension $D$. 

Each transformer block contains four important components: 

1. An [**attention mechanism**]{.red} that lets each token exchange information with other tokens in the same sequence. The attention is described to have multiple "attention heads", allowing different heads to route different kinds of information between tokens.
2. A [**2-layer MLP**]{.blue} transforms each token independently into the representation for the next layer. *The MLP does not allow tokens to mix with each other.*
3. A [**Layer-Normalization**]{.green} normalizes the token representation for gradient and computation stability. We will consider the version of the Transformer with *pre-layernorms*, where normalization occurs before the attention and MLP operations.
4. A [**residual connection**]{.yellow} that adds the input to the output of each of these sub-operations. 

When blocks are stacked, the residual connections connect the blocks together and form a "residual highway" that consists entirely of additions and normalizations. Each `Attention` and `MLP` operation *exclusively add* information to the highway. If we index each stacked block as $t$, you can see some resemblance to a discrete dynamical system, where the final computation is an "integration" of sorts over the operation of the entire network. At a very high level, you can see the transformer as a dynamical system where $F_t$ is the operation of the $t$-th block.

$$
\mathbf{x}^{(t+1)} = \mathbf{x}^{(t)} + F_t(\mathbf{x}^{(t)})
$${#eq-generic-dynamical-system}

![A vanilla Transformer Block consisting of 4 main operations: [**(multi-headed) attention**]{.red}, [**MLP**]{.blue}, [**(pre-)layernorms**]{.green}, and [**residual connections**]{.yellow}. The Transformer is a stack of these blocks, which we show depicted as a "residual highway" design. The residual highway showcases how each block "perturbs" its input, and the mathematical operation looks like a dynamical system. If the system can be described such that the operation of each block is a gradient descent, the system becomes an energy-based model.](./figs/standard-transformer.png){#fig-standard-transformer}


**Associative Memory** (AM) requires a global energy function, where each "computational block" minimizes the total energy of the system. @eq-generic-dynamical-system would look like a gradient descent on some energy function if we could guarantee:

1. $F_t$ was the same each iteration i.e., all blocks $\{F_1, \ldots, F_L\}$ **share parameters**. Let's call this shared-parameter block $F$ and it is independent of the number of recurrent steps or "layers".
2. $F$ could be described as the gradient of a scalar **energy** (i.e., $F(\mathbf{x}) = -\nabla E(\mathbf{x})$).
3. The energy $E$ is both **continuous** and **bounded from below**. We should be able to write a continuous-time, gradient flow equation for the system and make some guarantees about the convergence limit of the system.

The rest of this notebook derives the familiar transformer into a true associative memory. This architecture is called the "Energy Transformer" [@hoover2024energy]. It will require fundamental modifications to the Transformer block. Specifically, the transformer block becomes a $\mathbb{R}^{N \times D} \mapsto \mathbb{R}$ scalar-valued energy function, and each "layer" of the model becomes a gradient descent step down the energy. However different the derivation, the end result preserves the four main components of a Transformer with attention, MLP, layernorms, and residual connections.

This overview is summarized in @fig-standard-transformer.

## Using a pretrained Energy Transformer

## Training an Energy Transformer

We train ET on a simple dataset, should work on CPU.




The Transformer is a very flexible computing paradigm that can be used for the two major approaches of modern language modeling: **masked token prediction** (e.g., BERT and diffusion-style transformers) where you predict the fraction of input tokens that are MASKed using information from the unmasked tokens, and **autoregressive language modeling** (e.g., GPT-style models), where each token in the input sequence is transformed into the next prediction token.
