---
title: "Energy Transformer"
subtitle: "Rederiving the Transformer as an Energy-based Associative Memory"
author: "Your Name"
format:
  revealjs:
    theme: dark
    slide-number: true
    chalkboard: true
    preview-links: auto
    css: styles.css
---

## The Big Idea {.center}

:::: {.columns}

::: {.column width="50%"}
**Traditional View:**
- Transformers are feedforward networks
- Layer-by-layer processing
- Black box representations
:::

::: {.column width="50%"}
**Energy View:**
- Transformers are dynamical systems
- Energy minimization process  
- Interpretable stored patterns
:::

::::

::: {.fragment}
> **"Squint, and the Transformer looks like a dynamical system"**
:::

---

## Transformers as Flow Systems

![](../nbs/tutorial/assets/figs/standard-transformer.png){.center width="60%"}

::: {.incremental}
- Residual connections form a "highway"
- Each block "perturbs" its input
- Looks like gradient descent steps!
:::

---

## The Energy Transformer Block

![](../nbs/tutorial/assets/figs/et-block.png){.center width="40%"}

::: {.incremental}
- **Single global energy function**
- Attention + Memory energies computed in parallel
- Residual connections emerge naturally
:::

---

## Two Sources of Energy

:::: {.columns}

::: {.column width="50%"}
### Attention Energy
```python
def attn_energy(self, xhat):
  K = xhat @ self.Wk  # Keys
  Q = xhat @ self.Wq  # Queries
  A = K @ Q.T         # Raw scores
  
  # Negative log-sum-exp
  return -1/β * logsumexp(β * A).sum()
```
:::

::: {.column width="50%"}
### Memory Energy  
```python
def hn_energy(self, xhat):
  # Hopfield Network energy
  hid = xhat @ self.Xi.T
  return -0.5 * (hid.clip(0) ** 2).sum()
```
:::

::::

::: {.fragment}
**Key insight:** Gradient of energy = Transformer operations!
:::

---

## Remarkably Simple Implementation

```python
class EnergyTransformer(eqx.Module):
  Wq: Array  # Query weights
  Wk: Array  # Key weights  
  Xi: Array  # Memory patterns
  
  def energy(self, xhat):
    # That's it - just 2 energy terms!
    return self.attn_energy(xhat) + self.hn_energy(xhat)
```

::: {.fragment}
```python
# Inference = gradient descent
energy, grad = jax.value_and_grad(model.energy)(x)
x_next = x - step_size * grad
```
:::

---

## Energy Always Decreases

::: {.center}
![Energy descent during inference](../nbs/tutorial/01_energy_transformer_files/figure-commonmark/fig-energy-descent-combined-output-1.png){width="70%"}
:::

**Guaranteed convergence** to stable attractors

---

## Masked Image Inpainting

::: {.center}
<video width="800" controls>
  <source src="../nbs/tutorial/cache/01_energy_transformer/et_reconstruction.mp4" type="video/mp4">
</video>
:::

**Memory recall through energy minimization**

---

## What Did the Model Learn?

:::: {.columns}

::: {.column width="40%"}
### Traditional Transformers
- Opaque weight matrices
- Hard to interpret
- Black box features
:::

::: {.column width="60%"}
### Energy Transformers
![Stored patterns visualization](../data/et-figs/00_parrot.png){width="100%"}

**Hopfield weights = Actual stored patterns!**
:::

::::

---

## Stored Pattern Gallery

::: {.center}
![Sample of learned visual patterns from the Hopfield Network](../nbs/tutorial/01_energy_transformer_files/figure-commonmark/cell-33-output-1.png){width="80%"}
:::

::: {.fragment}
**Interpretability by design** - not post-hoc analysis!
:::

---

## Key Advantages

::: {.incremental}
1. **Theoretical Foundation**: Grounded in energy-based models
2. **Guaranteed Dynamics**: Energy always decreases  
3. **Built-in Interpretability**: Stored patterns are visible
4. **Unified Framework**: Attention + Memory in single energy
5. **Simple Implementation**: ~10 lines of core code
:::

---

## Limitations & Future Work

::: {.incremental}
- **Training Constraints**: Current model trained for fixed 12 steps
- **Metastable States**: Not true energy minima  
- **Scale**: Demonstrated on vision, needs language evaluation
- **Architecture**: Fixed weight sharing constraints
:::

::: {.fragment}
**Opportunity:** True end-to-end energy training
:::

---

## Takeaways {.center}

:::: {.columns}

::: {.column width="33%"}
### 🔬 **Science**
Energy perspective reveals transformer mechanics
:::

::: {.column width="33%"}
### 🛠️ **Engineering**  
Simpler, more interpretable architectures
:::

::: {.column width="33%"}
### 🔮 **Future**
Foundation for next-gen associative memories
:::

::::

::: {.fragment}
> **Energy-based models: Making AI more transparent, one gradient at a time**
:::

---

## Thank You! {.center}

:::: {.columns}

::: {.column width="50%"}
### Resources
- [Tutorial Notebook](https://colab.research.google.com/github/bhoov/amtutorial/blob/main/tutorial_ipynbs/01_energy_transformer.ipynb)
- [Paper: Energy Transformer](https://arxiv.org/abs/2401.09442)
- [Code Repository](https://github.com/bhoov/amtutorial)
:::

::: {.column width="50%"}
### Questions?

**Contact:** 
- GitHub: @bhoov
- Email: your.email@domain.com
:::

::::
